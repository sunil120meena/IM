{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOpWXRCkSeNe",
    "outputId": "c6b4ba6b-ee0e-406a-dbe4-7e196e27403f"
   },
   "outputs": [],
   "source": [
    "# %pip install 'networkx<2.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3Qlb0a7x8Jb",
    "outputId": "e698392f-587f-496c-b22c-4e47d3eb3ae7"
   },
   "outputs": [],
   "source": [
    "# !pip install cdlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import heapq\n",
    "import argparse\n",
    "import threading\n",
    "import multiprocessing\n",
    "import sys\n",
    "import queue\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import math, time\n",
    "from copy import deepcopy\n",
    "import multiprocessing, json\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "\n",
    "\n",
    "#importing libraries that will be used\n",
    "# import networkx as nx#for creating network\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt#for plotting plots\n",
    "# import random\n",
    "# import time#claculating time\n",
    "# import math\n",
    "# from collections import Counter\n",
    "# from itertools import permutations \n",
    "# from itertools import combinations\n",
    "\n",
    "# from collections import defaultdict\n",
    "# from scipy.io import mmread# to read dataset\n",
    "# import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OD7RKjsuuzaY",
    "outputId": "a70ee33e-fa14-4538-c7c0-11b89af314c2"
   },
   "outputs": [],
   "source": [
    "#importing libraries that will be used\n",
    "import networkx as nx#for creating network\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt#for plotting plots\n",
    "\n",
    "import random\n",
    "import time#claculating time\n",
    "import math\n",
    "from collections import Counter\n",
    "from itertools import permutations \n",
    "from itertools import combinations\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.io import mmread# to read dataset\n",
    "import pandas as pd\n",
    "\n",
    "# from cdlib import algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def celf(graph, k):\n",
    "    \"\"\"\n",
    "    Implementation of CELF algorithm for influence maximization in social networks\n",
    "    \n",
    "    Args:\n",
    "    - graph: NetworkX graph object representing the social network\n",
    "    - k: number of nodes to select\n",
    "    \n",
    "    Returns:\n",
    "    - nodes: list of k nodes with the highest influence scores\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    nodes = []\n",
    "    heap = []\n",
    "    marg_gains = {}\n",
    "\n",
    "    # Calculate the marginal gain for each node\n",
    "    for node in graph.nodes():\n",
    "        # Run Monte Carlo simulations to estimate the influence of each node\n",
    "        sim_res = linear_Threshold(graph, nodes + [node])\n",
    "        marg_gains[node] = len(sim_res) - len(nodes)\n",
    "        # Add the node to the heap with its marginal gain as key\n",
    "        heapq.heappush(heap, (-marg_gains[node], node))\n",
    "\n",
    "    # Select the k nodes with the highest influence scores\n",
    "    while len(nodes) < k:\n",
    "        # Get the node with the highest marginal gain\n",
    "        _, node = heapq.heappop(heap)\n",
    "        # Recalculate the marginal gain of the selected node\n",
    "        sim_res = linear_Threshold(graph, nodes + [node])\n",
    "        marg_gains[node] = len(sim_res) - len(nodes)\n",
    "        # Add the node to the list of selected nodes\n",
    "        nodes.append(node)\n",
    "        # Update the heap with the new marginal gains\n",
    "        for n in graph.neighbors(node):\n",
    "            if n not in nodes:\n",
    "                heapq.heappush(heap, (-marg_gains[n], n))\n",
    "\n",
    "    return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Implementation of SimPath algorithm'''\n",
    "\n",
    "class CELFQueue:\n",
    "    # create if not exist\n",
    "    nodes = None\n",
    "    q = None\n",
    "    nodes_gain = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.q = []\n",
    "        self.nodes_gain = {}\n",
    "\n",
    "    def put(self, node, marginalgain):\n",
    "        self.nodes_gain[node] = marginalgain\n",
    "        heapq.heappush(self.q, (-marginalgain, node))\n",
    "\n",
    "    def update(self, node, marginalgain):\n",
    "        self.remove(node)\n",
    "        self.put(node, marginalgain)\n",
    "\n",
    "    def remove(self, node):\n",
    "        self.q.remove((-self.nodes_gain[node], node))\n",
    "        self.nodes_gain[node] = None\n",
    "        heapq.heapify(self.q)\n",
    "\n",
    "    def topn(self, n):\n",
    "        top = heapq.nsmallest(n, self.q)\n",
    "        top_ = list()\n",
    "        for t in top:\n",
    "            top_.append(t[1])\n",
    "        return top_\n",
    "\n",
    "    def get_gain(self, node):\n",
    "        return self.nodes_gain[node]\n",
    "\n",
    "    \n",
    "    \n",
    "def init_D(graph):\n",
    "    D = {}\n",
    "#     for i in range(graph.node_num + 1):\n",
    "    for i in graph.nodes:\n",
    "        D[i]=[]\n",
    "    return D\n",
    "\n",
    "class Graph:\n",
    "    nodes = None\n",
    "    edges = None\n",
    "    children = None\n",
    "    parents = None\n",
    "    node_num = None\n",
    "    edge_num = None\n",
    "\n",
    "    def __init__(self, nodes, edges, children, parents, node_num, edge_num):\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "        self.children = children\n",
    "        self.parents = parents\n",
    "        self.node_num = node_num\n",
    "        self.edge_num = edge_num\n",
    "#         print(\"Numnodes:\",self.node_num)\n",
    "    def get_children(self, node):\n",
    "        ch = self.children.get(node)\n",
    "        if ch is None:\n",
    "            self.children[node] = []\n",
    "        return self.children[node]\n",
    "\n",
    "    def get_parents(self, node):\n",
    "        pa = self.parents.get(node)\n",
    "        if pa is None:\n",
    "            self.parents[node] = []\n",
    "        return self.parents[node]\n",
    "\n",
    "    def get_weight(self, src, dest):\n",
    "        weight = self.edges.get((src, dest))\n",
    "        if weight is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return weight\n",
    "\n",
    "    # return true if node1 is parent of node 2 , else return false\n",
    "    def is_parent_of(self, node1, node2):\n",
    "        if self.get_weight(node1, node2) != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # return true if node1 is child of node 2 , else return false\n",
    "    def is_child_of(self, node1, node2):\n",
    "        return self.is_parent_of(node2, node1)\n",
    "\n",
    "    def get_out_degree(self, node):\n",
    "        return len(self.get_children(node))\n",
    "\n",
    "    def get_in_degree(self, node):\n",
    "        return len(self.get_parents(node))\n",
    "\n",
    "    \n",
    "def read_graph_info(path):\n",
    "    if os.path.exists(path):\n",
    "        parents = {}\n",
    "        children = {}\n",
    "        edges = {}\n",
    "        nodes = set()\n",
    "\n",
    "        try:\n",
    "            f = open(path, 'r')\n",
    "            txt = f.readlines()\n",
    "            header = str.split(txt[0])\n",
    "            node_num = int(header[0])\n",
    "            edge_num = int(header[1])\n",
    "\n",
    "            for line in txt[1:]:\n",
    "#                 print(\"line:\",line)\n",
    "                row = str.split(line)\n",
    "#                 print(row)\n",
    "                src = int(row[0])\n",
    "                des = int(row[1])\n",
    "                nodes.add(src)\n",
    "                nodes.add(des)\n",
    "#                 print(src,des)    \n",
    "                if children.get(src) is None:\n",
    "                    children[src] = []\n",
    "                if parents.get(des) is None:\n",
    "                    parents[des] = []\n",
    "\n",
    "#                 weight = float(row[2])\n",
    "                weight=round(random.uniform(0.0,1.0),2)\n",
    "                edges[(src, des)] = weight\n",
    "                children[src].append(des)\n",
    "                parents[des].append(src)\n",
    "\n",
    "            return list(nodes), edges, children, parents, node_num, edge_num\n",
    "        except IOError:\n",
    "            print('IOError')\n",
    "    else:\n",
    "        print('file can not found')\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# def get_vertex_cover(graph):\n",
    "#     # dv[i] out degree of node i+1\n",
    "#     dv = np.zeros(graph.node_num)\n",
    "#     # e[i,j] = 0: edge (i+1,j+1),(j+1,i+1) checked\n",
    "#     check_array = np.zeros((graph.node_num, graph.node_num))\n",
    "#     checked = 0\n",
    "# #     print(range(graph.node_num))\n",
    "#     for i in range(graph.node_num):\n",
    "#         # for a edge (i,j) and (j,i) may be count twice but the algorithm is to find a vertex cover. it doesn't mater\n",
    "#         dv[i] = graph.get_out_degree(i + 1) + graph.get_in_degree(i + 1)\n",
    "#     # V: Vertex cover\n",
    "#     V = set()\n",
    "# #     print(dv)\n",
    "#     while checked < graph.edge_num:\n",
    "#         s = dv.argmax() + 1\n",
    "#         V.add(s)\n",
    "#         # make sure that never to select this node again\n",
    "#         children = graph.get_children(s)\n",
    "#         parents = graph.get_parents(s)\n",
    "#         for child in children:\n",
    "#             if check_array[s - 1][child - 1] == 0:\n",
    "#                 check_array[s - 1][child - 1] = 1\n",
    "#                 checked = checked + 1\n",
    "#         for parent in parents:\n",
    "#             if check_array[parent - 1][s - 1] == 0:\n",
    "#                 check_array[parent - 1][s - 1] = 1\n",
    "#                 checked = checked + 1\n",
    "#         dv[s - 1] = -1\n",
    "#     return list(V)\n",
    "\n",
    "\n",
    "\n",
    "def forward(Q, D, spd, pp, r, W, U, spdW_u, graph):\n",
    "    x = Q[-1]\n",
    "    if U is None:\n",
    "        U = []\n",
    "    children = graph.get_children(x)\n",
    "    count = 0\n",
    "    while True:\n",
    "        # any suitable chid is ok\n",
    "\n",
    "#         for child in range(count, len(children)):\n",
    "        flag=1\n",
    "        for y in children:\n",
    "            \n",
    "#             print(\"y,D[x],type:\",y,D[x],type(D[x]))\n",
    "#             if(y not in D[x]):\n",
    "#                 print(\"YES\")\n",
    "                \n",
    "            if (y in W) and (y not in Q) and (y not in D[x]):\n",
    "#                 y = children[child]\n",
    "                flag=0\n",
    "                break\n",
    "#             count = count + 1\n",
    "\n",
    "        # no such child:\n",
    "        if flag==1:\n",
    "            return Q, D, spd, pp\n",
    "\n",
    "        if pp * graph.get_weight(x, y) < r:\n",
    "            D[x].append(y)\n",
    "        else:\n",
    "            Q.append(y)\n",
    "            pp = pp * graph.get_weight(x, y)\n",
    "            spd = spd + pp\n",
    "            D[x].append(y)\n",
    "            x = Q[-1]\n",
    "            for v in U:\n",
    "                if v not in Q:\n",
    "                    spdW_u[v] = spdW_u[v] + pp\n",
    "            children = graph.get_children(x)\n",
    "            count = 0\n",
    "\n",
    "\n",
    "            \n",
    "def backtrack(u, r, W, U, spdW_, graph):\n",
    "    Q = [u]\n",
    "    spd = 1\n",
    "    pp = 1\n",
    "    D = init_D(graph)\n",
    "\n",
    "    while len(Q) != 0:\n",
    "        Q, D, spd, pp = forward(Q, D, spd, pp, r, W, U, spdW_, graph)\n",
    "        u = Q.pop()\n",
    "#         print(\"In backtrack:type,Q,u\",type(Q),Q,u)\n",
    "        D[u] = []\n",
    "        if len(Q) != 0:\n",
    "            v = Q[-1]\n",
    "            pp = pp / graph.get_weight(v, u)\n",
    "    return spd\n",
    "\n",
    "\n",
    "\n",
    "def simpath_spread(S, r, U, graph, spdW_=None):\n",
    "    spread = 0\n",
    "    # W: V-S\n",
    "    W = set(graph.nodes).difference(S)\n",
    "    if U is None or spdW_ is None:\n",
    "        spdW_={}\n",
    "        for i in graph.nodes:\n",
    "            spdW_[i]=0\n",
    "#         spdW_ = np.zeros(graph.node_num + 1)\n",
    "        # print 'U None'\n",
    "    for u in S:\n",
    "        W.add(u)\n",
    "        # print spdW_[u]\n",
    "        spread = spread + backtrack(u, r, W, U, spdW_[u], graph)\n",
    "        # print spdW_[u]\n",
    "        W.remove(u)\n",
    "    return spread\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simpath(graph, k, r, l):\n",
    "    C = set(get_vertex_cover(graph))\n",
    "    V = set(graph.nodes)\n",
    "\n",
    "    V_C = V.difference(C)\n",
    "    # spread[x] is spd of S + x\n",
    "#     spread = np.zeros(graph.node_num + 1)\n",
    "    spread={}\n",
    "    for i in graph.nodes:\n",
    "        spread[i]=0\n",
    "#     spdV_ = np.ones((graph.node_num + 1, graph.node_num + 1))\n",
    "    spdV_={}\n",
    "    for i in graph.nodes:\n",
    "        dd={}\n",
    "        for j in graph.nodes:\n",
    "            dd[j]=0\n",
    "        spdV_[i]=dd\n",
    "        \n",
    "        \n",
    "    for u in C:\n",
    "        U = V_C.intersection(set(graph.get_parents(u)))\n",
    "        spread[u] = simpath_spread(set([u]), r, U, graph, spdV_)\n",
    "    for v in V_C:\n",
    "        v_children = graph.get_children(v)\n",
    "        for child in v_children:\n",
    "            spread[v] = spread[v] + spdV_[child][v] * graph.get_weight(v, child)\n",
    "        spread[v] = spread[v] + 1\n",
    "    celf = CELFQueue()\n",
    "    # put all nodes into celf queqe\n",
    "    # spread[v] is the marginal gain at this time\n",
    "    \n",
    "#     for node in range(1, graph.node_num + 1):\n",
    "#         celf.put(node, spread[node])\n",
    "    for node in graph.nodes:\n",
    "        celf.put(node, spread[node])\n",
    "    \n",
    "    S = set()\n",
    "    W = V\n",
    "    spd = 0\n",
    "    # mark the node that checked before during the same Si\n",
    "#     checked = np.zeros(graph.node_num + 1)\n",
    "    checked={}\n",
    "    for i in graph.nodes:\n",
    "        checked[i]=0\n",
    "\n",
    "    while len(S) < k:\n",
    "        U = celf.topn(l)\n",
    "#         spdW_ = np.ones((graph.node_num + 1, graph.node_num + 1))\n",
    "#         spdV_x = np.zeros(graph.node_num + 1)\n",
    "        \n",
    "        spdW_={}\n",
    "        spdV_x={}\n",
    "        for i in graph.nodes:\n",
    "            dd={}\n",
    "            spdV_x[i]=0\n",
    "            for j in graph.nodes:\n",
    "                dd[j]=1\n",
    "            spdW_[i]=dd\n",
    "        \n",
    "        simpath_spread(S, r, U, graph, spdW_=spdW_)\n",
    "        for x in U:\n",
    "            for s in S:\n",
    "                spdV_x[x] = spdV_x[x] + spdW_[s][x]\n",
    "        for x in U:\n",
    "            if checked[x] != 0:\n",
    "                S.add(x)\n",
    "                W = W.difference(set([x]))\n",
    "                spd = spread[x]\n",
    "                # print spread[x],simpath_spread(S,r,None,None)\n",
    "#                 checked = np.zeros(graph.node_num + 1)\n",
    "#                 for i in graph.nodes:\n",
    "#                     checked[i]=0\n",
    "                celf.remove(x)\n",
    "                break\n",
    "            else:\n",
    "                spread[x] = backtrack(x, r, W, None, None, graph) + spdV_x[x]\n",
    "                checked[x] = 1\n",
    "                celf.update(x, spread[x] - spd)\n",
    "    return S\n",
    "\n",
    "\n",
    "\n",
    "# node,edges,children,parents,nodenum,edge_num=read_graph_info('dolphins.txt') #Input dataset with 'total nodes' and 'total edges' in the 'first line'\n",
    "# graph = Graph(node,edges,children,parents,nodenum,edge_num)\n",
    "# seeds = simpath(graph, 3, 0.5, 4)\n",
    "# print(\"seed:\",seeds)\n",
    "\n",
    "\n",
    "\n",
    "def SIMPATH_setup(G):\n",
    "    parents = {}\n",
    "    children = {}\n",
    "    edges = {}\n",
    "    nodes = set()\n",
    "    node_num = len(G.nodes())\n",
    "    edge_num = len(G.edges())\n",
    "#     print(node_num,edge_num)\n",
    "    for src,des in G.edges():\n",
    "        nodes.add(src)\n",
    "        nodes.add(des)\n",
    "        if children.get(src) is None:\n",
    "            children[src] = []\n",
    "        if parents.get(des) is None:\n",
    "            parents[des] = []\n",
    "        weight=G[src][des]['weight']\n",
    "        edges[(src, des)] = weight\n",
    "        children[src].append(des)\n",
    "        parents[des].append(src)\n",
    "    return list(nodes), edges, children, parents, node_num, edge_num\n",
    "     \n",
    "def run_SIMPATH(G,k):\n",
    "    node,edges,children,parents,nodenum,edge_num=SIMPATH_setup(G)\n",
    "    graph = Graph(node,edges,children,parents,nodenum,edge_num)\n",
    "    seeds = simpath(graph, k, 0.5, k+1)\n",
    "    return list(seeds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vertex_cover(graph):\n",
    "#     dv = np.zeros(graph.node_num)\n",
    "    dv=dict()\n",
    "#     check_array = np.zeros((graph.node_num, graph.node_num))\n",
    "    check_array={}\n",
    "    for i in graph.nodes:\n",
    "        dd={}\n",
    "        for j in graph.nodes:\n",
    "            dd[j]=0\n",
    "        check_array[i]=dd\n",
    "        \n",
    "#     checked = 0\n",
    "    \n",
    "#     for i in range(graph.node_num):\n",
    "#         dv[i] =graph.get_out_degree(i + 1) + graph.get_in_degree(i + 1)\n",
    "    for i in graph.nodes:\n",
    "        dv[i]=graph.get_out_degree(i) + graph.get_in_degree(i)\n",
    "    \n",
    "    # V: Vertex cover\n",
    "    V = set()\n",
    "#     while checked < graph.edge_num:\n",
    "    for checked in graph.nodes:    \n",
    "#         s = dv.argmax() + 1\n",
    "        _,s=max(zip(dv.values(), dv.keys()))\n",
    "        V.add(s)\n",
    "        # make sure that never to select this node again\n",
    "        children = graph.get_children(s)\n",
    "        parents = graph.get_parents(s)\n",
    "        for child in children:\n",
    "            if check_array[s][child] == 0:\n",
    "                check_array[s][child] = 1\n",
    "#                 checked = checked + 1\n",
    "        for parent in parents:\n",
    "            if check_array[parent][s] == 0:\n",
    "                check_array[parent][s] = 1\n",
    "#                 checked = checked + 1\n",
    "        dv[s] = -1\n",
    "#     print(\"In Vertex conver V is:\",V)\n",
    "    return list(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Implementation of PMIA algorithm [1].\n",
    "[1] -- Scalable Influence Maximization for Prevalent Viral Marketing in Large-Scale Social Networks.\n",
    "'''\n",
    "\n",
    "\n",
    "def updateAP(ap, S, PMIIAv, PMIIA_MIPv, Ep):\n",
    "    ''' Assumption: PMIIAv is a directed tree, which is a subgraph of general G.\n",
    "    PMIIA_MIPv -- dictionary of MIP from nodes in PMIIA\n",
    "    PMIIAv is rooted at v.\n",
    "    '''\n",
    "    # going from leaves to root\n",
    "    sorted_MIPs = sorted(PMIIA_MIPv.items(), key = lambda MIP: len(MIP), reverse = True)\n",
    "#     print(\"Edges:\",PMIIAv.nodes)\n",
    "#     for e in PMIIAv.edges:\n",
    "#         print(e)\n",
    "#     for u,_ in sorted_MIPs:\n",
    "#         print(u,_)\n",
    "        \n",
    "        \n",
    "    for u, _ in sorted_MIPs:\n",
    "        if u in S:\n",
    "            ap[(u, PMIIAv)] = 1\n",
    "        elif not PMIIAv.in_edges([u]):\n",
    "#             print(\"\\n\\nGoin in elif\")\n",
    "            ap[(u, PMIIAv)] = 0\n",
    "        else:\n",
    "            in_edges = PMIIAv.in_edges([u], data=True)\n",
    "            prod = 1\n",
    "            for w, _, edata in in_edges:\n",
    "                # p = (1 - (1 - Ep[(w, u)])**edata[\"weight\"])\n",
    "                \n",
    "                print(\"In updateAP\")\n",
    "                print(\"hello:\",w,PMIIAv)\n",
    "                if (w,PMIIAv) in ap.keys():\n",
    "                    print(\"Key present\")\n",
    "                else:\n",
    "                    print(\"Key is NOT present\")\n",
    "#                 print(\"keys:\",\n",
    "    \n",
    "                p = Ep[(w,u)]\n",
    "                prod *= 1 - ap[(w, PMIIAv)]*p\n",
    "#             print(ap,u, PMIIAv)\n",
    "            ap[(u, PMIIAv)] = 1 - prod\n",
    "\n",
    "def updateAlpha(alpha, v, S, PMIIAv, PMIIA_MIPv, Ep, ap):\n",
    "    # going from root to leaves\n",
    "    sorted_MIPs =  sorted(PMIIA_MIPv.items(), key = lambda MIP: len(MIP))\n",
    "    for u, mip in sorted_MIPs:\n",
    "        if u == v:\n",
    "            alpha[(PMIIAv, u)] = 1\n",
    "        else:\n",
    "            out_edges = PMIIAv.out_edges([u])\n",
    "            assert len(out_edges) == 1, \"node u=%s must have exactly one neighbor, got %s instead\" %(u, len(out_edges))\n",
    "            out_edges=list(out_edges)\n",
    "#             print(\"out_edges:\",out_edges,type(out_edges))\n",
    "            \n",
    "            w = out_edges[0][1]\n",
    "            if w in S:\n",
    "                alpha[(PMIIAv, u)] = 0\n",
    "            else:\n",
    "                in_edges = PMIIAv.in_edges([w], data=True)\n",
    "                prod = 1\n",
    "                for up, _, edata in in_edges:\n",
    "                    if up != u:\n",
    "                        # pp_upw = 1 - (1 - Ep[(up, w)])**edata[\"weight\"]\n",
    "                        pp_upw = Ep[(up, w)]\n",
    "                        prod *= (1 - ap[up]*pp_upw)\n",
    "                # alpha[(PMIIAv, u)] = alpha[(PMIIAv, w)]*(1 - (1 - Ep[(u,w)])**PMIIAv[u][w][\"weight\"])*prod\n",
    "                alpha[(PMIIAv, u)] = alpha[(PMIIAv, w)]*(Ep[(u,w)])*prod\n",
    "\n",
    "def computePMIOA(G, u, theta, S, Ep):\n",
    "    '''\n",
    "     Compute PMIOA -- subgraph of G that's rooted at u.\n",
    "     Uses Dijkstra's algorithm until length of path doesn't exceed -log(theta)\n",
    "     or no more nodes can be reached.\n",
    "    '''\n",
    "    # initialize PMIOA\n",
    "    PMIOA = nx.DiGraph()\n",
    "    PMIOA.add_node(u)\n",
    "    PMIOA_MIP = {u: [u]} # MIP(u,v) for v in PMIOA\n",
    "\n",
    "    crossing_edges = set([out_edge for out_edge in G.out_edges([u]) if out_edge[1] not in S + [u]])\n",
    "    edge_weights = dict()\n",
    "    dist = {u: 0} # shortest paths from the root u\n",
    "\n",
    "    # grow PMIOA\n",
    "    while crossing_edges:\n",
    "        # Dijkstra's greedy criteria\n",
    "        min_dist = float(\"Inf\")\n",
    "        sorted_crossing_edges = sorted(crossing_edges) # to break ties consistently\n",
    "        for edge in sorted_crossing_edges:\n",
    "            if edge not in edge_weights:\n",
    "                # edge_weights[edge] = -math.log(1 - (1 - Ep[edge])**G[edge[0]][edge[1]][\"weight\"])\n",
    "                edge_weights[edge] = -math.log(Ep[edge])\n",
    "            edge_weight = edge_weights[edge]\n",
    "            if dist[edge[0]] + edge_weight < min_dist:\n",
    "                min_dist = dist[edge[0]] + edge_weight\n",
    "                min_edge = edge\n",
    "        # check stopping criteria\n",
    "        if min_dist < -math.log(theta):\n",
    "            dist[min_edge[1]] = min_dist\n",
    "            # PMIOA.add_edge(min_edge[0], min_edge[1], {\"weight\": G[min_edge[0]][min_edge[1]][\"weight\"]})\n",
    "            PMIOA.add_edge(min_edge[0], min_edge[1])\n",
    "            PMIOA_MIP[min_edge[1]] = PMIOA_MIP[min_edge[0]] + [min_edge[1]]\n",
    "            # update crossing edges\n",
    "            crossing_edges.difference_update(G.in_edges(min_edge[1]))\n",
    "            crossing_edges.update([out_edge for out_edge in G.out_edges(min_edge[1])\n",
    "                                   if (out_edge[1] not in PMIOA) and (out_edge[1] not in S)])\n",
    "        else:\n",
    "            break\n",
    "    return PMIOA, PMIOA_MIP\n",
    "\n",
    "def updateIS(IS, S, u, PMIOA, PMIIA):\n",
    "    for v in PMIOA[u]:\n",
    "        for si in S:\n",
    "            # if seed node is effective and it's blocked by u\n",
    "            # then it becomes ineffective\n",
    "            if (si in PMIIA[v]) and (si not in IS[v]) and (u in PMIIA[v][si]):\n",
    "                    IS[v].append(si)\n",
    "\n",
    "def computePMIIA(G, ISv, v, theta, S, Ep):\n",
    "\n",
    "    # initialize PMIIA\n",
    "    PMIIA = nx.DiGraph()\n",
    "    PMIIA.add_node(v)\n",
    "    PMIIA_MIP = {v: [v]} # MIP(u,v) for u in PMIIA\n",
    "\n",
    "    crossing_edges = set([in_edge for in_edge in G.in_edges([v]) if in_edge[0] not in ISv + [v]])\n",
    "    edge_weights = dict()\n",
    "    dist = {v: 0} # shortest paths from the root u\n",
    "\n",
    "    # grow PMIIA\n",
    "    while crossing_edges:\n",
    "        # Dijkstra's greedy criteria\n",
    "        min_dist = float(\"Inf\")\n",
    "        sorted_crossing_edges = sorted(crossing_edges) # to break ties consistently\n",
    "        for edge in sorted_crossing_edges:\n",
    "            if edge not in edge_weights:\n",
    "                # edge_weights[edge] = -math.log(1 - (1 - Ep[edge])**G[edge[0]][edge[1]][\"weight\"])\n",
    "#                 edgevalue=Ep[edge]\n",
    "#                 print(\"Ep[edge]:\",Ep[edge],edgevalue)\n",
    "                edge_weights[edge] = -(math.log(Ep[edge]))\n",
    "#                 edge_weights[edge] = -(math.log(edgevalue))\n",
    "                \n",
    "            edge_weight = edge_weights[edge]\n",
    "            if dist[edge[1]] + edge_weight < min_dist:\n",
    "                min_dist = dist[edge[1]] + edge_weight\n",
    "                min_edge = edge\n",
    "        # check stopping criteria\n",
    "        # print min_edge, ':', min_dist, '-->', -math.log(theta)\n",
    "        if min_dist < -math.log(theta):\n",
    "            dist[min_edge[0]] = min_dist\n",
    "            # PMIIA.add_edge(min_edge[0], min_edge[1], {\"weight\": G[min_edge[0]][min_edge[1]][\"weight\"]})\n",
    "            PMIIA.add_edge(min_edge[0], min_edge[1])\n",
    "            PMIIA_MIP[min_edge[0]] = PMIIA_MIP[min_edge[1]] + [min_edge[0]]\n",
    "            # update crossing edges\n",
    "            crossing_edges.difference_update(G.out_edges(min_edge[0]))\n",
    "            if min_edge[0] not in S:\n",
    "                crossing_edges.update([in_edge for in_edge in G.in_edges(min_edge[0])\n",
    "                                       if (in_edge[0] not in PMIIA) and (in_edge[0] not in ISv)])\n",
    "        else:\n",
    "            break\n",
    "    return PMIIA, PMIIA_MIP\n",
    "\n",
    "def PMIA(G, k, theta, Ep):\n",
    "    start = time.time()\n",
    "    # initialization\n",
    "    S = []\n",
    "    IncInf = dict(zip(G.nodes(), [0]*len(G)))\n",
    "    PMIIA = dict() # node to tree\n",
    "    PMIOA = dict()\n",
    "    PMIIA_MIP = dict() # node to MIPs (dict)\n",
    "    PMIOA_MIP = dict()\n",
    "    ap = dict()\n",
    "    alpha = dict()\n",
    "    IS = dict()\n",
    "    for v in G:\n",
    "        IS[v] = []\n",
    "        PMIIA[v], PMIIA_MIP[v] = computePMIIA(G, IS[v], v, theta, S, Ep)\n",
    "        for u in PMIIA[v]:\n",
    "            ap[u] = 0 # ap of u node in PMIIA[v]\n",
    "        updateAlpha(alpha, v, S, PMIIA[v], PMIIA_MIP[v], Ep, ap)\n",
    "        for u in PMIIA[v]:\n",
    "            IncInf[u] += alpha[(PMIIA[v], u)]*(1 - ap[u])\n",
    "#     print('Finished initialization')\n",
    "#     print(time.time() - start)\n",
    "\n",
    "    # main loop\n",
    "    for i in range(k):\n",
    "#         print(IncInf)\n",
    "#         u, _ = max(IncInf.items(), key = lambda dk, dv: dv)\n",
    "        _,u=max(zip(IncInf.values(), IncInf.keys()))\n",
    "        IncInf.pop(u) # exclude node u for next iterations\n",
    "        PMIOA[u], PMIOA_MIP[u] = computePMIOA(G, u, theta, S, Ep)\n",
    "        for v in PMIOA[u]:\n",
    "            for w in PMIIA[v]:\n",
    "                if w not in S + [u]:\n",
    "                    IncInf[w] -= alpha[(PMIIA[v],w)]*(1 - ap[w])\n",
    "\n",
    "        updateIS(IS, S, u, PMIOA_MIP, PMIIA_MIP)\n",
    "\n",
    "        S.append(u)\n",
    "\n",
    "        for v in PMIOA[u]:\n",
    "            if v != u:\n",
    "                PMIIA[v], PMIIA_MIP[v] = computePMIIA(G, IS[v], v, theta, S, Ep)\n",
    "                \n",
    "                \n",
    "                for uu in PMIIA[v].nodes:\n",
    "                    if uu in S:\n",
    "#                         print(\"IN s\")\n",
    "                        ap[u] = 1\n",
    "                    elif not PMIIA[v].in_edges([uu]):\n",
    "#                         print(\"Second else if\")\n",
    "            #             print(\"\\n\\nGoin in elif\")\n",
    "                        ap[uu] = 0\n",
    "                    else:\n",
    "                        in_edges = PMIIA[v].in_edges([uu], data=True)\n",
    "                        prod = 1\n",
    "#                         print(\"inEdgessss:\",in_edges)\n",
    "                        for w, _, edata in in_edges:\n",
    "                            # p = (1 - (1 - Ep[(w, u)])**edata[\"weight\"]\n",
    "                            p = Ep[(w,uu)]\n",
    "                            prod *= 1 - ap[w]*p\n",
    "                        ap[uu] = 1 - prod\n",
    "                \n",
    "#                 updateAP(ap, S, PMIIA[v], PMIIA_MIP[v], Ep)\n",
    "                updateAlpha(alpha, v, S, PMIIA[v], PMIIA_MIP[v], Ep, ap)\n",
    "                # add new incremental influence\n",
    "                for w in PMIIA[v]:\n",
    "                    if w not in S:\n",
    "                        IncInf[w] += alpha[(PMIIA[v], w)]*(1 - ap[w])\n",
    "\n",
    "    return S\n",
    "\n",
    "def getCoverage(G, S, Ep):\n",
    "    return IC(G, S)\n",
    "\n",
    "\n",
    "def run_PMIA(GG,k):\n",
    "    Ep = dict()\n",
    "    G=nx.DiGraph()\n",
    "    for edgee in GG.edges():\n",
    "        s=edgee[0]\n",
    "        t=edgee[1]\n",
    "        Ep[(int(s), int(t))] = GG[s][t]['weight']\n",
    "        G.add_edge(s,t,weight=GG[s][t]['weight'])   \n",
    "        G.nodes[s]['thres']=(G.degree(s)/2)\n",
    "        G.nodes[t]['thres']=(G.degree(t)/2)    \n",
    "    theta = 1.0/20\n",
    "    S = PMIA(G, k, theta, Ep)\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Greedy(G,k):\n",
    "    print(\"in Greedy\")\n",
    "    Dict={}\n",
    "    mySet1=[]\n",
    "    V=G.nodes()\n",
    "    mySet1.clear()\n",
    "    for i in range(k):\n",
    "        for v in (V-mySet1):\n",
    "            mySet1.append(v)\n",
    "            a=linear_Threshold(G,mySet1)\n",
    "            Dict[v]=len(a)#influence as value and current node as key\n",
    "            mySet1.remove(v)#remove crrent node from mySet for rest nodes to go for IC\n",
    "        Keymax = max(zip(Dict.values(), Dict.keys()))[1]# finding node with max influence\n",
    "        Dict.clear()\n",
    "        mySet1.append(Keymax)\n",
    "#     print(\"Final seed set is:\",mySet1,compute_Phi(G,mySet1,comm,k))\n",
    "    return list(mySet1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "j86wDWWIP_YB"
   },
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "  file1 = open(path,'r')\n",
    "  sender = list()\n",
    "  receiver = list()\n",
    "\n",
    "  for i in file1.readlines():\n",
    "    sender.append(int(i.split(' ')[0]))\n",
    "    receiver.append(int(i.split(' ')[1].split('\\n')[0]))\n",
    "    \n",
    "  df = pd.DataFrame(list(zip(sender,receiver)),columns =['source', 'target'])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmltotxt(filename):\n",
    "    import networkx as nx\n",
    "    import pandas as pd\n",
    "    g = nx.read_gml('airlines.gml')\n",
    "    nx.write_edgelist(g, 'edgelistFile.csv', delimiter=',')\n",
    "    df = pd.read_csv('edgelistFile.csv')\n",
    "    file = open(\"myfile.txt\",\"w\")\n",
    "    for i in range(len(df)):\n",
    "        x=df.iloc[i][0]\n",
    "        y=df.iloc[i][1]\n",
    "        file.write(str(x)+\" \"+str(y)+\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcand(G,k, comm):#Take df and  all new nodes as input and return a seed node.\n",
    "    s=[]\n",
    "    s=GreedyDiv(G,k, comm)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fhJ1sm7yt_nD"
   },
   "outputs": [],
   "source": [
    "def GreedyDiv(G,k, comm):\n",
    "    Dict={}\n",
    "    mySet1=[]\n",
    "    V=G.nodes()\n",
    "    mySet1.clear()\n",
    "#     print(\"Community:\",comm,\"k:\",k)\n",
    "    for i in range(k):\n",
    "#         print(\"myset:\",mySet1)\n",
    "        for v in (V-mySet1):\n",
    "#             print(\"v,i:\",v,i)\n",
    "            mySet1.append(v)\n",
    "#             print(\"myset:\",mySet1)\n",
    "            a=compute_Phi(G,mySet1,comm,i+1)\n",
    "#             print(\"Phi of \",mySet1,\" is :\",a)\n",
    "            Dict[v]=a#influence as value and current node as key\n",
    "            mySet1.remove(v)#remove crrent node from mySet for rest nodes to go for IC\n",
    "               #print(Dict)\n",
    "        Keymax = max(zip(Dict.values(), Dict.keys()))[1]# finding node with max influence\n",
    "        Dict.clear()\n",
    "        mySet1.append(Keymax)\n",
    "#     print(\"Final seed set is:\",mySet1,compute_Phi(G,mySet1,comm,k))\n",
    "    return list(mySet1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZn0MKQet_nE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ER71qO5zt_nE"
   },
   "outputs": [],
   "source": [
    "def linear_Threshold(graph, seeds):\n",
    "    influnces = seeds[:]\n",
    "    queue = influnces[:]\n",
    "    pre_node_record = defaultdict(float) \n",
    "#     print(\"Queue:\",queue)\n",
    "#     print(\"Influences:\",influnces)\n",
    "    while len(queue) != 0:\n",
    "        node = queue.pop(0)\n",
    "#         print(\"----------------------------------------------------------------------\")\n",
    "#         print(\"Take node:\",node)\n",
    "#         print(\"Neighbour:\",graph[node])\n",
    "        for element in graph[node]:\n",
    "            if element not in influnces:\n",
    "#                 print(\"Element:\",element,\"prerecored\",pre_node_record[element])\n",
    "                pre_node_record[element] = pre_node_record[element] + graph[node][element]['weight'] \n",
    "#                 print(pre_node_record[element])\n",
    "                if pre_node_record[element] >= graph.nodes[element]['thres']:\n",
    "#                     print(\">>>>>>>>>>>>>>>>>>node influeced:\",element)\n",
    "                    influnces.append(element)\n",
    "                    queue.append(element)\n",
    "#     influnce_num = len(influnces)\n",
    "#     print(\"Seed set:\",seeds,\"Activated nodes:\",influnces)\n",
    "    return influnces\n",
    "# linear_Threshold(GG,[45,29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communityDiversityFunction(G, S, communities):\n",
    "\n",
    "    activated = linear_Threshold(G, S)\n",
    "    noofcommunity=0\n",
    "    outersum=0\n",
    "    for com in communities:\n",
    "        innersum=0\n",
    "        for rvj in com:\n",
    "            if rvj in activated:\n",
    "                innersum=innersum+1\n",
    "        outersum=outersum+math.sqrt(innersum)\n",
    "            \n",
    "#         if any(x in activated for x in com):\n",
    "#             noofcommunity=noofcommunity+1\n",
    "    \n",
    "    \n",
    "    return outersum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Phi(G, S, communities, k):\n",
    "    if len(S)<1:\n",
    "        return 0\n",
    "    lambda_G = 0.5\n",
    "    \n",
    "    v_length = G.number_of_nodes()\n",
    "    diversity_V = communityDiversityFunction(G, list(G.nodes), communities)\n",
    "\n",
    "    IC_S = linear_Threshold(G, S)\n",
    "    activated_set_S_length = len(IC_S) \n",
    "    diversity_activated_set_S = communityDiversityFunction(G, IC_S, communities)\n",
    "    phi_S = ((1 - lambda_G)* (activated_set_S_length/v_length)) + (lambda_G * (diversity_activated_set_S/diversity_V))\n",
    "    return phi_S\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# obj=TBCD_mine()\n",
    "# perc=[20,40,60,80]\n",
    "# filename='200_0'\n",
    "# print(\"hello\")\n",
    "# obj.execute_TBCD_txt(filename,perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BocmOfr-H5po"
   },
   "outputs": [],
   "source": [
    "def findk(i):#dynamic calculation of k according the percentage of current dataset\n",
    "  k=(0.03*i)\n",
    "  if(i==0):\n",
    "    k=1\n",
    "  if(k>int(k)):\n",
    "   k=int(k)+1\n",
    "  return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmG7U7uqu8kV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayresult(result,itr,perc):\n",
    "    print(\"\\n\\n\\n\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "    linestylee=['dashdot','dashed','dotted','-', '--', ':','-.' ]\n",
    "    \n",
    "    markerss = ['o','v','s','*','+','x','D','d','X','P']\n",
    "#     descriptions = ['circle', 'triangle_down','square','star', 'plus','x','diamond', 'thin_diamond','x (filled)','plus (filled)']\n",
    "    communitiesITR=[]\n",
    "    Name=[]\n",
    "    activatednodesITR=[]\n",
    "    totalcomm=[]\n",
    "#     print(itr,result)\n",
    "    \n",
    "    for i in result:\n",
    "        for j in range(len(result[i])):\n",
    "            Name.append(result[i][j]['Name'])\n",
    "        break\n",
    "#     print(Name)\n",
    "    \n",
    "#     for i in result:\n",
    "# #         print(result[i])\n",
    "#         for j in range(len(result[i])):\n",
    "#             totalcomm.append(result[i][j]['Total communitites'])\n",
    "#             break\n",
    "#     print(\"Total communitites:\",totalcomm)\n",
    "    \n",
    "    for i in result:\n",
    "        active=[]\n",
    "        comm=[]\n",
    "        for j in range(len(result[i])):\n",
    "            active.append(result[i][j]['length of activated nodes'])\n",
    "            comm.append(result[i][j]['number of communities'])\n",
    "        activatednodesITR.append(active)\n",
    "        communitiesITR.append(comm)\n",
    "#     print(\"Activated nodes:\",activatednodesITR)\n",
    "#     print(\"Communities:\",communitiesITR)\n",
    "        \n",
    "    \n",
    "        \n",
    "#     print(\"hello\");\n",
    "    \n",
    "    plt.xlabel(\"Percentage\")\n",
    "    plt.ylabel(\"Activated nodes\")\n",
    "    plt.title(\"Activated nodes - LFR_1000_0.0 - Greedy - lemda=0.5 - Degree\")\n",
    "    for i in range(len(activatednodesITR[0])):\n",
    "        plt.plot(perc,[pt[i] for pt in activatednodesITR],label = '%s'%Name[i],linestyle='%s'%linestylee[i],marker='%s'%markerss[i])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.xlabel(\"Percentage\")\n",
    "    plt.ylabel(\"No of community\")\n",
    "    plt.title(\"Communitites - LFR_1000_0.0 - Greedy - lemda=0.5 - Degree\")\n",
    "    for i in range(len(communitiesITR[0])):\n",
    "        plt.plot(perc,[pt[i] for pt in communitiesITR],label = '%s'%Name[i],linestyle='%s'%linestylee[i],marker='%s'%markerss[i])\n",
    "#     plt.plot(perc,totalcomm,label='Total community',linestyle='%s'%linestylee[-1],marker='%s'%markerss[-1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findcommunity(G,seedset,k, comm,algoname):\n",
    "    print (\"Communities formed: \",len(comm))\n",
    "    activated = linear_Threshold(G, seedset)\n",
    "    noofcommunity=0\n",
    "    for com in comm:\n",
    "        if any(x in activated for x in com):\n",
    "            noofcommunity=noofcommunity+1\n",
    "    print(\"Community we got:\",noofcommunity)\n",
    "    upperBound_dict = {\n",
    "        'Name':algoname,\n",
    "        'k_nodes': k,\n",
    "#         'Total communitites':len(comm),\n",
    "        'number of communities':noofcommunity,\n",
    "        'length of activated nodes': len(activated),\n",
    "        \n",
    "        # 'length of communities': len(community_df['Unnamed: 1'].unique())\n",
    "    }\n",
    "    print(upperBound_dict)\n",
    "    return upperBound_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findresult(G,k,comm_set_final):\n",
    "    #TBCD\n",
    "    print('\\n\\n--------------- CDIM -------------------')\n",
    "    R_seed = [getcand(G,int(k),comm_set_final)]\n",
    "    print(\"CDIM:\",R_seed)\n",
    "    R_seed = sum(R_seed, [])\n",
    "    CDIMresult=findcommunity(G,R_seed,k, comm_set_final,'CDIM')\n",
    "    \n",
    "\n",
    "    \n",
    "#     Greedy\n",
    "#     st=time.time()\n",
    "#     GreedyseedSet =Greedy(G,int(k))\n",
    "#     Greedytime=time.time()-st\n",
    "#     print('\\n\\n--------------- Greedy -------------------')\n",
    "#     Greedyresult=findcommunity(G,GreedyseedSet,k, comm_set_final,'Greedy')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #Single Degree Discount---Not Defined\n",
    "#     st=time.time()\n",
    "#     SingleDegreeSeedset = single_degree_discount(G,int(k))\n",
    "#     SingleDegreetime=time.time()-st\n",
    "#     print('\\n\\n--------------- Single Degree Discount -------------------')\n",
    "#     SingleDegreeresult=findcommunity(G,SingleDegreeSeedset,k, comm_set_final,'SingleDegreeDiscount')\n",
    "    \n",
    "    \n",
    "    #CELF\n",
    "#     st=time.time()\n",
    "#     CELFseedSet = celf(G,int(k))\n",
    "#     CELFtime=time.time()-st\n",
    "#     print('\\n\\n--------------- CELF -------------------')\n",
    "#     CELFresult=findcommunity(G,CELFseedSet,k, comm_set_final,'CELF')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PMIA\n",
    "#     st=time.time()\n",
    "#     PMIAseedSet = run_PMIA(G,int(k))\n",
    "#     PMIAtime=time.time()-st\n",
    "#     print('\\n\\n--------------- PMIA -------------------')\n",
    "#     PMIAresult=findcommunity(G,PMIAseedSet,k, comm_set_final,'PMIA')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #SIMPATH\n",
    "#     st=time.time()\n",
    "#     SIMPATHseedSet = run_SIMPATH(G,int(k))\n",
    "#     SIMPATHtime=time.time()-st\n",
    "#     print('\\n\\n--------------- SIMPATH -------------------')\n",
    "#     SIMPATHresult=findcommunity(G,SIMPATHseedSet,k, comm_set_final,'SIMPATH')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PAGERANK\n",
    "#     st=time.time()\n",
    "#     pagerank = sorted(nx.pagerank(G).items(), key=lambda x: x[1], reverse=True)\n",
    "#     pagerank_seed = [node for node, value in pagerank[0:k]]\n",
    "#     PRtime=time.time()-st    \n",
    "#     print('\\n\\n--------------- PAGERANK -------------------')\n",
    "#     Pagerankresult=findcommunity(G,pagerank_seed,k, comm_set_final,'PageRank')\n",
    "\n",
    "#     DEGREE\n",
    "    st=time.time()\n",
    "    degree = sorted(G.degree(), key=lambda x: x[1], reverse=True)\n",
    "    degree_seed = [node for node, value in degree[0:k]]\n",
    "    Dtime=time.time()-st\n",
    "    print('\\n\\n--------------- DEGREE -------------------')\n",
    "    Degreeresult=findcommunity(G,degree_seed,k, comm_set_final,'Degree')\n",
    "\n",
    "\n",
    "#     #HUB\n",
    "#     st=time.time()\n",
    "#     hub, authority = nx.hits(G)     \n",
    "#     hub  = sorted(hub.items(), key=lambda x: x[1], reverse=True)\n",
    "#     hub_seed = [node for node, value in hub[0:k]]\n",
    "#     Hubtime=time.time()-st\n",
    "#     print('\\n\\n--------------- HUB -------------------')\n",
    "#     Hubresult=findcommunity(G,hub_seed,k, comm_set_final,'Hub')\n",
    "\n",
    "#     #AUTHORITY\n",
    "#     st=time.time()\n",
    "#     authority  = sorted(authority.items(), key=lambda x: x[1], reverse=True)\n",
    "#     authority_seed = [node for node, value in authority[0:k]]\n",
    "#     Atime=time.time()-st\n",
    "#     print('\\n\\n--------------- AUTHORITY -------------------')\n",
    "#     Authorityresult=findcommunity(G,authority_seed,k, comm_set_final,'Authority')\n",
    "\n",
    "#     #NBKCORE\n",
    "#     st=time.time()\n",
    "#     # neighborhood coreness\n",
    "#     node2nbcore =  {node: np.sum([G.degree(nb) for nb in G.neighbors(node)]) for node in G.nodes() }\n",
    "#     nbkcore = sorted(node2nbcore.items(), key=lambda x: x[1], reverse=True)\n",
    "#     nbkcore_seed = [node for node, value in nbkcore[0:k]]\n",
    "#     N2time=time.time()-st\n",
    "#     print('\\n\\n--------------- neighborhood coreness -------------------')\n",
    "#     Neighbourresult=findcommunity(G,nbkcore_seed,k, comm_set_final,'Neighbourhood')\n",
    "    \n",
    "\n",
    "#     resultt=[DDR,Greedyresult,CELFresult,Pagerankresult,Degreeresult,Hubresult,Authorityresult,Neighbourresult]\n",
    "#     resultt=[DDR,SingleDegreeresult,CELFresult,CELFPPresult,Pagerankresult,Degreeresult,Hubresult,Authorityresult,Neighbourresult]\n",
    "#     resultt=[DDR,Greedyresult,CELFresult,Pagerankresult,Degreeresult,Hubresult,Authorityresult,Neighbourresult]\n",
    "\n",
    "#     resultt=[TBCDresult,Greedyresult,CELFresult,Pagerankresult,PMIAresult,SIMPATHresult,Degreeresult]\n",
    "#     resultt=[TBCDresult,CELFresult,Pagerankresult,PMIAresult,SIMPATHresult,Degreeresult]\n",
    "    resultt=[CDIMresult,Degreeresult]\n",
    "\n",
    "    return resultt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def findresult(G,k,comm_set_final):\n",
    "    \n",
    "#     R_seed = [getcand(G,int(k),comm_set_final)]\n",
    "\n",
    "# #     G_seed = [Greeddy(G,int(k))]\n",
    "    \n",
    "    \n",
    "#     pagerank = sorted(nx.pagerank(G).items(), key=lambda x: x[1], reverse=True)\n",
    "#     pagerank_seed = [node for node, value in pagerank[0:k]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     degree = sorted(G.degree(), key=lambda x: x[1], reverse=True)\n",
    "#     degree_seed = [node for node, value in degree[0:k]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     hub, authority = nx.hits(G)     \n",
    "#     hub  = sorted(hub.items(), key=lambda x: x[1], reverse=True)\n",
    "#     hub_seed = [node for node, value in hub[0:k]]\n",
    "\n",
    "#     authority  = sorted(authority.items(), key=lambda x: x[1], reverse=True)\n",
    "#     authority_seed = [node for node, value in authority[0:k]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #     kcore = sorted(nx.core_number(G).items(), key=lambda x: x[1], reverse=True)\n",
    "# #     kcore_seed = [node for node, value in kcore[0:k]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # neighborhood coreness\n",
    "#     node2nbcore =  {node: np.sum([G.degree(nb) for nb in G.neighbors(node)]) for node in G.nodes() }\n",
    "#     nbkcore = sorted(node2nbcore.items(), key=lambda x: x[1], reverse=True)\n",
    "#     nbkcore_seed = [node for node, value in nbkcore[0:k]]\n",
    "\n",
    "    \n",
    "# #     print('\\n\\n--------------- Greedy -------------------')\n",
    "# #     print(\"Greedy:\",G_seed)\n",
    "# #     G_seed = sum(G_seed, [])\n",
    "# #     result=findcommunity(G,G_seed,k, comm_set_final)\n",
    "    \n",
    "#     print('\\n\\n--------------- TBCD -------------------')\n",
    "#     print(\"TBCD:\",R_seed)\n",
    "#     R_seed = sum(R_seed, [])\n",
    "#     TBCDresult=findcommunity(G,R_seed,k, comm_set_final,'TBCD')\n",
    "\n",
    "\n",
    "#     print('\\n\\n--------------- PAGERANK -------------------')\n",
    "#     print(\"pagerank:\",pagerank_seed)\n",
    "#     Pagerankresult=findcommunity(G,pagerank_seed,k, comm_set_final,'PAGERank')\n",
    "\n",
    "\n",
    "\n",
    "#     print('\\n\\n--------------- DEGREE -------------------')\n",
    "#     print(\"degree:\",degree_seed)\n",
    "#     Degreeresult=findcommunity(G,degree_seed,k, comm_set_final,'Degree')\n",
    "\n",
    "\n",
    "#     print('\\n\\n--------------- HUB -------------------')\n",
    "#     print(\"hub:\",hub_seed)\n",
    "#     Hubresult=findcommunity(G,hub_seed,k, comm_set_final,'Hub')\n",
    "\n",
    "\n",
    "\n",
    "#     print('\\n\\n--------------- AUTHORITY -------------------')\n",
    "#     print(\"authority:\",authority_seed)\n",
    "#     Authorityresult=findcommunity(G,authority_seed,k, comm_set_final,'Authority')\n",
    "\n",
    "\n",
    "#     print('\\n\\n--------------- KCORE -------------------')\n",
    "# #     print(\"kcore:\",kcore_seed)\n",
    "# #     result=findcommunity(G,kcore_seed,k, comm_set_final)\n",
    "\n",
    "\n",
    "#     print('\\n\\n--------------- neighborhood coreness -------------------')\n",
    "#     print(\"neighborhood-coreness:\",nbkcore_seed)\n",
    "#     Neighbourresult=findcommunity(G,nbkcore_seed,k, comm_set_final,'Neighbourhood')\n",
    "\n",
    "    \n",
    "\n",
    "#     print(\"\\n\\n##############################################################################\")\n",
    "#     resultt=[TBCDresult,Pagerankresult,Degreeresult,Hubresult,Authorityresult,Neighbourresult]\n",
    "#     return resultt\n",
    "    \n",
    "# #         return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeExcel(result,itr,perc):\n",
    "    timeITR=[]\n",
    "    Name=[]\n",
    "    activatednodesITR=[]\n",
    "    for i in result:\n",
    "        for j in range(len(result[i])):\n",
    "            Name.append(result[i][j]['Name'])\n",
    "        break\n",
    "    for i in result:\n",
    "        active=[]\n",
    "        timee=[]\n",
    "        for j in range(len(result[i])):\n",
    "            active.append(result[i][j]['length of activated nodes'])\n",
    "            timee.append(result[i][j]['number of communities'])\n",
    "        activatednodesITR.append(active)\n",
    "        timeITR.append(timee)\n",
    "    df=pd.DataFrame()\n",
    "    time=\"Community\"\n",
    "    for i in range(len(timeITR)):\n",
    "        timestr=time+\"_\"+str(itr[i])+\"_\"+str(perc[i])+\"%\"\n",
    "        df[timestr]=timeITR[i]\n",
    "#     print(\"After name\",df)\n",
    "    ICnodes=\"Activated_Nodes\"\n",
    "    for i in range(len(activatednodesITR)):\n",
    "        ICnodesstr=ICnodes+\"_\"+str(itr[i])+\"_\"+str(perc[i])+\"%\"\n",
    "        df[ICnodesstr]=activatednodesITR[i]\n",
    "    df.insert(0,\"Name of Algorithm\",Name)\n",
    "    return df\n",
    "\n",
    "# upperBound_dict = {\n",
    "#         'Name':algoname,\n",
    "#         'k_nodes': k,\n",
    "#         'Total communitites':len(comm),\n",
    "#         'number of communities':noofcommunity,\n",
    "#         'length of activated nodes': len(activated),\n",
    "        \n",
    "#         # 'length of communities': len(community_df['Unnamed: 1'].unique())\n",
    "#     }\n",
    "# result_df=pd.DataFrame()\n",
    "# for i in range(1):\n",
    "#     obj=TBCD_mine()\n",
    "#     perc=[10,20,30,40,50,60,70,80,90,100]\n",
    "#     filename='sample'\n",
    "#     dff=obj.execute_TBCD_txt(filename,perc)\n",
    "#     dff2=result_df\n",
    "#     result_df=dff\n",
    "#     result_df = dff2.append(dff,ignore_index = True)\n",
    "#     result_df.to_excel('Outputt.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "FUYNIegxE4lr"
   },
   "outputs": [],
   "source": [
    "class TBCD_mine(object):\n",
    "    \"\"\"\n",
    "        tree based community detection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename=None, g=nx.Graph(), ttl=float('inf'), obs=7, path=\"\",\n",
    "                 start=None, end=None, level_max = 6, window_size = 20, theta = 0.5,\n",
    "                 dataset = None, edge_list = None, comm_list = None,window_ratio = 0.10):\n",
    "        \"\"\"\n",
    "            Constructor\n",
    "            :param g: networkx graph\n",
    "            :param ttl: edge time to live (days)\n",
    "            :param obs: observation window (days)\n",
    "            :param path: Path where generate the results and find the edge file\n",
    "            :param start: starting date\n",
    "            :param end: ending date\n",
    "        \"\"\"\n",
    "        print(\"initialization\")\n",
    "        self.path = path\n",
    "        self.graph = g\n",
    "        self.removed = 0\n",
    "        self.added = 0\n",
    "        self.filename = filename\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.obs = obs\n",
    "        self.communities = {}\n",
    "        self.intra_conn = {}\n",
    "        self.inter_conn = {}\n",
    "        self.select = {}\n",
    "        self.cluster_head = set()\n",
    "        self.cluster_h = {}\n",
    "        self.level = {}\n",
    "        self.occurence = {}\n",
    "        self.level_max = level_max\n",
    "        self.w_temp = 0\n",
    "        self.window_size = window_size\n",
    "        self.theta = theta\n",
    "        self.dataset = \"\"\n",
    "        self.edge_list = \"\"\n",
    "        self.comm_list = \"\"\n",
    "        self.ground_truth_comm = {}\n",
    "        self.precision = -1\n",
    "        self.recall = -1\n",
    "        self.nmi = -1\n",
    "        self.f_measure = -1\n",
    "        self.purity = -1\n",
    "        self.ari = -1\n",
    "        self.entropy = -1\n",
    "        self.modularity = -1\n",
    "        self.coverage = -1\n",
    "        self.external_density = -1\n",
    "        self.average_isolability = -1\n",
    "        self.mat_file_adj = \"\"\n",
    "        self.mat_file_label = \"\"\n",
    "        self.row = 1\n",
    "        self.result_array = []\n",
    "        self.window_ratio = window_ratio\n",
    "\n",
    "\n",
    "    def detachability(self,label):\n",
    "\n",
    "#         print(\"calculating detachability for label = \"+str(label))\n",
    "        G = self.graph\n",
    "        internal = 0\n",
    "        external = 0\n",
    "        DZ = 0\n",
    "        '''cluster_h_set = set()\n",
    "        cluster_head_set = set()\n",
    "        for node in G :\n",
    "            cluster_h_set.add(self.cluster_h[node])\n",
    "            cluster_head_set.add(self.cluster_head[node])\n",
    "        count_h = len(cluster_h_set)\n",
    "        count_head = len(cluster_head_set)\n",
    "        print(\"\\n\\n\\n count h = \"+str(count_h))\n",
    "        print(\"\\n\\n\\n count head = \" + str(count_head))'''\n",
    "        # node and node neighbour only taken into account\n",
    "        for node in G:\n",
    "            if self.cluster_h[node] == label:\n",
    "                for node_neighbour in G.neighbors(node):\n",
    "                    if self.cluster_h[node_neighbour] == label:\n",
    "                        internal = internal + G[node][node_neighbour]['weight']\n",
    "                    else:\n",
    "                        external = external + G[node][node_neighbour]['weight']\n",
    "                '''internal += self.intra_conn[node]\n",
    "                external += self.inter_conn[node]'''\n",
    "        if internal + external != 0:\n",
    "            DZ = internal / (internal + external)\n",
    "#         print(\"detachbility = \"+str(DZ))\n",
    "        return DZ\n",
    "\n",
    "    def max_comm_label(self,node):\n",
    "        #removed influence part\n",
    "        G = self.graph\n",
    "        all_labels = set()\n",
    "        # print(\"initially for node \"+str(node)+\" label is \"+str(var_dict[node]))\n",
    "        for node_neighbour in G.neighbors(node):\n",
    "            all_labels.add(self.cluster_h[node_neighbour])\n",
    "        prob_actual = 1\n",
    "        label_actual = self.cluster_h[node]\n",
    "        for label in all_labels:\n",
    "            # print(\"for label \"+str(label))\n",
    "            '''prob_new = 1\n",
    "            for node_chk in G.neighbors(node):\n",
    "                # print(\"u is-\"+str(u)+\" v is-\"+str(v))\n",
    "                if self.cluster_h[node_chk] == label:\n",
    "                    # print(\"prob_new = \"+str(prob_new)+\" edge weight \"+str(G[node][node_chk]['weight']))\n",
    "                    chk = 0\n",
    "                    if G.has_edge(node, node_chk):\n",
    "                        chk = G[node][node_chk]['weight']\n",
    "                    if var_dict['influence'][node][node_chk] == 1:\n",
    "                        # print(\"influence and edge weight true for \"+str(node)+\"-\"+str(node_chk))\n",
    "                        prob_new = prob_new * (1 - chk)\n",
    "            if prob_new < prob_actual:\n",
    "                prob_actual = prob_new\n",
    "                label_actual = label\n",
    "                self.cluster_h[node] = label'''\n",
    "        # print(\"after max_comm_label for node \" + str(node) + \" label is \" + str(var_dict[node]))\n",
    "        return label_actual\n",
    "\n",
    "    def isolability_measure_single_label(self,label):\n",
    "\n",
    "        G = self.graph\n",
    "        isolability = 0\n",
    "        internal = 0\n",
    "        external = 0\n",
    "        for node in G:\n",
    "            if self.cluster_h[node] == label:\n",
    "                for node_neighbour in G.neighbors(node):\n",
    "                    if (self.cluster_h[node] == self.cluster_h[node_neighbour]):\n",
    "                        internal = internal + G[node][node_neighbour]['weight']\n",
    "                    else:\n",
    "                        external = external + G[node][node_neighbour]['weight']\n",
    "        if external != 0: isolability = internal / (internal+external)\n",
    "        return isolability\n",
    "\n",
    "    def external_density_eval(self):\n",
    "\n",
    "        G = self.graph\n",
    "        numerator = 0\n",
    "        n = len(G)\n",
    "        denominator = n * (n - 1)\n",
    "        total_labels = set()\n",
    "        for node in G:\n",
    "            total_labels.add(self.cluster_h[node])\n",
    "        for label in total_labels:\n",
    "            nodes_per_label = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label: nodes_per_label += 1\n",
    "            denominator -= (nodes_per_label * (nodes_per_label - 1))\n",
    "        for (node1, node2) in G.edges():\n",
    "            if self.cluster_h[node1] != self.cluster_h[node2] and node1 != node2: numerator += 1\n",
    "        if denominator != 0:\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def coverage_eval(self):\n",
    "\n",
    "        G = self.graph\n",
    "        numerator = 0\n",
    "        denominator = len(G.edges)\n",
    "        for (node1, node2) in G.edges():\n",
    "            if self.cluster_h[node1] == self.cluster_h[node2] and node1 != node2: numerator += 1\n",
    "        if denominator != 0:\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def modularity_eval(self):\n",
    "\n",
    "        G = self.graph\n",
    "        total_edges = len(G.edges)\n",
    "        total_labels = set()\n",
    "        for node in G:\n",
    "            total_labels.add(self.cluster_h[node])\n",
    "        modularity = 0\n",
    "        internal_final = 0\n",
    "        external_final = 0\n",
    "        for label in total_labels:\n",
    "            internal = 0\n",
    "            external = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label:\n",
    "                    for node_neighbour in G.neighbors(node):\n",
    "                        if self.cluster_h[node_neighbour] == label:\n",
    "                            internal += 1\n",
    "                        else:\n",
    "                            external += 1\n",
    "            '''internal_final_check = internal/total_edges\n",
    "            external_final_check = external/total_edges\n",
    "            if internal_final_check >= 1 : print(\"internal problem\")\n",
    "            if external_final_check >= 1: print(\"external problem\")\n",
    "            modularity_current_label = internal_final - external_final*external_final\n",
    "            if modularity_current_label >= 1 : print(\"modularity current label problem\")\n",
    "            modularity += ((internal/total_edges) - ((external*external)/(total_edges*total_edges)))'''\n",
    "            modularity += internal / len(G.edges) - ((external * external) / (len(G.edges) * len(G.edges)))\n",
    "            internal_final += internal\n",
    "            external_final += external\n",
    "        internal_final = internal_final / 2\n",
    "        external_final = external_final / 2\n",
    "        modularity_final = (internal_final / total_edges) - (\n",
    "                    (external_final * external_final) / (total_edges * total_edges))\n",
    "        modularity_final = modularity / 2\n",
    "        if (internal_final + external_final) == total_edges:\n",
    "            print(\"modularity edge check correct\")\n",
    "            print(str(modularity_final))\n",
    "        return modularity_final\n",
    "\n",
    "    def TBCD_dissol(self):\n",
    "\n",
    "        G = self.graph\n",
    "        print(\"Inside dissolution phase\")\n",
    "        labels = set()\n",
    "        for node in self.cluster_head: labels.add(node) \n",
    "#         print(\"labels:\",labels)\n",
    "        \n",
    "#         print(\"no of sets = \" + str(len(labels)))\n",
    "#         print(\"no of nodes = \" + str(len(G)))\n",
    "        for label in labels:\n",
    "            comm_set = []\n",
    "            for node in self.graph:\n",
    "                if self.cluster_h[node] == label:\n",
    "                    comm_set.append(node)\n",
    "                    comm_set.append(self.level[node])\n",
    "#                     print(\"Community:\",comm_set)\n",
    "                    \n",
    "        label_set = set()\n",
    "        for label in self.cluster_head:\n",
    "            label_set.add(label)\n",
    "#         print(\"label_set:\",label_set)\n",
    "        \n",
    "        for label in label_set :\n",
    "#             print(\"for label = \"+str(label))\n",
    "            detachabil = self.detachability(label)\n",
    "#             print(\"detachabil:\",detachabil,\"theta:\",self.theta)\n",
    "            \n",
    "            if detachabil < self.theta :\n",
    "                n_e = set()\n",
    "                for node in G :\n",
    "                    self.occurence[node] = 0\n",
    "                for node in G :\n",
    "                    if self.cluster_h[node] == label :\n",
    "                        for single in G.neighbors(node) :\n",
    "                            if self.cluster_h[single] != label :\n",
    "                                n_e.add(single)\n",
    "                                self.occurence[single] += 1\n",
    "#                 print(\"n_e set has = \"+str(len(n_e)))\n",
    "                max = -1\n",
    "                for node in G :\n",
    "                    if self.occurence[node] > max :\n",
    "                        max = self.occurence[node]\n",
    "                n_max = set()\n",
    "                for node in G :\n",
    "                    if self.occurence[node] == max :\n",
    "                        n_max.add(node)\n",
    "#                 print(\"n_max set has = \" + str(len(n_max)))\n",
    "                \n",
    "                c_s = set()\n",
    "                for node in n_max :\n",
    "                    c_s.add(self.cluster_h[node])\n",
    "#                 print(\"c_s set has = \" + str(len(c_s)))\n",
    "                mid = -999999\n",
    "                big_label = -1\n",
    "                for other_label in c_s :\n",
    "                    label_node_set = set()\n",
    "                    for node in G :\n",
    "                        if self.cluster_h[node] == label :\n",
    "                            label_node_set.add(node)\n",
    "                    for node in label_node_set :\n",
    "                        self.cluster_h[node] = other_label\n",
    "                    term1 = self.detachability(other_label)\n",
    "                    for node in label_node_set :\n",
    "                        self.cluster_h[node] = label\n",
    "                    term2 = self.detachability(other_label)\n",
    "                    tid = term1 - term2\n",
    "#                     print(\"for label = \"+str(other_label))\n",
    "#                     print(\"tid = \"+str(tid))\n",
    "#                     print(\"mid = \" + str(mid))\n",
    "                    if tid > mid :\n",
    "#                         print(\"big label is other label\")\n",
    "                        mid = tid\n",
    "                        big_label = other_label\n",
    "                to_be_removed = -9999\n",
    "                if len(self.cluster_head) != 1 :\n",
    "                    self.cluster_head.remove(label)\n",
    "                    to_be_removed = label\n",
    "#                     print(\"big label = \"+str(big_label))\n",
    "#                     print(\"to be removed label = \" + str(label))\n",
    "                    for node in G :\n",
    "                        if self.cluster_h[node] == to_be_removed :\n",
    "                            self.cluster_h[node] = big_label\n",
    "\n",
    "    def combin(self,n):\n",
    "\n",
    "        n = int(n)\n",
    "        term = n*(n-1)\n",
    "        if term > 2:\n",
    "            return term/2\n",
    "        else : return 0\n",
    "\n",
    "\n",
    "    def all_metric_nog(self):\n",
    "\n",
    "        G = self.graph\n",
    "\n",
    "        self.modularity = self.modularity_eval()\n",
    "        self.coverage = self.coverage_eval()\n",
    "        self.external_density = self.external_density_eval()\n",
    "        print(\"\\n\\n\\n\\nwithout ground truth\\n\\n\\n\\n\")\n",
    "        print(\"modularity = \" + str(self.modularity))\n",
    "        print(\"coverage = \" + str(self.coverage))\n",
    "        print(\"external density = \" + str(self.external_density))\n",
    "        total_labels_mine = set()\n",
    "        for node in G:\n",
    "            total_labels_mine.add(self.cluster_h[node])\n",
    "        total_isolability = 0\n",
    "        for label in total_labels_mine:\n",
    "            total_isolability = total_isolability + self.isolability_measure_single_label(label)\n",
    "        average_isolability = total_isolability / len(total_labels_mine)\n",
    "        self.average_isolability = average_isolability\n",
    "        print(\"average isolability = \" + str(average_isolability))\n",
    "\n",
    "        self.result_array.append([self.row,len(total_labels_mine),self.modularity,self.coverage,self.external_density,\n",
    "                                  self.average_isolability])\n",
    "        self.row += 1\n",
    "\n",
    "    def all_metric(self):\n",
    "\n",
    "        G = self.graph\n",
    "\n",
    "        self.modularity = self.modularity_eval()\n",
    "        self.coverage = self.coverage_eval()\n",
    "        self.external_density = self.external_density_eval()\n",
    "        print(\"\\n\\n\\n\\nwithout ground truth\\n\\n\\n\\n\")\n",
    "        print(\"modularity = \" + str(self.modularity))\n",
    "        print(\"coverage = \" + str(self.coverage))\n",
    "        print(\"external density = \" + str(self.external_density))\n",
    "        total_labels_mine = set()\n",
    "        total_labels_ground = set()\n",
    "        for node in G:\n",
    "            total_labels_mine.add(self.cluster_h[node])\n",
    "            #total_labels_ground.add(self.ground_truth_comm[node])\n",
    "        total_isolability = 0\n",
    "        for label in total_labels_mine:\n",
    "            total_isolability = total_isolability + self.isolability_measure_single_label(label)\n",
    "        average_isolability = total_isolability / len(total_labels_mine)\n",
    "        self.average_isolability = average_isolability\n",
    "        print(\"average isolability = \" + str(average_isolability))\n",
    "\n",
    "        print(\"calculating ground truth metric\")\n",
    "        comm_list = open(self.comm_list)\n",
    "        print(\"before reading community text file\")\n",
    "\n",
    "        no_nodes = 0\n",
    "        ground_truth_comm_set = set()\n",
    "        for l in comm_list:\n",
    "            l = l.split(\" \")\n",
    "            node = int(l[0])\n",
    "            comm_label = int(l[1])\n",
    "            #print(\"for node x : \" + str(node) + \" community is  : \" + str(comm_label))\n",
    "            ground_truth_comm_set.add(comm_label)\n",
    "            self.ground_truth_comm[node] = comm_label\n",
    "            no_nodes += 1\n",
    "\n",
    "        print(\"no of communities in ground truth = \"+str(len(ground_truth_comm_set)))\n",
    "        #print(str(ground_truth_comm_set))\n",
    "        #for node in range(no_nodes): print(str(node)+\" \"+str(self.ground_truth_comm[node]))\n",
    "\n",
    "        algo_comm_set = set()\n",
    "        for node in G :\n",
    "            algo_comm_set.add(self.cluster_h[node])\n",
    "        print(\"no of communities from algo = \"+str(len(algo_comm_set)))\n",
    "\n",
    "\n",
    "        ground_truth_comm_list = list(ground_truth_comm_set)\n",
    "        algo_comm_list = list(algo_comm_set)\n",
    "        common_matrix = np.zeros((len(ground_truth_comm_set),len(algo_comm_set)))\n",
    "        for i in range(len(ground_truth_comm_set)) :\n",
    "            for j in range(len(algo_comm_set)) :\n",
    "                for node in G :\n",
    "                    if self.ground_truth_comm[node] == ground_truth_comm_list[i] and \\\n",
    "                            self.cluster_h[node] == algo_comm_list[j] :\n",
    "                        common_matrix[i][j] += 1\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        true_neg = 0\n",
    "        false_neg = 0\n",
    "        for node1 in G :\n",
    "            for node2 in G :\n",
    "                if self.cluster_h[node1] == self.cluster_h[node2] :\n",
    "                    if self.ground_truth_comm[node1] == self.ground_truth_comm[node2] :\n",
    "                        true_pos += 1\n",
    "                    else :\n",
    "                        false_pos += 1\n",
    "                else :\n",
    "                    if self.ground_truth_comm[node1] != self.ground_truth_comm[node2] :\n",
    "                        true_neg += 1\n",
    "                    else :\n",
    "                        false_neg += 1\n",
    "\n",
    "        self.precision = true_pos/(true_pos+false_pos)\n",
    "        self.recall = true_pos/(true_pos+false_neg)\n",
    "\n",
    "        ground_truth_comm_no = {}\n",
    "        ground_truth_comm_no_index = np.zeros(len(ground_truth_comm_set))\n",
    "        algo_comm_no = {}\n",
    "        algo_comm_no_index = np.zeros(len(algo_comm_set))\n",
    "        index = -1\n",
    "        for label in ground_truth_comm_set :\n",
    "            index += 1\n",
    "            ground_truth_comm_no[label] = 0\n",
    "            ground_truth_comm_no_index[index] = 0\n",
    "            for node in range(no_nodes) :\n",
    "                if self.ground_truth_comm[node] == label :\n",
    "                    #print(\"counting ground truth increase\")\n",
    "                    ground_truth_comm_no[label] += 1\n",
    "                    ground_truth_comm_no_index[index] += 1\n",
    "\n",
    "        index = -1\n",
    "        for label in algo_comm_set:\n",
    "            index += 1\n",
    "            algo_comm_no[label] = 0\n",
    "            algo_comm_no_index[index] = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label:\n",
    "                    algo_comm_no[label] += 1\n",
    "                    algo_comm_no_index[index] += 1\n",
    "\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "\n",
    "        for i in range(len(ground_truth_comm_set)):\n",
    "            if ground_truth_comm_no_index[i] == 0 : print(\"error in ground truth label counting\")\n",
    "\n",
    "\n",
    "        for i in range(len(ground_truth_comm_set)) :\n",
    "            for j in range(len(algo_comm_set)) :\n",
    "                if common_matrix[i][j] != 0 :\n",
    "                    numerator += common_matrix[i][j] * math.log((common_matrix[i][j]*no_nodes)/(algo_comm_no_index[j]*ground_truth_comm_no_index[i]))\n",
    "\n",
    "        numerator *= -2\n",
    "\n",
    "        for i in range(len(ground_truth_comm_set)):\n",
    "            denominator += ground_truth_comm_no_index[i] * math.log(ground_truth_comm_no_index[i]/no_nodes)\n",
    "\n",
    "        for j in range(len(algo_comm_set)):\n",
    "            denominator += algo_comm_no_index[j] * math.log(algo_comm_no_index[j]/no_nodes)\n",
    "\n",
    "        self.nmi = numerator/denominator\n",
    "\n",
    "        self.f_measure = 2 * (self.precision * self.recall)/(self.precision + self.recall)\n",
    "\n",
    "        self.purity = 0\n",
    "\n",
    "        for ground_truth_label in ground_truth_comm_set:\n",
    "            max = -1\n",
    "            for algo_label in algo_comm_set:\n",
    "                count = 0\n",
    "                for node in G :\n",
    "                    if self.cluster_h[node] == algo_label and self.ground_truth_comm[node] == ground_truth_label :\n",
    "                        count += 1\n",
    "                if count > max : max = count\n",
    "\n",
    "            self.purity += max\n",
    "\n",
    "        self.purity = self.purity/no_nodes\n",
    "\n",
    "        self.entropy = 0\n",
    "\n",
    "        for label_mine in algo_comm_set :\n",
    "            n_c = algo_comm_no[label_mine]\n",
    "            n = no_nodes\n",
    "            m = len(ground_truth_comm_set)\n",
    "            term2 = 0\n",
    "            for label_ground in ground_truth_comm_set :\n",
    "                n_i_j = 0\n",
    "                for node in G :\n",
    "                    if self.cluster_h[node] == label_mine and self.ground_truth_comm[node] == label_ground :\n",
    "                        n_i_j += 1\n",
    "                if n_i_j != 0 :\n",
    "                    term2 += (n_i_j/n_c)* math.log(n_i_j/n_c)\n",
    "            self.entropy += (n_c/n)*(1/math.log(m))*term2*(-1)\n",
    "\n",
    "        print(\"entropy = \" + str(self.entropy))\n",
    "\n",
    "        self.ari = 0\n",
    "        term_common = 0\n",
    "        for i in range(len(ground_truth_comm_set)):\n",
    "            for j in range(len(algo_comm_set)):\n",
    "                term_common += self.combin(common_matrix[i][j])\n",
    "\n",
    "        term_ground = 0\n",
    "\n",
    "        print(\"ground truth label set\")\n",
    "        print(str(ground_truth_comm_set))\n",
    "        for label in ground_truth_comm_set:\n",
    "            count = 0\n",
    "            for node in G:\n",
    "                if self.ground_truth_comm[node] == label: count += 1\n",
    "            term_ground += self.combin(count)\n",
    "\n",
    "        print(\"algo label set\")\n",
    "        print(str(algo_comm_set))\n",
    "        term_algo = 0\n",
    "        for label in algo_comm_set:\n",
    "            count = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label: count += 1\n",
    "            term_algo += self.combin(count)\n",
    "\n",
    "\n",
    "        term_sum = (term_ground + term_algo) / 2\n",
    "        term_prod = (term_ground * term_algo) / self.combin(no_nodes)\n",
    "\n",
    "        '''print(\"term_common = \"+str(term_common))\n",
    "        print(\"term_ground = \" + str(term_ground))\n",
    "        print(\"term_algo = \" + str(term_algo))\n",
    "        print(\"term_sum = \" + str(term_sum))\n",
    "        print(\"term_prod = \" + str(term_prod))'''\n",
    "\n",
    "        self.ari = (term_common - term_prod) / (term_sum - term_prod)\n",
    "\n",
    "        self.result_array.append([self.row,len(algo_comm_set),self.modularity,self.coverage,self.external_density,self.average_isolability,self.f_measure,self.nmi,self.purity,self.entropy,self.ari])\n",
    "        self.row += 1\n",
    "        \n",
    "    def calldiverified(self):\n",
    "        print(self.graph)\n",
    "        return 1,3\n",
    "    \n",
    "\n",
    "    def execute_TBCD_txt(self,file_name,perc):\n",
    "        \"\"\"\n",
    "            Execute tree based community detection algorithm from static dataset\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"reading static edge list with file name : \"+str(file_name))\n",
    "#         self.edge_list = './datasets_txt/' + file_name + '_edge_list.txt'\n",
    "#         self.comm_list = './datasets_txt/' + file_name + '_comm_list.txt'\n",
    "        self.edge_list = file_name + '_edge_list.txt'\n",
    "#         self.comm_list =  file_name + '_comm_list.txt'\n",
    "    \n",
    "        edge_list = open(self.edge_list)\n",
    "#         print(\"counting edges to set window size\")\n",
    "        window_counter = 0\n",
    "        count = 0\n",
    "        for l in edge_list:\n",
    "            count += 1\n",
    "#         print(\"Count:\",count,\"Window ratio:\",self.window_ratio)\n",
    "        \n",
    "        self.window_size = int(count*self.window_ratio)\n",
    "        # print(\"Window size:\",self.window_size)\n",
    "        \n",
    "        self.graph = nx.Graph()\n",
    "        # print(self.graph)\n",
    "        \n",
    "#       print(\"before reading edge list text file\")\n",
    "        no_nodes = 0\n",
    "        w = 0\n",
    "        edge_list = open(self.edge_list)\n",
    "        R_seed=[]\n",
    "        seedSet=[]\n",
    "        pagerank=[]\n",
    "        G = nx.DiGraph()\n",
    "#         MultiDiGraph\n",
    "        # print(\"------------------------------------------------------------------------------------------\")\n",
    "        \n",
    "        print(count)\n",
    "        itr=[]\n",
    "        for i in range(len(perc)):\n",
    "            vv=int((perc[i]/100)*count)\n",
    "            itr.append(vv)\n",
    "        print(itr)\n",
    "        totalnodes=0\n",
    "        resultt={}\n",
    "        \n",
    "        for l in edge_list:\n",
    "#             print(l)\n",
    "            totalnodes=totalnodes+1\n",
    "            l = l.split(\" \")\n",
    "            s=int(l[0])\n",
    "            t=int(l[1])\n",
    "            value = round(random.uniform(0.01,1.0),2)\n",
    "            G.add_edge(s,t,weight=value)\n",
    "            G.nodes[s]['thres']=(G.in_degree(s)/2)\n",
    "            G.nodes[t]['thres']=(G.in_degree(t)/2)\n",
    "            k=int(findk(len(G.nodes)))\n",
    "            \n",
    "            self.added += 1\n",
    "            e = {}\n",
    "            x = int(l[0])\n",
    "            y = int(l[1])\n",
    "#             print(\"for edge x : \"+str(x)+\" y : \"+str(y))\n",
    "#             print(\".....................................................\")\n",
    "            if x == y:\n",
    "#                 print(\"self edge found\")\n",
    "                if totalnodes in itr:\n",
    "                    self.TBCD_dissol()\n",
    "                    w = 0\n",
    "                    print(\"_______________________________________________________________________________________________\")\n",
    "                    print(\"Get result at edge:\",totalnodes)\n",
    "                    comm_set_final = detect_comm(self.graph, self.cluster_head, self.cluster_h)\n",
    "                    resultt[totalnodes]=findresult(G,k,comm_set_final)\n",
    "                continue\n",
    "\n",
    "            if not self.graph.has_node(x) :\n",
    "#                 print(\"Graph Does not have node x.\")\n",
    "                self.graph.add_node(x)\n",
    "#                 print(\"intra connection of x:\",self.intra_conn[x])\n",
    "                self.intra_conn[x] = -1\n",
    "                self.inter_conn[x] = -1\n",
    "                self.select[x] = 0\n",
    "                no_nodes = no_nodes + 1\n",
    "#                 print(\"new node added\")\n",
    "                # print(\"no nodes = \"+str(no_nodes))\n",
    "\n",
    "            if not self.graph.has_node(y):\n",
    "#                 print(\"Graph Does not have node y.\")\n",
    "                self.graph.add_node(y)\n",
    "#                 print(\"intra connection of y:\",self.intra_conn[y])\n",
    "                self.intra_conn[y] = -1\n",
    "                self.inter_conn[y] = -1\n",
    "                self.select[y] = 0\n",
    "                no_nodes = no_nodes + 1\n",
    "#                 print(\"new node added\")\n",
    "                # print(\"no nodes = \" + str(no_nodes))\n",
    "\n",
    "            self.graph.add_edge(x, y, weight=1.0)\n",
    "\n",
    "            if self.select[x] == 0 and self.select[y] == 0:\n",
    "#                 print(\"If both are new.\")\n",
    "#                 print(\"select[x]==select[y]\")\n",
    "                # print(type(self.cluster_head))\n",
    "#                 print(\"Cluster head:\",self.cluster_head)\n",
    "                self.cluster_head.add(x)\n",
    "#                 print(\"Cluster head after added:\",self.cluster_head)\n",
    "                self.cluster_h[x] = x\n",
    "                self.level[x] = 0\n",
    "                self.level[y] = 1\n",
    "                self.cluster_h[y] = x\n",
    "                self.intra_conn[x] = 1\n",
    "                self.intra_conn[y] = 1\n",
    "                self.select[x] = 1\n",
    "                self.select[y] = 1\n",
    "\n",
    "            elif self.select[y] == 0 or self.select[x] == 0 :\n",
    "\n",
    "                if self.select[x] == 0 :\n",
    "                    term = y\n",
    "                    y = x\n",
    "                    x = term\n",
    "#                 print(\"Max level:\",self.level_max)\n",
    "                \n",
    "                \n",
    "                if self.level[x] <= self.level_max - 1:\n",
    "                    self.level[y] = self.level[x] + 1\n",
    "                    self.intra_conn[x] += 1\n",
    "                    self.intra_conn[y] = 1\n",
    "                    self.cluster_h[y] = self.cluster_h[x]\n",
    "                    self.select[y] = 1\n",
    "\n",
    "                else:\n",
    "                    self.cluster_head.add(x)\n",
    "                    self.cluster_h[x] = x\n",
    "                    self.level[x] = 0\n",
    "                    self.level[y] = 1\n",
    "                    self.cluster_h[y] = x\n",
    "                    self.inter_conn[x] = self.intra_conn[x]\n",
    "                    self.intra_conn[x] = 1\n",
    "                    self.intra_conn[y] = 1\n",
    "                    self.select[x] = 1\n",
    "                    self.select[y] = 1\n",
    "\n",
    "            else:  #If clusters are same\n",
    "#                 print(\"If clusters are same...,\")\n",
    "                if self.cluster_h[x] == self.cluster_h[y]:\n",
    "                    self.intra_conn[x] += 1\n",
    "                    self.intra_conn[y] += 1\n",
    "                else:\n",
    "                    self.inter_conn[x] += 1\n",
    "                    self.inter_conn[y] += 1\n",
    "#             print(\"cluster_h x:\",self.cluster_h[x],\"cluster_h y:\",self.cluster_h[y])\n",
    "#             print(\"Expansion phase started.\")\n",
    "            \n",
    "#             labels = set()\n",
    "#             for node in self.cluster_head: labels.add(node)\n",
    "#             comm_set_final = []\n",
    "#             for label in labels:\n",
    "#                 comm_set = []\n",
    "#                 for node in self.graph :\n",
    "#                     if self.cluster_h[node] == label :\n",
    "#                         comm_set.append(node)\n",
    "#                 comm_set_final.append(comm_set)\n",
    "            w += 1\n",
    "            # print(\"w:\",w,\" window size:\",self.window_size)\n",
    "#             print(itr,totalnodes)\n",
    "            if w == self.window_size or totalnodes==count:\n",
    "                print(\"**************************************************************************\")\n",
    "                window_counter += 1  \n",
    "#                 print(\"Window sizeee\",totalnodes)\n",
    "                self.TBCD_dissol()\n",
    "#                 seedset,activatednodes=self.calldiverified()\n",
    "#                 self.all_metric()\n",
    "                w = 0\n",
    "#                 findresult(G,k,comm_set_final):\n",
    "            \n",
    "            if totalnodes in itr:\n",
    "                self.TBCD_dissol()\n",
    "                w = 0\n",
    "                print(\"_______________________________________________________________________________________________\")\n",
    "                print(\"Get result at node:\",totalnodes)\n",
    "                comm_set_final = detect_comm(self.graph, self.cluster_head, self.cluster_h)\n",
    "                resultt[totalnodes]=findresult(G,k,comm_set_final)\n",
    "        \n",
    "        displayresult(resultt,itr,perc)\n",
    "        dff = makeExcel(resultt,itr,perc)\n",
    "        return dff\n",
    "\n",
    "            \n",
    "    def checkingg(self):\n",
    "        print(\"666\")\n",
    "        \n",
    "def detect_comm(graph, cluster_head, cluster_h):\n",
    "    labels = set()\n",
    "    for node in cluster_head:\n",
    "        labels.add(node)\n",
    "    comm_set_final = []\n",
    "    for label in labels:\n",
    "        comm_set = []\n",
    "        for node in graph :\n",
    "            if cluster_h[node] == label :\n",
    "                comm_set.append(node)\n",
    "        comm_set_final.append(comm_set)\n",
    "    return comm_set_final        \n",
    "\n",
    "# obj=TBCD_mine()\n",
    "# perc=[20,40,60,80,100]\n",
    "# filename='LFR_500_0.0'\n",
    "# obj.execute_TBCD_txt(filename,perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj=TBCD_mine()\n",
    "# perc=[20,40,60,80,100]\n",
    "# filename='LFR_500_0.0'\n",
    "# obj.execute_TBCD_txt(filename,perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "3W9mywEit_nM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello.\n"
     ]
    }
   ],
   "source": [
    "print(\"hello.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "haTniG2Lt_nM",
    "outputId": "957ade05-8092-4df7-d054-c3cf8dd5040c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization\n",
      "6594\n",
      "[659, 1318, 1978, 2637, 3297, 3956, 4615, 5275, 5934, 6594]\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "Inside dissolution phase\n",
      "_______________________________________________________________________________________________\n",
      "Get result at node: 659\n",
      "\n",
      "\n",
      "--------------- CDIM -------------------\n",
      "CDIM: [[352, 466, 469, 201, 140, 362, 337, 4341, 325, 205, 189, 171, 346, 339, 98, 68]]\n",
      "Communities formed:  75\n",
      "Community we got: 28\n",
      "{'Name': 'CDIM', 'k_nodes': 16, 'number of communities': 28, 'length of activated nodes': 59}\n",
      "\n",
      "\n",
      "--------------- DEGREE -------------------\n",
      "Communities formed:  75\n",
      "Community we got: 16\n",
      "{'Name': 'Degree', 'k_nodes': 16, 'number of communities': 16, 'length of activated nodes': 31}\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "Inside dissolution phase\n",
      "_______________________________________________________________________________________________\n",
      "Get result at node: 1318\n",
      "\n",
      "\n",
      "--------------- CDIM -------------------\n",
      "CDIM: [[352, 658, 466, 469, 201, 140, 929, 362, 337, 4341, 2312, 803, 736, 720, 614, 325, 205, 189, 171, 346, 772, 691, 531, 339, 98, 68, 696, 692, 496, 4446, 467, 381]]\n",
      "Communities formed:  137\n",
      "Community we got: 55\n",
      "{'Name': 'CDIM', 'k_nodes': 32, 'number of communities': 55, 'length of activated nodes': 107}\n",
      "\n",
      "\n",
      "--------------- DEGREE -------------------\n",
      "Communities formed:  137\n",
      "Community we got: 32\n",
      "{'Name': 'Degree', 'k_nodes': 32, 'number of communities': 32, 'length of activated nodes': 54}\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "Inside dissolution phase\n",
      "_______________________________________________________________________________________________\n",
      "Get result at node: 1978\n",
      "\n",
      "\n",
      "--------------- CDIM -------------------\n",
      "CDIM: [[1057, 1488, 352, 466, 469, 658, 201, 140, 1049, 929, 362, 337, 1021, 4341, 2312, 1459, 1197, 736, 720, 614, 325, 205, 189, 346, 200, 691, 1035, 531, 1017, 772, 339, 98, 68, 1774, 1469, 1709, 1402, 1326, 696, 692, 496, 4446, 467, 381, 227, 207, 154, 129, 9]]\n",
      "Communities formed:  212\n",
      "Community we got: 83\n",
      "{'Name': 'CDIM', 'k_nodes': 49, 'number of communities': 83, 'length of activated nodes': 161}\n",
      "\n",
      "\n",
      "--------------- DEGREE -------------------\n",
      "Communities formed:  212\n",
      "Community we got: 46\n",
      "{'Name': 'Degree', 'k_nodes': 49, 'number of communities': 46, 'length of activated nodes': 76}\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "Inside dissolution phase\n",
      "_______________________________________________________________________________________________\n",
      "Get result at node: 2637\n",
      "\n",
      "\n",
      "--------------- CDIM -------------------\n",
      "CDIM: [[1423, 1057, 1488, 352, 1461, 658, 466, 469, 201, 140, 1049, 929, 362, 337, 1021, 4341, 2312, 1920, 1459, 1433, 1304, 1197, 205, 736, 720, 614, 325, 189, 2158, 346, 200, 1760, 1505, 1345, 1035, 1017, 2193, 531, 1466, 772, 691, 339, 98, 68, 1774, 1631, 1469, 1402, 1449, 696, 692, 496, 4446, 467, 381, 227, 207, 154, 1811, 129, 9, 1709, 702]]\n",
      "Communities formed:  263\n",
      "Community we got: 102\n",
      "{'Name': 'CDIM', 'k_nodes': 63, 'number of communities': 102, 'length of activated nodes': 208}\n",
      "\n",
      "\n",
      "--------------- DEGREE -------------------\n",
      "Communities formed:  263\n",
      "Community we got: 53\n",
      "{'Name': 'Degree', 'k_nodes': 63, 'number of communities': 53, 'length of activated nodes': 93}\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "Inside dissolution phase\n",
      "_______________________________________________________________________________________________\n",
      "Get result at node: 3297\n",
      "\n",
      "\n",
      "--------------- CDIM -------------------\n",
      "CDIM: [[1423, 1057, 1488, 2163, 352, 1920, 2192, 2203, 1461, 658, 466, 469, 201, 140, 1049, 2151, 929, 362, 337, 1021, 4341, 2312, 2407, 2011, 2183, 1816, 2007, 1826, 1778, 1459, 1433, 1304, 736, 720, 614, 325, 205, 189, 200, 2158, 2141, 346, 2143, 1760, 1505, 2372, 2193, 2190, 2107, 2026, 1954, 1840, 1345, 1035, 1017, 772, 691, 531, 339, 98, 68, 2405, 2395, 2049, 2023, 1896, 2371, 2363, 2346, 2042, 1970, 2310, 1830, 1774, 1402, 696]]\n",
      "Communities formed:  315\n",
      "Community we got: 135\n",
      "{'Name': 'CDIM', 'k_nodes': 76, 'number of communities': 135, 'length of activated nodes': 248}\n",
      "\n",
      "\n",
      "--------------- DEGREE -------------------\n",
      "Communities formed:  315\n",
      "Community we got: 63\n",
      "{'Name': 'Degree', 'k_nodes': 76, 'number of communities': 63, 'length of activated nodes': 111}\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "Inside dissolution phase\n",
      "_______________________________________________________________________________________________\n",
      "Get result at node: 3956\n",
      "\n",
      "\n",
      "--------------- CDIM -------------------\n",
      "CDIM: [[1423, 1057, 1488, 2163, 352, 1920, 2192, 2203, 1461, 658, 466, 469, 201, 140, 2151, 1049, 929, 362, 337, 1021, 4341, 2312, 2407, 2183, 1816, 2011, 2007, 1826, 1778, 1459, 1433, 1304, 736, 720, 614, 325, 205, 189, 200, 2158, 2141, 346, 2143, 1760, 1505, 2443, 2372, 2193, 2190, 2107, 2026, 1954, 1840, 1345, 1035, 1017, 772, 691, 531, 339, 98, 68, 1774, 2049, 2042, 2023, 1970, 1896, 1830, 1402, 2934, 2617, 2405, 2395, 2371, 2363, 2346, 2310, 696, 692, 496, 4446, 381, 227, 207, 154, 2129, 1811, 1197, 2075, 129, 9]]\n",
      "Communities formed:  372\n",
      "Community we got: 155\n",
      "{'Name': 'CDIM', 'k_nodes': 92, 'number of communities': 155, 'length of activated nodes': 294}\n",
      "\n",
      "\n",
      "--------------- DEGREE -------------------\n",
      "Communities formed:  372\n",
      "Community we got: 74\n",
      "{'Name': 'Degree', 'k_nodes': 92, 'number of communities': 74, 'length of activated nodes': 124}\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "Inside dissolution phase\n",
      "_______________________________________________________________________________________________\n",
      "Get result at node: 4615\n",
      "\n",
      "\n",
      "--------------- CDIM -------------------\n"
     ]
    }
   ],
   "source": [
    "def executeDiv():\n",
    "    result_df=pd.DataFrame()\n",
    "    for i in range(1):\n",
    "        obj=TBCD_mine()\n",
    "        perc=[10,20,30,40,50,60,70,80,90,100]\n",
    "        filename='power'\n",
    "        dff=obj.execute_TBCD_txt(filename,perc)\n",
    "        dff2=result_df\n",
    "        result_df=dff\n",
    "        result_df = pd.concat([dff2, dff], ignore_index=True)\n",
    "        result_df.to_excel('Outputt.xlsx')\n",
    "def processResults():\n",
    "    df = pd.read_excel('Outputt.xlsx')\n",
    "    df=df.drop(columns='Unnamed: 0')\n",
    "    print(df.shape)\n",
    "#     print(df.head(8))\n",
    "    TBCD = df.loc[df['Name of Algorithm']=='TBCD']\n",
    "    CDIM = df.loc[df['Name of Algorithm']=='CDIM']\n",
    "    Greedy = df.loc[df['Name of Algorithm']=='Greedy']\n",
    "    CELF = df.loc[df['Name of Algorithm']=='CELF']\n",
    "    PMIA = df.loc[df['Name of Algorithm']=='PMIA']\n",
    "    SIMPATH = df.loc[df['Name of Algorithm']=='SIMPATH']\n",
    "    DEGREE = df.loc[df['Name of Algorithm']=='Degree']\n",
    "    PAGERANK = df.loc[df['Name of Algorithm']=='PageRank']\n",
    "    \n",
    "    cols= df.columns\n",
    "\n",
    "    ddris = pd.DataFrame(TBCD.iloc[:,1:].mean(),columns = ['TBCD'])\n",
    "    cdim = pd.DataFrame(CDIM.iloc[:,1:].mean(),columns = ['CDIM'])\n",
    "    greedy = pd.DataFrame(Greedy.iloc[:,1:].mean(),columns = ['Greedy'])\n",
    "    celf = pd.DataFrame(CELF.iloc[:,1:].mean(),columns = ['CELF'])\n",
    "    pmia = pd.DataFrame(PMIA.iloc[:,1:].mean(),columns = ['PMIA'])\n",
    "    simpath = pd.DataFrame(SIMPATH.iloc[:,1:].mean(),columns = ['SIMPATH'])\n",
    "    degree = pd.DataFrame(DEGREE.iloc[:,1:].mean(),columns = ['DEGREE'])\n",
    "    pagerank = pd.DataFrame(PAGERANK.iloc[:,1:].mean(),columns = ['PageRank'])\n",
    "    \n",
    "    \n",
    "    resultant = pd.concat([ddris, cdim,greedy,celf,pmia,simpath,degree,pagerank], axis='columns')\n",
    "#     resultant = pd.concat([ddris,pmia,simpath,degree,pagerank], axis='columns')\n",
    "#     print(\"Resultant:\",resultant)\n",
    "    \n",
    "    result=resultant.iloc[0:]\n",
    "#     print(\"Result:\",result)\n",
    "    \n",
    "    result.to_excel('EJ_Processed_POwer03.xlsx')\n",
    "    \n",
    "    \n",
    "executeDiv()\n",
    "processResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
