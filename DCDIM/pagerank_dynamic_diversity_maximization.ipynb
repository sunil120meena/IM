{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOpWXRCkSeNe",
    "outputId": "c6b4ba6b-ee0e-406a-dbe4-7e196e27403f"
   },
   "outputs": [],
   "source": [
    "# %pip install 'networkx<2.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3Qlb0a7x8Jb",
    "outputId": "e698392f-587f-496c-b22c-4e47d3eb3ae7"
   },
   "outputs": [],
   "source": [
    "# !pip install cdlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import heapq\n",
    "import argparse\n",
    "import threading\n",
    "import multiprocessing\n",
    "import sys\n",
    "import queue\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import math, time\n",
    "from copy import deepcopy\n",
    "import multiprocessing, json\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "\n",
    "\n",
    "#importing libraries that will be used\n",
    "# import networkx as nx#for creating network\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt#for plotting plots\n",
    "# import random\n",
    "# import time#claculating time\n",
    "# import math\n",
    "# from collections import Counter\n",
    "# from itertools import permutations \n",
    "# from itertools import combinations\n",
    "\n",
    "# from collections import defaultdict\n",
    "# from scipy.io import mmread# to read dataset\n",
    "# import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OD7RKjsuuzaY",
    "outputId": "a70ee33e-fa14-4538-c7c0-11b89af314c2"
   },
   "outputs": [],
   "source": [
    "#importing libraries that will be used\n",
    "import networkx as nx#for creating network\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt#for plotting plots\n",
    "\n",
    "import random\n",
    "import time#claculating time\n",
    "import math\n",
    "from collections import Counter\n",
    "from itertools import permutations \n",
    "from itertools import combinations\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.io import mmread# to read dataset\n",
    "import pandas as pd\n",
    "\n",
    "# from cdlib import algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def celf(graph, k):\n",
    "    \"\"\"\n",
    "    Implementation of CELF algorithm for influence maximization in social networks\n",
    "    \n",
    "    Args:\n",
    "    - graph: NetworkX graph object representing the social network\n",
    "    - k: number of nodes to select\n",
    "    \n",
    "    Returns:\n",
    "    - nodes: list of k nodes with the highest influence scores\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    nodes = []\n",
    "    heap = []\n",
    "    marg_gains = {}\n",
    "\n",
    "    # Calculate the marginal gain for each node\n",
    "    for node in graph.nodes():\n",
    "        # Run Monte Carlo simulations to estimate the influence of each node\n",
    "        sim_res = linear_Threshold(graph, nodes + [node])\n",
    "        marg_gains[node] = len(sim_res) - len(nodes)\n",
    "        # Add the node to the heap with its marginal gain as key\n",
    "        heapq.heappush(heap, (-marg_gains[node], node))\n",
    "\n",
    "    # Select the k nodes with the highest influence scores\n",
    "    while len(nodes) < k:\n",
    "        # Get the node with the highest marginal gain\n",
    "        _, node = heapq.heappop(heap)\n",
    "        # Recalculate the marginal gain of the selected node\n",
    "        sim_res = linear_Threshold(graph, nodes + [node])\n",
    "        marg_gains[node] = len(sim_res) - len(nodes)\n",
    "        # Add the node to the list of selected nodes\n",
    "        nodes.append(node)\n",
    "        # Update the heap with the new marginal gains\n",
    "        for n in graph.neighbors(node):\n",
    "            if n not in nodes:\n",
    "                heapq.heappush(heap, (-marg_gains[n], n))\n",
    "\n",
    "    return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Implementation of SimPath algorithm'''\n",
    "\n",
    "class CELFQueue:\n",
    "    # create if not exist\n",
    "    nodes = None\n",
    "    q = None\n",
    "    nodes_gain = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.q = []\n",
    "        self.nodes_gain = {}\n",
    "\n",
    "    def put(self, node, marginalgain):\n",
    "        self.nodes_gain[node] = marginalgain\n",
    "        heapq.heappush(self.q, (-marginalgain, node))\n",
    "\n",
    "    def update(self, node, marginalgain):\n",
    "        self.remove(node)\n",
    "        self.put(node, marginalgain)\n",
    "\n",
    "    def remove(self, node):\n",
    "        self.q.remove((-self.nodes_gain[node], node))\n",
    "        self.nodes_gain[node] = None\n",
    "        heapq.heapify(self.q)\n",
    "\n",
    "    def topn(self, n):\n",
    "        top = heapq.nsmallest(n, self.q)\n",
    "        top_ = list()\n",
    "        for t in top:\n",
    "            top_.append(t[1])\n",
    "        return top_\n",
    "\n",
    "    def get_gain(self, node):\n",
    "        return self.nodes_gain[node]\n",
    "\n",
    "    \n",
    "    \n",
    "def init_D(graph):\n",
    "    D = {}\n",
    "#     for i in range(graph.node_num + 1):\n",
    "    for i in graph.nodes:\n",
    "        D[i]=[]\n",
    "    return D\n",
    "\n",
    "class Graph:\n",
    "    nodes = None\n",
    "    edges = None\n",
    "    children = None\n",
    "    parents = None\n",
    "    node_num = None\n",
    "    edge_num = None\n",
    "\n",
    "    def __init__(self, nodes, edges, children, parents, node_num, edge_num):\n",
    "#         print(\"hello[[[[[[[[[[[[[[[[[[[[[]]]]]]]]]]]]]]]]]]]]]\")\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "        self.children = children\n",
    "        self.parents = parents\n",
    "        self.node_num = node_num\n",
    "        self.edge_num = edge_num\n",
    "#         print(\"Numnodes:\",self.node_num)\n",
    "    def get_children(self, node):\n",
    "        ch = self.children.get(node)\n",
    "        if ch is None:\n",
    "            self.children[node] = []\n",
    "        return self.children[node]\n",
    "\n",
    "    def get_parents(self, node):\n",
    "        pa = self.parents.get(node)\n",
    "        if pa is None:\n",
    "            self.parents[node] = []\n",
    "        return self.parents[node]\n",
    "\n",
    "    def get_weight(self, src, dest):\n",
    "        weight = self.edges.get((src, dest))\n",
    "        if weight is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return weight\n",
    "\n",
    "    # return true if node1 is parent of node 2 , else return false\n",
    "    def is_parent_of(self, node1, node2):\n",
    "        if self.get_weight(node1, node2) != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # return true if node1 is child of node 2 , else return false\n",
    "    def is_child_of(self, node1, node2):\n",
    "        return self.is_parent_of(node2, node1)\n",
    "\n",
    "    def get_out_degree(self, node):\n",
    "        return len(self.get_children(node))\n",
    "\n",
    "    def get_in_degree(self, node):\n",
    "        return len(self.get_parents(node))\n",
    "\n",
    "    \n",
    "def read_graph_info(path):\n",
    "    if os.path.exists(path):\n",
    "        parents = {}\n",
    "        children = {}\n",
    "        edges = {}\n",
    "        nodes = set()\n",
    "\n",
    "        try:\n",
    "            f = open(path, 'r')\n",
    "            txt = f.readlines()\n",
    "            header = str.split(txt[0])\n",
    "            node_num = int(header[0])\n",
    "            edge_num = int(header[1])\n",
    "\n",
    "            for line in txt[1:]:\n",
    "#                 print(\"line:\",line)\n",
    "                row = str.split(line)\n",
    "#                 print(row)\n",
    "                src = int(row[0])\n",
    "                des = int(row[1])\n",
    "                nodes.add(src)\n",
    "                nodes.add(des)\n",
    "#                 print(src,des)    \n",
    "                if children.get(src) is None:\n",
    "                    children[src] = []\n",
    "                if parents.get(des) is None:\n",
    "                    parents[des] = []\n",
    "\n",
    "#                 weight = float(row[2])\n",
    "                weight=round(random.uniform(0.0,1.0),2)\n",
    "                edges[(src, des)] = weight\n",
    "                children[src].append(des)\n",
    "                parents[des].append(src)\n",
    "\n",
    "            return list(nodes), edges, children, parents, node_num, edge_num\n",
    "        except IOError:\n",
    "            print('IOError')\n",
    "    else:\n",
    "        print('file can not found')\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# def get_vertex_cover(graph):\n",
    "#     # dv[i] out degree of node i+1\n",
    "#     dv = np.zeros(graph.node_num)\n",
    "#     # e[i,j] = 0: edge (i+1,j+1),(j+1,i+1) checked\n",
    "#     check_array = np.zeros((graph.node_num, graph.node_num))\n",
    "#     checked = 0\n",
    "# #     print(range(graph.node_num))\n",
    "#     for i in range(graph.node_num):\n",
    "#         # for a edge (i,j) and (j,i) may be count twice but the algorithm is to find a vertex cover. it doesn't mater\n",
    "#         dv[i] = graph.get_out_degree(i + 1) + graph.get_in_degree(i + 1)\n",
    "#     # V: Vertex cover\n",
    "#     V = set()\n",
    "# #     print(dv)\n",
    "#     while checked < graph.edge_num:\n",
    "#         s = dv.argmax() + 1\n",
    "#         V.add(s)\n",
    "#         # make sure that never to select this node again\n",
    "#         children = graph.get_children(s)\n",
    "#         parents = graph.get_parents(s)\n",
    "#         for child in children:\n",
    "#             if check_array[s - 1][child - 1] == 0:\n",
    "#                 check_array[s - 1][child - 1] = 1\n",
    "#                 checked = checked + 1\n",
    "#         for parent in parents:\n",
    "#             if check_array[parent - 1][s - 1] == 0:\n",
    "#                 check_array[parent - 1][s - 1] = 1\n",
    "#                 checked = checked + 1\n",
    "#         dv[s - 1] = -1\n",
    "#     return list(V)\n",
    "\n",
    "\n",
    "\n",
    "def forward(Q, D, spd, pp, r, W, U, spdW_u, graph):\n",
    "    x = Q[-1]\n",
    "    if U is None:\n",
    "        U = []\n",
    "    children = graph.get_children(x)\n",
    "    count = 0\n",
    "    while True:\n",
    "        # any suitable chid is ok\n",
    "\n",
    "#         for child in range(count, len(children)):\n",
    "        flag=1\n",
    "        for y in children:\n",
    "            \n",
    "#             print(\"y,D[x],type:\",y,D[x],type(D[x]))\n",
    "#             if(y not in D[x]):\n",
    "#                 print(\"YES\")\n",
    "                \n",
    "            if (y in W) and (y not in Q) and (y not in D[x]):\n",
    "#                 y = children[child]\n",
    "                flag=0\n",
    "                break\n",
    "#             count = count + 1\n",
    "\n",
    "        # no such child:\n",
    "        if flag==1:\n",
    "            return Q, D, spd, pp\n",
    "\n",
    "        if pp * graph.get_weight(x, y) < r:\n",
    "            D[x].append(y)\n",
    "        else:\n",
    "            Q.append(y)\n",
    "            pp = pp * graph.get_weight(x, y)\n",
    "            spd = spd + pp\n",
    "            D[x].append(y)\n",
    "            x = Q[-1]\n",
    "            for v in U:\n",
    "                if v not in Q:\n",
    "                    spdW_u[v] = spdW_u[v] + pp\n",
    "            children = graph.get_children(x)\n",
    "            count = 0\n",
    "\n",
    "\n",
    "            \n",
    "def backtrack(u, r, W, U, spdW_, graph):\n",
    "    Q = [u]\n",
    "    spd = 1\n",
    "    pp = 1\n",
    "    D = init_D(graph)\n",
    "\n",
    "    while len(Q) != 0:\n",
    "        Q, D, spd, pp = forward(Q, D, spd, pp, r, W, U, spdW_, graph)\n",
    "        u = Q.pop()\n",
    "#         print(\"In backtrack:type,Q,u\",type(Q),Q,u)\n",
    "        D[u] = []\n",
    "        if len(Q) != 0:\n",
    "            v = Q[-1]\n",
    "            pp = pp / graph.get_weight(v, u)\n",
    "    return spd\n",
    "\n",
    "\n",
    "\n",
    "def simpath_spread(S, r, U, graph, spdW_=None):\n",
    "    spread = 0\n",
    "    # W: V-S\n",
    "    W = set(graph.nodes).difference(S)\n",
    "    if U is None or spdW_ is None:\n",
    "        spdW_={}\n",
    "        for i in graph.nodes:\n",
    "            spdW_[i]=0\n",
    "#         spdW_ = np.zeros(graph.node_num + 1)\n",
    "        # print 'U None'\n",
    "    for u in S:\n",
    "        W.add(u)\n",
    "        # print spdW_[u]\n",
    "        spread = spread + backtrack(u, r, W, U, spdW_[u], graph)\n",
    "        # print spdW_[u]\n",
    "        W.remove(u)\n",
    "    return spread\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simpath(graph, k, r, l):\n",
    "    C = set(get_vertex_cover(graph))\n",
    "    V = set(graph.nodes)\n",
    "\n",
    "    V_C = V.difference(C)\n",
    "    # spread[x] is spd of S + x\n",
    "#     spread = np.zeros(graph.node_num + 1)\n",
    "    spread={}\n",
    "    for i in graph.nodes:\n",
    "        spread[i]=0\n",
    "#     spdV_ = np.ones((graph.node_num + 1, graph.node_num + 1))\n",
    "    spdV_={}\n",
    "    for i in graph.nodes:\n",
    "        dd={}\n",
    "        for j in graph.nodes:\n",
    "            dd[j]=0\n",
    "        spdV_[i]=dd\n",
    "        \n",
    "        \n",
    "    for u in C:\n",
    "        U = V_C.intersection(set(graph.get_parents(u)))\n",
    "        spread[u] = simpath_spread(set([u]), r, U, graph, spdV_)\n",
    "    for v in V_C:\n",
    "        v_children = graph.get_children(v)\n",
    "        for child in v_children:\n",
    "            spread[v] = spread[v] + spdV_[child][v] * graph.get_weight(v, child)\n",
    "        spread[v] = spread[v] + 1\n",
    "    celf = CELFQueue()\n",
    "    # put all nodes into celf queqe\n",
    "    # spread[v] is the marginal gain at this time\n",
    "    \n",
    "#     for node in range(1, graph.node_num + 1):\n",
    "#         celf.put(node, spread[node])\n",
    "    for node in graph.nodes:\n",
    "        celf.put(node, spread[node])\n",
    "    \n",
    "    S = set()\n",
    "    W = V\n",
    "    spd = 0\n",
    "    # mark the node that checked before during the same Si\n",
    "#     checked = np.zeros(graph.node_num + 1)\n",
    "    checked={}\n",
    "    for i in graph.nodes:\n",
    "        checked[i]=0\n",
    "\n",
    "    while len(S) < k:\n",
    "        U = celf.topn(l)\n",
    "#         spdW_ = np.ones((graph.node_num + 1, graph.node_num + 1))\n",
    "#         spdV_x = np.zeros(graph.node_num + 1)\n",
    "        \n",
    "        spdW_={}\n",
    "        spdV_x={}\n",
    "        for i in graph.nodes:\n",
    "            dd={}\n",
    "            spdV_x[i]=0\n",
    "            for j in graph.nodes:\n",
    "                dd[j]=1\n",
    "            spdW_[i]=dd\n",
    "        \n",
    "        simpath_spread(S, r, U, graph, spdW_=spdW_)\n",
    "        for x in U:\n",
    "            for s in S:\n",
    "                spdV_x[x] = spdV_x[x] + spdW_[s][x]\n",
    "        for x in U:\n",
    "            if checked[x] != 0:\n",
    "                S.add(x)\n",
    "                W = W.difference(set([x]))\n",
    "                spd = spread[x]\n",
    "                # print spread[x],simpath_spread(S,r,None,None)\n",
    "#                 checked = np.zeros(graph.node_num + 1)\n",
    "#                 for i in graph.nodes:\n",
    "#                     checked[i]=0\n",
    "                celf.remove(x)\n",
    "                break\n",
    "            else:\n",
    "                spread[x] = backtrack(x, r, W, None, None, graph) + spdV_x[x]\n",
    "                checked[x] = 1\n",
    "                celf.update(x, spread[x] - spd)\n",
    "    return S\n",
    "\n",
    "\n",
    "\n",
    "# node,edges,children,parents,nodenum,edge_num=read_graph_info('dolphins.txt') #Input dataset with 'total nodes' and 'total edges' in the 'first line'\n",
    "# graph = Graph(node,edges,children,parents,nodenum,edge_num)\n",
    "# seeds = simpath(graph, 3, 0.5, 4)\n",
    "# print(\"seed:\",seeds)\n",
    "\n",
    "\n",
    "\n",
    "def SIMPATH_setup(G):\n",
    "    parents = {}\n",
    "    children = {}\n",
    "    edges = {}\n",
    "    nodes = set()\n",
    "    node_num = len(G.nodes())\n",
    "    edge_num = len(G.edges())\n",
    "#     print(node_num,edge_num)\n",
    "    for src,des in G.edges():\n",
    "        nodes.add(src)\n",
    "        nodes.add(des)\n",
    "        if children.get(src) is None:\n",
    "            children[src] = []\n",
    "        if parents.get(des) is None:\n",
    "            parents[des] = []\n",
    "        weight=G[src][des]['weight']\n",
    "        edges[(src, des)] = weight\n",
    "        children[src].append(des)\n",
    "        parents[des].append(src)\n",
    "    return list(nodes), edges, children, parents, node_num, edge_num\n",
    "     \n",
    "def run_SIMPATH(G,k):\n",
    "    node,edges,children,parents,nodenum,edge_num=SIMPATH_setup(G)\n",
    "    graph = Graph(node,edges,children,parents,nodenum,edge_num)\n",
    "    seeds = simpath(graph, k, 0.5, k+1)\n",
    "    return list(seeds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vertex_cover(graph):\n",
    "#     dv = np.zeros(graph.node_num)\n",
    "    dv=dict()\n",
    "#     check_array = np.zeros((graph.node_num, graph.node_num))\n",
    "    check_array={}\n",
    "    for i in graph.nodes:\n",
    "        dd={}\n",
    "        for j in graph.nodes:\n",
    "            dd[j]=0\n",
    "        check_array[i]=dd\n",
    "        \n",
    "#     checked = 0\n",
    "    \n",
    "#     for i in range(graph.node_num):\n",
    "#         dv[i] =graph.get_out_degree(i + 1) + graph.get_in_degree(i + 1)\n",
    "    for i in graph.nodes:\n",
    "        dv[i]=graph.get_out_degree(i) + graph.get_in_degree(i)\n",
    "    \n",
    "    # V: Vertex cover\n",
    "    V = set()\n",
    "#     while checked < graph.edge_num:\n",
    "    for checked in graph.nodes:    \n",
    "#         s = dv.argmax() + 1\n",
    "        _,s=max(zip(dv.values(), dv.keys()))\n",
    "        V.add(s)\n",
    "        # make sure that never to select this node again\n",
    "        children = graph.get_children(s)\n",
    "        parents = graph.get_parents(s)\n",
    "        for child in children:\n",
    "            if check_array[s][child] == 0:\n",
    "                check_array[s][child] = 1\n",
    "#                 checked = checked + 1\n",
    "        for parent in parents:\n",
    "            if check_array[parent][s] == 0:\n",
    "                check_array[parent][s] = 1\n",
    "#                 checked = checked + 1\n",
    "        dv[s] = -1\n",
    "#     print(\"In Vertex conver V is:\",V)\n",
    "    return list(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- coding: utf-8 --\n",
    "# @Time : 2022/5/14 16:00\n",
    "# @Author : Mkc\n",
    "# @Email : mkc17@foxmail.com\n",
    "# @Software: PyCharm\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "from time import strftime, localtime\n",
    "\n",
    "def IC_model(graph,seeds,mc,p=0.01):\n",
    "    g=graph\n",
    "    S=seeds\n",
    "    spread = []\n",
    "    for i in range(mc):\n",
    "        new_active, Au = S[:], S[:]\n",
    "        while new_active:\n",
    "            new_ones = []\n",
    "            for node in new_active:\n",
    "                nbs = list(set(g.neighbors(node)) - set(Au))\n",
    "                for nb in nbs:\n",
    "                    if random.random() <= p:\n",
    "                        new_ones.append(nb)\n",
    "            new_active = list(set(new_ones))\n",
    "            Au += new_active\n",
    "        spread.append(len(Au))\n",
    "    return np.mean(spread)\n",
    "\n",
    "\n",
    "#     influnces = seeds[:]\n",
    "#     queue = influnces[:]\n",
    "#     pre_node_record = defaultdict(float)\n",
    "# #     print(\"Queue:\",queue)\n",
    "# #     print(\"Influences:\",influnces)\n",
    "#     while len(queue) != 0:\n",
    "#         node = queue.pop(0)\n",
    "# #         print(\"----------------------------------------------------------------------\")\n",
    "# #         print(\"Take node:\",node)\n",
    "# #         print(\"Neighbour:\",graph[node])\n",
    "#         for element in graph[node]:\n",
    "#             if element not in influnces:\n",
    "# #                 print(\"Element:\",element,\"prerecored\",pre_node_record[element])\n",
    "#                 pre_node_record[element] = pre_node_record[element] + graph[node][element]['weight']\n",
    "# #                 print(pre_node_record[element])\n",
    "#                 if pre_node_record[element] >= graph.nodes[element]['thres']:\n",
    "# #                     print(\">>>>>>>>>>>>>>>>>>node influeced:\",element)\n",
    "#                     influnces.append(element)\n",
    "#                     queue.append(element)\n",
    "# #     influnce_num = len(influnces)\n",
    "# #     print(\"Seed set:\",seeds,\"Activated nodes:\",influnces)\n",
    "#     return len(influnces)\n",
    "# # linear_Threshold(GG,[45,29])\n",
    "\n",
    "#EDV\n",
    "def Eval(G, S):\n",
    "    Neighbors = Neighbor_Nodeset(G, S)\n",
    "\n",
    "    fitness = len(S)\n",
    "    L = list(set(Neighbors) - set(S))\n",
    "    for TIME in L:\n",
    "        fitness += 1 - (1 - 0.01) ** len(set(G.neighbors(TIME)) & set(S))\n",
    "    return fitness\n",
    "\n",
    "def Neighbor_nodes(G,u):\n",
    "    return list(G.neighbors(u)) + [u]\n",
    "\n",
    "def Neighbor_Nodeset(G,S):\n",
    "    neighbors = [ ]\n",
    "    for i in S:\n",
    "        neighbors += Neighbor_nodes(G,i)\n",
    "    neighbors = list(set(neighbors))\n",
    "    return neighbors\n",
    "\n",
    "def pop_init(nodes,pop,K,t,avg_d):\n",
    "    P = []\n",
    "\n",
    "    for i in range(pop):\n",
    "        P_Item = []\n",
    "        for Kt in range(K):\n",
    "            temp = math.ceil((Kt+1) * math.exp(t * avg_d))+Kt+1\n",
    "            if temp>len(nodes):\n",
    "                temp = len(nodes)\n",
    "\n",
    "            P_Item.append(nodes[random.randint(0,temp-1)])\n",
    "        P.append(P_Item)\n",
    "\n",
    "    return P\n",
    "\n",
    "def mutation(G,P,P_remain,mu,K,t,avg_d,nodes):\n",
    "    P_new = copy.deepcopy(P)\n",
    "\n",
    "    for P_It in P_new:\n",
    "        if P_new.index(P_It) == 0:\n",
    "            temp = math.ceil(K * math.exp(t * avg_d))+K\n",
    "            if temp <= K:\n",
    "                temp += 1\n",
    "            if temp>len(nodes):\n",
    "                temp = len(nodes)\n",
    "\n",
    "            ran_int = random.randint(0,K-1)\n",
    "\n",
    "            while True:\n",
    "                ran_temp = random.randint(0,temp-1)\n",
    "                if nodes[ran_temp] not in P_It:\n",
    "                    P_It[ran_int] = nodes[ran_temp]\n",
    "                    break\n",
    "            continue\n",
    "\n",
    "        if P_new.index(P_It) < len(P_new)/2:\n",
    "            temp = math.ceil(K * math.exp(t * avg_d))+K\n",
    "        else:\n",
    "            temp = math.ceil((K-1) * math.exp(t * avg_d))+K-1\n",
    "\n",
    "        if temp <= K:\n",
    "            temp += 1\n",
    "        if temp > len(nodes):\n",
    "            temp = len(nodes)\n",
    "\n",
    "        # candidate nodes\n",
    "        node_set = []\n",
    "        # probability of candidate nodes being selected\n",
    "        degree_pro = []\n",
    "\n",
    "        for i in range(temp):\n",
    "            node_set.append(nodes[i])\n",
    "            degree_pro.append(G.degree(nodes[i]))\n",
    "\n",
    "        degree_sum = sum(degree_pro)\n",
    "        for deg in range(len(degree_pro)):\n",
    "            degree_pro[deg] = degree_pro[deg] / degree_sum\n",
    "        degree_pro = np.array(degree_pro)\n",
    "\n",
    "        for index in range(len(P_It)):\n",
    "            if P_It[index] not in P_remain[P_new.index(P_It)]:\n",
    "                if random.random() < mu:\n",
    "                    while True:\n",
    "                        temp_d = np.random.choice(node_set, size=1, p=degree_pro.ravel())\n",
    "\n",
    "                        if temp_d[0] not in P_It:\n",
    "                            P_It[index] = temp_d[0]\n",
    "                            break\n",
    "    return P_new\n",
    "\n",
    "def crossover(P,cr,pop):\n",
    "    P_c = []\n",
    "    P_remain = []\n",
    "\n",
    "    random_int = 0\n",
    "    P_c.append(P[random_int])\n",
    "\n",
    "    P_remain.append([])\n",
    "    for pop_index in range(pop):\n",
    "        if pop_index == random_int:\n",
    "            continue\n",
    "\n",
    "        ind = list(set(P[random_int]) & set(P[pop_index]))\n",
    "        P_remain.append(ind)\n",
    "        P_1 = copy.deepcopy(P)\n",
    "        P_1[random_int] = list(set(P_1[random_int]) - set(ind))\n",
    "        P_1[pop_index] = list(set(P_1[pop_index]) - set(ind))\n",
    "\n",
    "        for kk in range(len(P_1[random_int])):\n",
    "            if random.random() < cr:\n",
    "                ind.append(P_1[random_int][kk])\n",
    "            else:\n",
    "                ind.append(P_1[pop_index][kk])\n",
    "        P_c.append(ind)\n",
    "    return P_c,P_remain\n",
    "\n",
    "\n",
    "def main(G,k):\n",
    "#     for address in [\"dolphins_edge_list.txt\"]:\n",
    "#         print(address)\n",
    "    for K in [k]:\n",
    "        result1 = []#influence spread\n",
    "        result2 = []#running time\n",
    "        for A in range(2):\n",
    "#             print(strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "#                 G = nx.read_edgelist(address, create_using=nx.Graph())\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            nodes = list(G.nodes)\n",
    "            edges = list(G.edges)\n",
    "            nodes = sorted(nodes, key=lambda x: G.degree(x), reverse=True)\n",
    "\n",
    "            pop = 10\n",
    "            t = 0.04\n",
    "            avg_d = round(2*len(edges)/len(nodes),2)\n",
    "            mu = 0.1\n",
    "            cr = 0.6\n",
    "            maxgen = 15\n",
    "\n",
    "            #initialization\n",
    "            P = pop_init(nodes,pop,1,t,avg_d)\n",
    "\n",
    "            for KK in range(2,K+1):\n",
    "                i = 0\n",
    "                #seed addition\n",
    "                temp = math.ceil(KK * math.exp(t * avg_d)) + KK\n",
    "                if temp>len(nodes):\n",
    "                    temp = len(nodes)\n",
    "\n",
    "                for ind in range(len(P)):\n",
    "                    while True:\n",
    "                        ran_index = random.randint(0,temp-1)\n",
    "                        if nodes[ran_index] not in P[ind]:\n",
    "                            P[ind].append(nodes[ran_index])\n",
    "                            break\n",
    "\n",
    "                while i < maxgen:\n",
    "                    P = sorted(P,key=lambda x:Eval(G,x),reverse=True)\n",
    "\n",
    "                    # mutation&crossover\n",
    "                    P_cross, P_remain = crossover(P, cr, pop)\n",
    "                    P_mutation = mutation(G,P_cross, P_remain, mu, KK, t, avg_d, nodes)\n",
    "\n",
    "                    #selection\n",
    "                    for index in range(pop):\n",
    "                        Inf1 = Eval(G, P_mutation[index])\n",
    "                        Inf2 = Eval(G, P[index])\n",
    "                        if Inf1 > Inf2:\n",
    "                            P[index] = P_mutation[index]\n",
    "                    i += 1\n",
    "\n",
    "            solution = sorted(P,key=lambda x:Eval(G,x),reverse=True)[0]\n",
    "\n",
    "            end_time = time.perf_counter()\n",
    "            runningtime = end_time-start_time\n",
    "\n",
    "            result1.append(IC_model(G, solution, 10000))\n",
    "            result2.append(runningtime)\n",
    "\n",
    "#         print(result1)\n",
    "#         print(result2)\n",
    "#         print( \"K:\", K, \"Influence spread:\", round(np.mean(result1), 1),\"Running time:\",round(np.mean(result2),1))\n",
    "#         print(\"seed set:\",solution)\n",
    "    return solution\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "def runSSR(G,k):\n",
    "    seedset=main(G,k)\n",
    "#     print(\"end:\",seedset)\n",
    "    return seedset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import operator\n",
    "from collections import deque\n",
    "\n",
    "class SocialNetwork:\n",
    "    def __init__(self, networkx_graph, num_seeds):\n",
    "        self.networkx_graph = networkx_graph\n",
    "        self.num_seeds = num_seeds\n",
    "\n",
    "    def influence(self, seeds):\n",
    "        influences = set(seeds)\n",
    "        queue = deque(seeds)\n",
    "        pre_node_record = defaultdict(float)\n",
    "        while queue:\n",
    "            node = queue.popleft()\n",
    "            for neighbor in self.networkx_graph[node]:\n",
    "                if neighbor not in influences:\n",
    "                    pre_node_record[neighbor] = pre_node_record[neighbor] + self.networkx_graph[node][neighbor]['weight']\n",
    "                    # Check if the neighbor is not already influenced and has reached the threshold\n",
    "                    if pre_node_record[neighbor] >= self.networkx_graph.nodes[neighbor]['thres']:\n",
    "                        influences.add(neighbor)\n",
    "                        queue.append(neighbor)\n",
    "\n",
    "        return len(influences)\n",
    "\n",
    "\n",
    "class CuckooSearch:\n",
    "    def __init__(self, social_network, comm_set_final, prev_seed_set, G_new, population_size=1000, levy_flights_exponent=1.5):\n",
    "        self.social_network = social_network\n",
    "        self.population_size = population_size\n",
    "        # self.num_of_cuckoos = int(0.1 * population_size)\n",
    "        self.num_of_cuckoos = random.randint(int(population_size/2),population_size)\n",
    "        \n",
    "        self.levy_flights_exponent = levy_flights_exponent\n",
    "        self.res = {}\n",
    "\n",
    "        closeness_centrality = nx.closeness_centrality(social_network.networkx_graph)\n",
    "        degree_centrality = nx.degree_centrality(social_network.networkx_graph)\n",
    "\n",
    "        candidate_nodes = set(G_new.nodes)\n",
    "        new_candidate = list(candidate_nodes.union(prev_seed_set))\n",
    "        self.new_candidate = new_candidate\n",
    "\n",
    "        for node in new_candidate:\n",
    "            x =self.social_network.influence([node])\n",
    "            degree = degree_centrality[node]\n",
    "            close = closeness_centrality[node]\n",
    "            social_network.networkx_graph.nodes[node].update({'degree': degree, 'close': close, 'DV': x})\n",
    "            social_network.networkx_graph.nodes[node]['res1'] = degree * x\n",
    "            social_network.networkx_graph.nodes[node]['res2'] = close * x\n",
    "\n",
    "        for node in new_candidate:\n",
    "            self.res[node] = max(social_network.networkx_graph.nodes[node]['res1'], social_network.networkx_graph.nodes[node]['res2'])\n",
    "\n",
    "            \n",
    "#         print(\"Hello:\",self.social_network)\n",
    "        k = social_network.num_seeds\n",
    "        self.population = [random.sample(new_candidate, k) for _ in range(population_size)]\n",
    "        self.influence = {tuple(new_list): social_network.influence(new_list) for new_list in self.population}\n",
    "\n",
    "        self.cuckoos = [random.sample(new_candidate, k) for _ in range(self.num_of_cuckoos)]\n",
    "        self.cuckoo_influence = {tuple(new_list): social_network.influence(new_list) for new_list in self.cuckoos}\n",
    "\n",
    "        best_cuckoo = max(self.cuckoo_influence.items(), key=operator.itemgetter(1))\n",
    "        self.best_cuckoo = {best_cuckoo[0]: best_cuckoo[1]}\n",
    "\n",
    "    def find_updated_solution(self, key, best_cuckoo):\n",
    "        best_cuckoo = list(best_cuckoo)[0]\n",
    "        best_cuckoo_nodes = set(best_cuckoo)\n",
    "        new_candidate = list(set(key) | best_cuckoo_nodes)\n",
    "        solu = {element: self.res[element] for element in new_candidate}\n",
    "\n",
    "        sorted_items = sorted(solu.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k_items = sorted_items[:len(key)]\n",
    "        top_k_keys = [item[0] for item in top_k_items]\n",
    "        return top_k_keys\n",
    "\n",
    "    def update_population(self):\n",
    "        cuckoo_key = list(self.best_cuckoo.keys())[0]\n",
    "        best_cuckoo = set(cuckoo_key)\n",
    "        best_cuckoo_inf = self.best_cuckoo[cuckoo_key]\n",
    "\n",
    "        add_dict = {}\n",
    "        dell_dict = {}\n",
    "\n",
    "        for _ in range(self.population_size):\n",
    "            key = random.choice(list(self.influence.keys()))\n",
    "            if self.influence[key] < best_cuckoo_inf:\n",
    "                new_solution = self.find_updated_solution(key, self.best_cuckoo)\n",
    "                new_inf = self.social_network.influence(new_solution)\n",
    "\n",
    "                if new_inf > self.influence[key]:\n",
    "                    dell_dict.clear()\n",
    "                    add_dict.clear()\n",
    "                    dell_dict[key] = self.influence[key]\n",
    "                    add_dict[tuple(new_solution)] = new_inf\n",
    "\n",
    "                    new_key_value_pair = {tuple(new_solution): new_inf}\n",
    "                    self.influence.update({tuple(new_solution): self.influence.pop(key)})\n",
    "                    self.influence[tuple(new_solution)] = new_inf\n",
    "\n",
    "                if new_inf > best_cuckoo_inf:\n",
    "                    best_cuckoo_inf = new_inf\n",
    "                    best_cuckoo = set(new_solution)\n",
    "                    self.best_cuckoo.clear()\n",
    "                    self.best_cuckoo[tuple(new_solution)] = best_cuckoo_inf\n",
    "\n",
    "            else:\n",
    "                best_cuckoo_inf = self.influence[key]\n",
    "                best_cuckoo = set(key)\n",
    "                self.best_cuckoo.clear()\n",
    "                self.best_cuckoo[tuple(best_cuckoo)] = best_cuckoo_inf\n",
    "\n",
    "            # Introduce randomness for diversity\n",
    "            alien_finder = random.uniform(0, 1)\n",
    "            if alien_finder < 0.25:\n",
    "                if key in self.influence.keys():\n",
    "                    new_list = random.sample(self.new_candidate, self.social_network.num_seeds)\n",
    "                    new_inf = self.social_network.influence(new_list)\n",
    "                    self.influence[tuple(new_list)] = self.influence.pop(key)\n",
    "                    self.influence[tuple(new_list)] = new_inf\n",
    "\n",
    "    def run(self, max_iterations=5):\n",
    "#         for i in self.social_network.nodes():\n",
    "#             print()\n",
    "#         print(\"Hello:\",self.social_network)\n",
    "        for _ in range(max_iterations):\n",
    "            self.update_population()\n",
    "        return list(self.best_cuckoo.keys())[0]\n",
    "\n",
    "def CSOrun(G, k, comm_set_final, prev_seed_set, G_new):\n",
    "    print(G)\n",
    "    social_network = SocialNetwork(G, k)\n",
    "    cuckoo_search = CuckooSearch(social_network, comm_set_final, prev_seed_set, G_new)\n",
    "    seed_set = cuckoo_search.run()\n",
    "    return list(seed_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort node names based on your desired range\n",
    "def preprocess(G):\n",
    "    current_names = list(sorted(G.nodes()))\n",
    "\n",
    "    # Sort node names based on your desired range\n",
    "    num_nodes = len(current_names)\n",
    "    sorted_names = list(range(0, num_nodes))\n",
    "\n",
    "    # Create a mapping from current names to sorted names\n",
    "    mapping = {current_names[i]: sorted_names[i] for i in range(num_nodes)}\n",
    "\n",
    "    # Rename nodes\n",
    "    nx.relabel_nodes(G, mapping, copy=False)\n",
    "    return G\n",
    "\n",
    "\n",
    "# Now your graph nodes are renamed from 1 to the number of nodes\n",
    "# You can print the nodes to verify\n",
    "# print(sorted(G.nodes()))\n",
    "# weighted_degrees = dict(nx.degree(G))\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def GetCommunities(file_path):\n",
    "    b = {}\n",
    "    list_of_lists = []\n",
    "    file_path=comm_file_path\n",
    "    # Open the file in read mode\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Iterate through each line in the file\n",
    "        # a = 0\n",
    "        for line in file:\n",
    "            # Strip the newline character from the end of the line and split by spaces\n",
    "            elements = line.strip().split()\n",
    "            b[int(elements[0])] = int(elements[1])\n",
    "            # for i in range(len(elements)):\n",
    "            #     b[int(elements[i])] = a\n",
    "            # a+=1\n",
    "    return b\n",
    "\n",
    "\n",
    "def sort_nodes(nodes, centrality):\n",
    "    \"\"\"\n",
    "    Sorts the Nodes in the given list of nodes based on the corresponding Centrality Measure of each node\n",
    "    Parameters:\n",
    "        nodes = List of nodes which has to be sorted\n",
    "        centrality = List of Centrality Measures corresponding to each node in the list\n",
    "    Returns:\n",
    "        List of nodes sorted on the basis of their Centrality Measure in descending order\n",
    "    \"\"\"\n",
    "    result = sorted(list(zip(centrality, nodes)), reverse = True)\n",
    "    result = zip(*result)\n",
    "    result = [list(tuple) for tuple in result]\n",
    "    return result[1]\n",
    "\n",
    "def sort_degree_centrality(G,nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Degree Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    return sort_nodes(nodes,centrality)\n",
    "\n",
    "def sort_eigenvector_centrality(G,nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Eigenvector Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    cen_measure = []\n",
    "    for i in nodes:\n",
    "        cen_measure.append(centrality[i])\n",
    "    return sort_nodes(nodes, cen_measure)\n",
    "\n",
    "def sort_betweenness_centrality(G,nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Betweenness Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.betweenness_centrality(G)\n",
    "    return centrality\n",
    "\n",
    "def sort_closeness_centrality(G,nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Closeness Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.closeness_centrality(G)\n",
    "    return centrality\n",
    "\n",
    "def sort_katz_centrality(G,nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Katz Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    cen_measure = []\n",
    "    for i in nodes:\n",
    "        cen_measure.append(centrality[i])\n",
    "    return sort_nodes(nodes, cen_measure)\n",
    "\n",
    "def sort_percolation_centrality(G,nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Percolation Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    cen_measure = []\n",
    "    for i in nodes:\n",
    "        cen_measure.append(centrality[i])\n",
    "    return sort_nodes(nodes, cen_measure)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def one_hop_area(G,meme):\n",
    "\n",
    "    temp = [] # Temporary list to store the One-Hop Area nodes of the meme\n",
    "    for i in meme:\n",
    "        temp.extend(list(G.neighbors(i)))\n",
    "    return list(set(temp) - set(meme)) # Returning only unique nodes, to avoid Node Repetition\n",
    "\n",
    "def two_hop_area(G,meme):\n",
    "\n",
    "    one_hop = one_hop_area(G,meme)\n",
    "    temp = [] # Temporary list to store the Two-Hop Area nodes of the meme\n",
    "    for i in one_hop:\n",
    "        temp.extend(list(G.neighbors(i)))\n",
    "    return list(set(temp) - set(meme) - set(one_hop)) # Returning only unique nodes, to avoid Node Repetition\n",
    "\n",
    "def calc_pcm_prob(G,nodes,weighted_degrees):\n",
    "    prob = [] # Temporary List to store the corresponding Cascade Probability of the Nodes\n",
    "    for i in nodes:\n",
    "        prob.append(weighted_degrees[i] / G.number_of_nodes())\n",
    "    return prob\n",
    "\n",
    "def calc_edges(G,group1, group2):\n",
    "    count = [] # Temporary storage to store the Number of edges corresponding each node\n",
    "    for i in group2:\n",
    "        edges = list(nx.edges(G, nbunch = [i]))\n",
    "        temp = 0\n",
    "        for i in edges:\n",
    "            if (i[1] in group1) or (i[1] in group2):\n",
    "                temp += 1\n",
    "        count.append(temp)\n",
    "    return count\n",
    "\n",
    "def sum_pd(list1, list2):\n",
    "    p = 0 # Temporary variable to store the Sum-Product of the two lists\n",
    "    for i in range(len(list1)):\n",
    "        p += (list1[i] * list2[i])\n",
    "    return p\n",
    "\n",
    "def common_neighbors_directed(graph, u, v):\n",
    "    # Ensure the graph is directed\n",
    "    if not graph.is_directed():\n",
    "        raise ValueError(\"The graph must be directed.\")\n",
    "    \n",
    "    # Predecessors of u and v\n",
    "    pred_u = set(graph.predecessors(u))\n",
    "    pred_v = set(graph.predecessors(v))\n",
    "    \n",
    "    # Successors of u and v\n",
    "    succ_u = set(graph.successors(u))\n",
    "    succ_v = set(graph.successors(v))\n",
    "    \n",
    "    # Common neighbors are those nodes that are both successors and predecessors of u and v\n",
    "    common_neighbors = (pred_u & pred_v) | (succ_u & succ_v)\n",
    "    \n",
    "    return common_neighbors\n",
    "\n",
    "def calc_edge_prob(G,meme, one_hop):\n",
    "    N = G.number_of_nodes() # Total Nodes in the Graph\n",
    "    prob_sum = 0\n",
    "    for i in one_hop:\n",
    "        prob_prod = 1\n",
    "        for j in meme:\n",
    "            pij = 0.01\n",
    "            pij += ((G.degree(i) + G.degree(j)) / N)\n",
    "            pij += (len(list(common_neighbors_directed(G, i, j))) / N)\n",
    "            prob_prod *= (1 - pij)\n",
    "        prob_sum += (1 - prob_prod)\n",
    "    return prob_sum\n",
    "\n",
    "def LIE(G,meme,weighted_degrees):\n",
    "    k=len(meme)\n",
    "    Ns1_S = one_hop_area(G,meme) # One-Hop area of the Meme\n",
    "    Ns2_S = two_hop_area(G,meme) # Two-Hop Area of the Meme\n",
    "    pu = calc_pcm_prob(G,Ns2_S,weighted_degrees)\n",
    "    du = calc_edges(G,Ns1_S, Ns2_S)\n",
    "    return  k + ((1 + ((1 / len(Ns1_S)) * sum_pd(pu, du))) * calc_edge_prob(G,meme, Ns1_S)) if len(Ns1_S) !=0 else 0\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def pareto_dominance(seed_set_x, seed_set_y):\n",
    "    \"\"\"Check if seed set x dominates seed set y.\"\"\"\n",
    "    all_less_equal = seed_set_x['F'] >=seed_set_y['F'] and seed_set_x['sigma'] >= seed_set_y['sigma']\n",
    "    any_strictly_greater = seed_set_x['F'] > seed_set_y['F'] or seed_set_x['sigma'] > seed_set_y['sigma']\n",
    "    return all_less_equal and any_strictly_greater\n",
    "\n",
    "def pareto_optimal_seed_sets(seed_sets):\n",
    "    \"\"\"Identify Pareto-optimal seed sets from a list of seed sets.\"\"\"\n",
    "    pareto_optimal_sets = []\n",
    "    for seed_set_x in seed_sets:\n",
    "        if not any(pareto_dominance(seed_set_y, seed_set_x) for seed_set_y in seed_sets if seed_set_x != seed_set_y):\n",
    "            pareto_optimal_sets.append(seed_set_x)\n",
    "    return pareto_optimal_sets\n",
    "\n",
    "def pareto_rank(seed_sets):\n",
    "    \"\"\"Assign Pareto rank to seed sets.\"\"\"\n",
    "    ranks = [0] * len(seed_sets)\n",
    "    current_rank = 0\n",
    "    while True:\n",
    "        pareto_sets = []\n",
    "        for i, seed_set in enumerate(seed_sets):\n",
    "            if ranks[i] != 0:\n",
    "                continue  # Already assigned a rank\n",
    "            if all(not pareto_dominance(seed_set, other_set) for j, other_set in enumerate(seed_sets) if i != j and ranks[j] == 0):\n",
    "                pareto_sets.append(seed_set)\n",
    "                ranks[i] = current_rank + 1\n",
    "        if not pareto_sets:\n",
    "            break\n",
    "        current_rank += 1\n",
    "    for i in range(len(seed_sets)):\n",
    "      seed_sets[i]['rank'] = ranks[i]\n",
    "\n",
    "\n",
    "def crowding_distance(seed_sets):\n",
    "    \"\"\"Calculate crowding distance for seed sets based on their objectives.\"\"\"\n",
    "    for seed_set in seed_sets:\n",
    "        seed_set['distance'] = 0\n",
    "    seed_sets.sort(key=lambda x: x['F'])\n",
    "    seed_sets[0]['distance'] = float('inf')\n",
    "    seed_sets[len(seed_set)-1]['distance'] = float('inf')\n",
    "    rnge = seed_sets[len(seed_sets)-1]['F'] - seed_sets[0]['F']\n",
    "    for i in range(1,len(seed_sets)-1):\n",
    "      if rnge != 0:\n",
    "        seed_sets[i]['distance'] += (seed_sets[i+1]['F'] - seed_sets[i-1]['F'])/ rnge\n",
    "      else:\n",
    "        seed_sets[i]['distance'] = float('inf')\n",
    "\n",
    "    seed_sets.sort(key = lambda x: x['sigma'])\n",
    "    seed_sets[0]['distance'] = float('inf')\n",
    "    seed_sets[len(seed_sets)-1]['distance'] = float('inf')\n",
    "    rnge = seed_sets[len(seed_sets)-1]['sigma'] - seed_sets[0]['sigma']\n",
    "    for i in range(1,len(seed_sets)-1):\n",
    "      if rnge != 0:\n",
    "        seed_sets[i]['distance'] += (seed_sets[i+1]['sigma'] - seed_sets[i-1]['sigma'])/ rnge\n",
    "      else:\n",
    "        seed_sets[i]['distance'] = float('inf')\n",
    "\n",
    "\n",
    "def select_seed_sets(candidate_sets, Q):\n",
    "    \"\"\"Select Q seed sets from candidate sets.\"\"\"\n",
    "    selected = []\n",
    "#     print(candidate_sets)\n",
    "    candidate_sets.sort(key=lambda x: (-x['rank'], x['distance']))\n",
    "    selected = candidate_sets[:Q]\n",
    "    return selected\n",
    "\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "import collections\n",
    "def divide_nodes_into_groups(n, m):\n",
    "    if n.number_of_nodes() < m:\n",
    "        raise ValueError(\"Number of nodes must be greater than or equal to the number of groups.\")\n",
    "\n",
    "    # Initialize an empty list of groups\n",
    "    groups = [[] for _ in range(m)]\n",
    "\n",
    "    # Assign each node to a random group\n",
    "    nodes = list(n.nodes())\n",
    "    random.shuffle(nodes)\n",
    "    for i, node in enumerate(nodes):\n",
    "        group_index = i % m  # Cycle through groups\n",
    "        groups[group_index].append(node)\n",
    "\n",
    "    return groups\n",
    "\n",
    "def simulate_diffusion(graph, seeds, p=0.8):\n",
    "    influenced_nodes = set(seeds)\n",
    "    new_nodes = set(seeds)\n",
    "\n",
    "    while new_nodes:\n",
    "        current_nodes = new_nodes.copy()\n",
    "        new_nodes = set()\n",
    "\n",
    "        for node in current_nodes:\n",
    "            neighbors = set(graph.neighbors(node)) - influenced_nodes\n",
    "            for neighbor in neighbors:\n",
    "                edge_weight = graph[node][neighbor]['weight']\n",
    "                if edge_weight >= p:\n",
    "                    new_nodes.add(neighbor)\n",
    "                    influenced_nodes.add(neighbor)\n",
    "    return list(influenced_nodes)\n",
    "\n",
    "def calculate_hoover_index(network, seed_set,groups):\n",
    "    influenced_nodes = simulate_diffusion(network, seed_set)\n",
    "    # Calculate fractions of influenced nodes in each group\n",
    "    fractions = [len(set(group) & set(influenced_nodes)) / len(group) for group in groups]\n",
    "    N = len(fractions)\n",
    "    T = 0\n",
    "    A = 0\n",
    "    for i in range (0,N):\n",
    "        T += fractions[i]\n",
    "        for j in range (0,len(fractions)):\n",
    "            A = A + abs(fractions[i] - fractions[j])\n",
    "\n",
    "    hoover_index = (A)/(2*N*T)\n",
    "    # print(hoover_index)\n",
    "    return hoover_index\n",
    "\n",
    "def calculate_influence_spread(network, seed_set):\n",
    "    d = simulate_diffusion(network,seed_set)\n",
    "    # print(d)\n",
    "    return len(d)\n",
    "    \n",
    "\n",
    "def calculate_F(network, seed_set,groups):\n",
    "    \"\"\"Calculate fairness metric F(S) for a given seed set.\"\"\"\n",
    "    # gini_coefficient = calculate_gini_coefficient(network, seed_set,0.5)\n",
    "    hoover_index = calculate_hoover_index(network,seed_set,groups)\n",
    "\n",
    "    return 1 - hoover_index\n",
    "\n",
    "def calculate_sigma(network, seed_set):\n",
    "    \"\"\"Calculate influence spread sigma(S) for a given seed set.\"\"\"\n",
    "    weighted_degrees = dict(nx.degree(network))\n",
    "    return LIE(network,seed_set,weighted_degrees)\n",
    "\n",
    "def seed_set_evaluation(network, seed_sets,groups,b):\n",
    "    \"\"\"Evaluate F(S) and sigma(S) for a list of seed sets.\"\"\"\n",
    "    info = []\n",
    "    \n",
    "    for seed_set in seed_sets:\n",
    "        F = calculate_F(network, seed_set,groups)\n",
    "        sigma = calculate_sigma(network, seed_set)\n",
    "        inf_spr = calculate_influence_spread(network,seed_set)\n",
    "        \n",
    "        info.append({'seed':seed_set,'F':F,'sigma':sigma, 'spread':inf_spr})\n",
    "#     qwe = set()\n",
    "#     for i in range(len(inf)):\n",
    "#         qwe.add(b.get(int(inf[i]),0))\n",
    "    # print(qwe)\n",
    "    return info\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "def uniform_mutation(seed_set, mutation_probability, nodes):\n",
    "    \"\"\"Apply uniform mutation to a seed set.\"\"\"\n",
    "    mutated_seed_set = []\n",
    "    for gene in seed_set:\n",
    "        if random.random() < mutation_probability:\n",
    "            new_gene = random.choice(list(set(nodes) - set(seed_set)))\n",
    "            mutated_seed_set.append(new_gene)\n",
    "        else:\n",
    "            mutated_seed_set.append(gene)\n",
    "    return mutated_seed_set\n",
    "\n",
    "def uniform_crossover(parent1, parent2, crossover_probability):\n",
    "    \"\"\"Apply uniform crossover between two parent seed sets.\"\"\"\n",
    "    if random.random() < crossover_probability:\n",
    "        child1, child2 = [], []\n",
    "        for gene1, gene2 in zip(parent1, parent2):\n",
    "            if random.random() < 0.5:\n",
    "                child1.append(gene1)\n",
    "                child2.append(gene2)\n",
    "            else:\n",
    "                child1.append(gene2)\n",
    "                child2.append(gene1)\n",
    "        return child1, child2\n",
    "    else:\n",
    "        return parent1, parent2\n",
    "\n",
    "def initialize_particle(G,npop,k):\n",
    "    position=[]\n",
    "    for i in range(0,npop):\n",
    "      position.append(random.sample(range(G.number_of_nodes()),k))\n",
    "    return position\n",
    "\n",
    "def initialize_populationf(num_individuals, nodes,k):\n",
    "    \"\"\"Initialize the population with a combination of random and prior seed sets.\"\"\"\n",
    "    population = []\n",
    "    for _ in range(num_individuals):\n",
    "        seed_set = random.sample(list(nodes), k)\n",
    "        population.append(seed_set)\n",
    "    return population\n",
    "\n",
    "# In[12]:\n",
    "def influence_maximization_with_fairness(num_generations,network, num_individuals,  M_p, C_p, Q,groups,b):\n",
    "    \"\"\"Run influence maximization considering fairness.\"\"\"\n",
    "    # Preprocessing\n",
    "    prior_knowledge = list(sort_degree_centrality(network,network.nodes()))\n",
    "    prior_knowledge = prior_knowledge[:Q]\n",
    "    # Initialization\n",
    "    population = initialize_populationf(num_individuals, network.nodes(), Q)\n",
    "#     print(population)\n",
    "    population.append(prior_knowledge)\n",
    "#     print(population)\n",
    "    info = []\n",
    "    for _ in range(1, num_generations+1):\n",
    "        # Seed Set Evolution\n",
    "        offspring_population = []\n",
    "        for _ in range(num_individuals):\n",
    "            parent1, parent2 = random.sample(population, 2)\n",
    "            child1, child2 = uniform_crossover(parent1, parent2, C_p)\n",
    "            child1 = uniform_mutation(child1, M_p, network.nodes())\n",
    "            child2 = uniform_mutation(child2, M_p, network.nodes())\n",
    "            offspring_population.extend([child1, child2])\n",
    "\n",
    "        # Seed Set Evaluation\n",
    "        seed_sets = offspring_population\n",
    "#         print(seed_sets)\n",
    "        info = seed_set_evaluation(network, seed_sets,groups,b)\n",
    "        pareto_rank(info)\n",
    "        crowding_distance(info)\n",
    "    # Selection\n",
    "    population = select_seed_sets(info, 1)\n",
    "    return population\n",
    "\n",
    "def jaya_optimize(G,k,npop, max_iter, num_nodes,groups,b):\n",
    "    swarm = initialize_particle(G,npop,k)\n",
    "    # print(swarm)\n",
    "    # return\n",
    "    prev_swarm = copy.deepcopy(swarm)\n",
    "    info = []\n",
    "    ert = set()\n",
    "    # print(max_iter)\n",
    "    for iter in range(max_iter):\n",
    "        # print(swarm)\n",
    "        best_fitness = swarm[0]\n",
    "        worst_fitness = swarm[0]\n",
    "        # print(type(best_fitness))\n",
    "        # return\n",
    "        b_val = calculate_influence_spread(G,best_fitness)\n",
    "        w_val = calculate_influence_spread(G,worst_fitness)\n",
    "        for i in range(len(swarm)):\n",
    "          val = calculate_influence_spread(G,swarm[i])\n",
    "          if val > b_val:\n",
    "             best_fitness = swarm[i]\n",
    "             b_val = val\n",
    "          elif val < w_val:\n",
    "             worst_fitness = swarm[i]\n",
    "             w_val = val\n",
    "        for i, particle in enumerate(swarm):\n",
    "          fitness = calculate_influence_spread(G,particle)\n",
    "          current = []\n",
    "          present = set()\n",
    "          not_present = set(G.nodes() - particle)\n",
    "          for j in range(k):\n",
    "              update_val = particle[j] + (random.random())*(best_fitness[j] - particle[j]) - (random.random())*(worst_fitness[j] - particle[j])\n",
    "              update_val = round(update_val)\n",
    "              if update_val in present:\n",
    "                  update_val = random.choice(list(not_present))\n",
    "                  present.add(update_val)\n",
    "                  not_present.remove(update_val)\n",
    "              elif update_val not in not_present:\n",
    "                  update_val = random.choice(list(not_present))\n",
    "                  present.add(update_val)\n",
    "                  not_present.remove(update_val)\n",
    "              current.append(update_val)\n",
    "          new_fitness = calculate_influence_spread(G,current)\n",
    "          if new_fitness > fitness:\n",
    "              swarm[i] = current\n",
    "        seed_sets = swarm + prev_swarm\n",
    "        info = seed_set_evaluation(G, seed_sets,groups,b)\n",
    "        # print(info)\n",
    "        pareto_rank(info)\n",
    "        crowding_distance(info)\n",
    "    # Selection\n",
    "    # print(f\"fjsdbjsfkbjg :  {info}\")\n",
    "    population = select_seed_sets(info, 1)\n",
    "    return population,ert\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "def m_optimize(G,k,npop, max_iter, num_nodes,groups,b):\n",
    "    swarm = initialize_particle(G,npop,k)\n",
    "    prev_swarm = copy.deepcopy(swarm)\n",
    "    memory = initialize_particle(G,npop,k)\n",
    "    info = []\n",
    "    # print(max_iter)\n",
    "    ert = set()\n",
    "    flmx = G.number_of_nodes()\n",
    "    flmn = 0\n",
    "    for iter in range(max_iter):\n",
    "        DAP = 1 - iter/max_iter\n",
    "        Dfl = flmx - (flmx - flmn)* DAP\n",
    "        for i in range(len(swarm)):\n",
    "            present = set()\n",
    "            not_present = set(G.nodes() - swarm[i])\n",
    "            for j in range(len(swarm)):\n",
    "                if i==j:\n",
    "                    continue\n",
    "                rand_num = random.random()\n",
    "                for c in range(k):\n",
    "                    if DAP <= rand_num:\n",
    "                        update_val = round(swarm[i][c] + random.random() * Dfl * (memory[j][c] - swarm[i][c]))\n",
    "                        if update_val not in not_present and len(not_present)!=0:\n",
    "                            swarm[i][c] = random.choice(list(not_present))\n",
    "                        else:\n",
    "                            swarm[i][c] = update_val\n",
    "                    else:\n",
    "                        if(len(not_present)!=0):\n",
    "                            swarm[i][c] = random.choice(list(not_present))\n",
    "                    present.add(swarm[i][c])\n",
    "                    not_present.remove(swarm[i][c])\n",
    "\n",
    "        fitness = calculate_influence_spread(G,swarm[i])\n",
    "        m_fitness = calculate_influence_spread(G,memory[i])\n",
    "        if fitness > m_fitness:\n",
    "            memory[i] = swarm[i]\n",
    "        seed_sets = swarm + prev_swarm\n",
    "        info = seed_set_evaluation(G, seed_sets,groups,b)\n",
    "        # print(info)\n",
    "        pareto_rank(info)\n",
    "        crowding_distance(info)\n",
    "    # Selection\n",
    "    # print(f\"fjsdbjsfkbjg :  {info}\")\n",
    "    population = select_seed_sets(info, 1)\n",
    "    return population,ert\n",
    "\n",
    "def runJAYA(G,k,Communities):\n",
    "    G=preprocess(G)\n",
    "    npop = 5\n",
    "    max_iter = 5\n",
    "    num_nodes = G.number_of_nodes()\n",
    "#     k = 5\n",
    "    groups = divide_nodes_into_groups(G, 4)\n",
    "    # for e in G.edges():\n",
    "    #   G.edges[e]['weight'] = round(random.uniform(0.01,1.0),2)\n",
    "    # for n in G.nodes():\n",
    "    #   G.nodes[n]['t'] = G.degree(n)/2\n",
    "    # starttime = time.time()\n",
    "    b=Communities\n",
    "    # print(Communities)\n",
    "    info,ans=jaya_optimize(G,k, npop, max_iter, num_nodes,groups,b)\n",
    "#     print(info[0]['seed'])\n",
    "    return info[0]['seed']\n",
    "# k=10\n",
    "# file_path = 'LFR_1000_0.2_comm_list.txt'  # Replace with the path to your file\n",
    "# CommunitySet = GetCommunities(file_path)\n",
    "# runJAYA(G,k,CommunitySet)\n",
    "\n",
    "def runMOCSA(G,k,Communities):\n",
    "    npop = 10\n",
    "    max_iter = 5\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    groups = divide_nodes_into_groups(G, 4)\n",
    "    b=Communities\n",
    "    info,ans=m_optimize(G,k, npop, max_iter, num_nodes,groups,b)\n",
    "#     print(info[0]['seed'])\n",
    "    return info[0]['seed']\n",
    "\n",
    "def runFIMMOGA(G,k,Communities):\n",
    "    network = G\n",
    "    generations = 5\n",
    "    num_individuals = 5\n",
    "    M_p = 0.5\n",
    "    C_p = 0.5\n",
    "    Q = k\n",
    "    groups = divide_nodes_into_groups(G,4)\n",
    "    # p = np.zeros((G.number_of_nodes(),G.number_of_nodes()))\n",
    "    # for e in G.edges():\n",
    "    #   n = e[1]\n",
    "    #   if G.degree[n] > 0:\n",
    "    #     p[e[0],e[1]] =  round(random.uniform(0.01,1.0),2)\n",
    "    #   else:\n",
    "    #     p[e[0],e[1]] =  round(random.uniform(0.01,1.0),2)\n",
    "\n",
    "    # psum = p.sum(axis=0)\n",
    "\n",
    "    # for e in G.edges():\n",
    "    # G.edges[e]['p'] = round(random.uniform(0.01,1.0),2)\n",
    "\n",
    "    # for n in G.nodes():\n",
    "    # G.nodes[n]['t'] = G.degree(n)/2\n",
    "    # st = time.time()\n",
    "    b = Communities\n",
    "    info=influence_maximization_with_fairness(generations, network, num_individuals,  M_p, C_p, Q,groups,b)\n",
    "    # ft = time.time()-st\n",
    "    # print(info)\n",
    "    # print(len(ans))\n",
    "    # print(ft)\n",
    "    # print(len(b))\n",
    "#     print(info[0]['seed'])\n",
    "    return info[0]['seed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Implementation of PMIA algorithm [1].\n",
    "[1] -- Scalable Influence Maximization for Prevalent Viral Marketing in Large-Scale Social Networks.\n",
    "'''\n",
    "\n",
    "\n",
    "def updateAP(ap, S, PMIIAv, PMIIA_MIPv, Ep):\n",
    "    ''' Assumption: PMIIAv is a directed tree, which is a subgraph of general G.\n",
    "    PMIIA_MIPv -- dictionary of MIP from nodes in PMIIA\n",
    "    PMIIAv is rooted at v.\n",
    "    '''\n",
    "    # going from leaves to root\n",
    "    sorted_MIPs = sorted(PMIIA_MIPv.items(), key = lambda MIP: len(MIP), reverse = True)\n",
    "#     print(\"Edges:\",PMIIAv.nodes)\n",
    "#     for e in PMIIAv.edges:\n",
    "#         print(e)\n",
    "#     for u,_ in sorted_MIPs:\n",
    "#         print(u,_)\n",
    "        \n",
    "        \n",
    "    for u, _ in sorted_MIPs:\n",
    "        if u in S:\n",
    "            ap[(u, PMIIAv)] = 1\n",
    "        elif not PMIIAv.in_edges([u]):\n",
    "#             print(\"\\n\\nGoin in elif\")\n",
    "            ap[(u, PMIIAv)] = 0\n",
    "        else:\n",
    "            in_edges = PMIIAv.in_edges([u], data=True)\n",
    "            prod = 1\n",
    "            for w, _, edata in in_edges:\n",
    "                # p = (1 - (1 - Ep[(w, u)])**edata[\"weight\"])\n",
    "                \n",
    "                print(\"In updateAP\")\n",
    "                print(\"hello:\",w,PMIIAv)\n",
    "                if (w,PMIIAv) in ap.keys():\n",
    "                    print(\"Key present\")\n",
    "                else:\n",
    "                    print(\"Key is NOT present\")\n",
    "#                 print(\"keys:\",\n",
    "    \n",
    "                p = Ep[(w,u)]\n",
    "                prod *= 1 - ap[(w, PMIIAv)]*p\n",
    "#             print(ap,u, PMIIAv)\n",
    "            ap[(u, PMIIAv)] = 1 - prod\n",
    "\n",
    "def updateAlpha(alpha, v, S, PMIIAv, PMIIA_MIPv, Ep, ap):\n",
    "    # going from root to leaves\n",
    "    sorted_MIPs =  sorted(PMIIA_MIPv.items(), key = lambda MIP: len(MIP))\n",
    "    for u, mip in sorted_MIPs:\n",
    "        if u == v:\n",
    "            alpha[(PMIIAv, u)] = 1\n",
    "        else:\n",
    "            out_edges = PMIIAv.out_edges([u])\n",
    "            assert len(out_edges) == 1, \"node u=%s must have exactly one neighbor, got %s instead\" %(u, len(out_edges))\n",
    "            out_edges=list(out_edges)\n",
    "#             print(\"out_edges:\",out_edges,type(out_edges))\n",
    "            \n",
    "            w = out_edges[0][1]\n",
    "            if w in S:\n",
    "                alpha[(PMIIAv, u)] = 0\n",
    "            else:\n",
    "                in_edges = PMIIAv.in_edges([w], data=True)\n",
    "                prod = 1\n",
    "                for up, _, edata in in_edges:\n",
    "                    if up != u:\n",
    "                        # pp_upw = 1 - (1 - Ep[(up, w)])**edata[\"weight\"]\n",
    "                        pp_upw = Ep[(up, w)]\n",
    "                        prod *= (1 - ap[up]*pp_upw)\n",
    "                # alpha[(PMIIAv, u)] = alpha[(PMIIAv, w)]*(1 - (1 - Ep[(u,w)])**PMIIAv[u][w][\"weight\"])*prod\n",
    "                alpha[(PMIIAv, u)] = alpha[(PMIIAv, w)]*(Ep[(u,w)])*prod\n",
    "\n",
    "def computePMIOA(G, u, theta, S, Ep):\n",
    "    '''\n",
    "     Compute PMIOA -- subgraph of G that's rooted at u.\n",
    "     Uses Dijkstra's algorithm until length of path doesn't exceed -log(theta)\n",
    "     or no more nodes can be reached.\n",
    "    '''\n",
    "    # initialize PMIOA\n",
    "    PMIOA = nx.DiGraph()\n",
    "    PMIOA.add_node(u)\n",
    "    PMIOA_MIP = {u: [u]} # MIP(u,v) for v in PMIOA\n",
    "\n",
    "    crossing_edges = set([out_edge for out_edge in G.out_edges([u]) if out_edge[1] not in S + [u]])\n",
    "    edge_weights = dict()\n",
    "    dist = {u: 0} # shortest paths from the root u\n",
    "\n",
    "    # grow PMIOA\n",
    "    while crossing_edges:\n",
    "        # Dijkstra's greedy criteria\n",
    "        min_dist = float(\"Inf\")\n",
    "        sorted_crossing_edges = sorted(crossing_edges) # to break ties consistently\n",
    "        for edge in sorted_crossing_edges:\n",
    "            if edge not in edge_weights:\n",
    "                # edge_weights[edge] = -math.log(1 - (1 - Ep[edge])**G[edge[0]][edge[1]][\"weight\"])\n",
    "                edge_weights[edge] = -math.log(Ep[edge])\n",
    "            edge_weight = edge_weights[edge]\n",
    "            if dist[edge[0]] + edge_weight < min_dist:\n",
    "                min_dist = dist[edge[0]] + edge_weight\n",
    "                min_edge = edge\n",
    "        # check stopping criteria\n",
    "        if min_dist < -math.log(theta):\n",
    "            dist[min_edge[1]] = min_dist\n",
    "            # PMIOA.add_edge(min_edge[0], min_edge[1], {\"weight\": G[min_edge[0]][min_edge[1]][\"weight\"]})\n",
    "            PMIOA.add_edge(min_edge[0], min_edge[1])\n",
    "            PMIOA_MIP[min_edge[1]] = PMIOA_MIP[min_edge[0]] + [min_edge[1]]\n",
    "            # update crossing edges\n",
    "            crossing_edges.difference_update(G.in_edges(min_edge[1]))\n",
    "            crossing_edges.update([out_edge for out_edge in G.out_edges(min_edge[1])\n",
    "                                   if (out_edge[1] not in PMIOA) and (out_edge[1] not in S)])\n",
    "        else:\n",
    "            break\n",
    "    return PMIOA, PMIOA_MIP\n",
    "\n",
    "def updateIS(IS, S, u, PMIOA, PMIIA):\n",
    "    for v in PMIOA[u]:\n",
    "        for si in S:\n",
    "            # if seed node is effective and it's blocked by u\n",
    "            # then it becomes ineffective\n",
    "            if (si in PMIIA[v]) and (si not in IS[v]) and (u in PMIIA[v][si]):\n",
    "                    IS[v].append(si)\n",
    "\n",
    "def computePMIIA(G, ISv, v, theta, S, Ep):\n",
    "\n",
    "    # initialize PMIIA\n",
    "    PMIIA = nx.DiGraph()\n",
    "    PMIIA.add_node(v)\n",
    "    PMIIA_MIP = {v: [v]} # MIP(u,v) for u in PMIIA\n",
    "\n",
    "    crossing_edges = set([in_edge for in_edge in G.in_edges([v]) if in_edge[0] not in ISv + [v]])\n",
    "    edge_weights = dict()\n",
    "    dist = {v: 0} # shortest paths from the root u\n",
    "\n",
    "    # grow PMIIA\n",
    "    while crossing_edges:\n",
    "        # Dijkstra's greedy criteria\n",
    "        min_dist = float(\"Inf\")\n",
    "        sorted_crossing_edges = sorted(crossing_edges) # to break ties consistently\n",
    "        for edge in sorted_crossing_edges:\n",
    "            if edge not in edge_weights:\n",
    "                # edge_weights[edge] = -math.log(1 - (1 - Ep[edge])**G[edge[0]][edge[1]][\"weight\"])\n",
    "#                 edgevalue=Ep[edge]\n",
    "#                 print(\"Ep[edge]:\",Ep[edge],edgevalue)\n",
    "                edge_weights[edge] = -(math.log(Ep[edge]))\n",
    "#                 edge_weights[edge] = -(math.log(edgevalue))\n",
    "                \n",
    "            edge_weight = edge_weights[edge]\n",
    "            if dist[edge[1]] + edge_weight < min_dist:\n",
    "                min_dist = dist[edge[1]] + edge_weight\n",
    "                min_edge = edge\n",
    "        # check stopping criteria\n",
    "        # print min_edge, ':', min_dist, '-->', -math.log(theta)\n",
    "        if min_dist < -math.log(theta):\n",
    "            dist[min_edge[0]] = min_dist\n",
    "            # PMIIA.add_edge(min_edge[0], min_edge[1], {\"weight\": G[min_edge[0]][min_edge[1]][\"weight\"]})\n",
    "            PMIIA.add_edge(min_edge[0], min_edge[1])\n",
    "            PMIIA_MIP[min_edge[0]] = PMIIA_MIP[min_edge[1]] + [min_edge[0]]\n",
    "            # update crossing edges\n",
    "            crossing_edges.difference_update(G.out_edges(min_edge[0]))\n",
    "            if min_edge[0] not in S:\n",
    "                crossing_edges.update([in_edge for in_edge in G.in_edges(min_edge[0])\n",
    "                                       if (in_edge[0] not in PMIIA) and (in_edge[0] not in ISv)])\n",
    "        else:\n",
    "            break\n",
    "    return PMIIA, PMIIA_MIP\n",
    "\n",
    "def PMIA(G, k, theta, Ep):\n",
    "    start = time.time()\n",
    "    # initialization\n",
    "    S = []\n",
    "    IncInf = dict(zip(G.nodes(), [0]*len(G)))\n",
    "    PMIIA = dict() # node to tree\n",
    "    PMIOA = dict()\n",
    "    PMIIA_MIP = dict() # node to MIPs (dict)\n",
    "    PMIOA_MIP = dict()\n",
    "    ap = dict()\n",
    "    alpha = dict()\n",
    "    IS = dict()\n",
    "    for v in G:\n",
    "        IS[v] = []\n",
    "        PMIIA[v], PMIIA_MIP[v] = computePMIIA(G, IS[v], v, theta, S, Ep)\n",
    "        for u in PMIIA[v]:\n",
    "            ap[u] = 0 # ap of u node in PMIIA[v]\n",
    "        updateAlpha(alpha, v, S, PMIIA[v], PMIIA_MIP[v], Ep, ap)\n",
    "        for u in PMIIA[v]:\n",
    "            IncInf[u] += alpha[(PMIIA[v], u)]*(1 - ap[u])\n",
    "#     print('Finished initialization')\n",
    "#     print(time.time() - start)\n",
    "\n",
    "    # main loop\n",
    "    for i in range(k):\n",
    "#         print(IncInf)\n",
    "#         u, _ = max(IncInf.items(), key = lambda dk, dv: dv)\n",
    "        _,u=max(zip(IncInf.values(), IncInf.keys()))\n",
    "        IncInf.pop(u) # exclude node u for next iterations\n",
    "        PMIOA[u], PMIOA_MIP[u] = computePMIOA(G, u, theta, S, Ep)\n",
    "        for v in PMIOA[u]:\n",
    "            for w in PMIIA[v]:\n",
    "                if w not in S + [u]:\n",
    "                    IncInf[w] -= alpha[(PMIIA[v],w)]*(1 - ap[w])\n",
    "\n",
    "        updateIS(IS, S, u, PMIOA_MIP, PMIIA_MIP)\n",
    "\n",
    "        S.append(u)\n",
    "\n",
    "        for v in PMIOA[u]:\n",
    "            if v != u:\n",
    "                PMIIA[v], PMIIA_MIP[v] = computePMIIA(G, IS[v], v, theta, S, Ep)\n",
    "                \n",
    "                \n",
    "                for uu in PMIIA[v].nodes:\n",
    "                    if uu in S:\n",
    "#                         print(\"IN s\")\n",
    "                        ap[u] = 1\n",
    "                    elif not PMIIA[v].in_edges([uu]):\n",
    "#                         print(\"Second else if\")\n",
    "            #             print(\"\\n\\nGoin in elif\")\n",
    "                        ap[uu] = 0\n",
    "                    else:\n",
    "                        in_edges = PMIIA[v].in_edges([uu], data=True)\n",
    "                        prod = 1\n",
    "#                         print(\"inEdgessss:\",in_edges)\n",
    "                        for w, _, edata in in_edges:\n",
    "                            # p = (1 - (1 - Ep[(w, u)])**edata[\"weight\"]\n",
    "                            p = Ep[(w,uu)]\n",
    "                            prod *= 1 - ap[w]*p\n",
    "                        ap[uu] = 1 - prod\n",
    "                \n",
    "#                 updateAP(ap, S, PMIIA[v], PMIIA_MIP[v], Ep)\n",
    "                updateAlpha(alpha, v, S, PMIIA[v], PMIIA_MIP[v], Ep, ap)\n",
    "                # add new incremental influence\n",
    "                for w in PMIIA[v]:\n",
    "                    if w not in S:\n",
    "                        IncInf[w] += alpha[(PMIIA[v], w)]*(1 - ap[w])\n",
    "\n",
    "    return S\n",
    "\n",
    "def getCoverage(G, S, Ep):\n",
    "    return IC(G, S)\n",
    "\n",
    "\n",
    "def run_PMIA(GG,k):\n",
    "    Ep = dict()\n",
    "    G=nx.DiGraph()\n",
    "    for edgee in GG.edges():\n",
    "        s=edgee[0]\n",
    "        t=edgee[1]\n",
    "        Ep[(int(s), int(t))] = GG[s][t]['weight']\n",
    "        G.add_edge(s,t,weight=GG[s][t]['weight'])   \n",
    "        G.nodes[s]['thres']=(G.degree(s)/2)\n",
    "        G.nodes[t]['thres']=(G.degree(t)/2)    \n",
    "    theta = 1.0/20\n",
    "    S = PMIA(G, k, theta, Ep)\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Greedy(G,k):\n",
    "    print(\"in Greedy\")\n",
    "    Dict={}\n",
    "    mySet1=[]\n",
    "    V=G.nodes()\n",
    "    mySet1.clear()\n",
    "    for i in range(k):\n",
    "        for v in (V-mySet1):\n",
    "            mySet1.append(v)\n",
    "            a=linear_Threshold(G,mySet1)\n",
    "            Dict[v]=len(a)#influence as value and current node as key\n",
    "            mySet1.remove(v)#remove crrent node from mySet for rest nodes to go for IC\n",
    "        Keymax = max(zip(Dict.values(), Dict.keys()))[1]# finding node with max influence\n",
    "        Dict.clear()\n",
    "        mySet1.append(Keymax)\n",
    "#     print(\"Final seed set is:\",mySet1,compute_Phi(G,mySet1,comm,k))\n",
    "    return list(mySet1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "j86wDWWIP_YB"
   },
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "  file1 = open(path,'r')\n",
    "  sender = list()\n",
    "  receiver = list()\n",
    "\n",
    "  for i in file1.readlines():\n",
    "    sender.append(int(i.split(' ')[0]))\n",
    "    receiver.append(int(i.split(' ')[1].split('\\n')[0]))\n",
    "    \n",
    "  df = pd.DataFrame(list(zip(sender,receiver)),columns =['source', 'target'])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmltotxt(filename):\n",
    "    import networkx as nx\n",
    "    import pandas as pd\n",
    "    g = nx.read_gml('airlines.gml')\n",
    "    nx.write_edgelist(g, 'edgelistFile.csv', delimiter=',')\n",
    "    df = pd.read_csv('edgelistFile.csv')\n",
    "    file = open(\"myfile.txt\",\"w\")\n",
    "    for i in range(len(df)):\n",
    "        x=df.iloc[i][0]\n",
    "        y=df.iloc[i][1]\n",
    "        file.write(str(x)+\" \"+str(y)+\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcand(G,k, comm):#Take df and  all new nodes as input and return a seed node.\n",
    "    s=[]\n",
    "    s=GreedyDiv(G,k, comm)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fhJ1sm7yt_nD"
   },
   "outputs": [],
   "source": [
    "def GreedyDiv(G,k, comm):\n",
    "    Dict={}\n",
    "    mySet1=[]\n",
    "    V=G.nodes()\n",
    "    mySet1.clear()\n",
    "#     print(\"Community:\",comm,\"k:\",k)\n",
    "    for i in range(k):\n",
    "#         print(\"myset:\",mySet1)\n",
    "        for v in (V-mySet1):\n",
    "#             print(\"v,i:\",v,i)\n",
    "            mySet1.append(v)\n",
    "#             print(\"myset:\",mySet1)\n",
    "            a=compute_Phi(G,mySet1,comm,i+1)\n",
    "#             print(\"Phi of \",mySet1,\" is :\",a)\n",
    "            Dict[v]=a#influence as value and current node as key\n",
    "            mySet1.remove(v)#remove crrent node from mySet for rest nodes to go for IC\n",
    "               #print(Dict)\n",
    "        Keymax = max(zip(Dict.values(), Dict.keys()))[1]# finding node with max influence\n",
    "        Dict.clear()\n",
    "        mySet1.append(Keymax)\n",
    "#     print(\"Final seed set is:\",mySet1,compute_Phi(G,mySet1,comm,k))\n",
    "    return list(mySet1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZn0MKQet_nE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ER71qO5zt_nE"
   },
   "outputs": [],
   "source": [
    "def linear_Threshold(graph, seeds):\n",
    "    influnces = seeds[:]\n",
    "    queue = influnces[:]\n",
    "    pre_node_record = defaultdict(float) \n",
    "#     print(\"Queue:\",queue)\n",
    "#     print(\"Influences:\",influnces)\n",
    "    while len(queue) != 0:\n",
    "        node = queue.pop(0)\n",
    "#         print(\"----------------------------------------------------------------------\")\n",
    "#         print(\"Take node:\",node)\n",
    "#         print(\"Neighbour:\",graph[node])\n",
    "        for element in graph[node]:\n",
    "            if element not in influnces:\n",
    "#                 print(\"Element:\",element,\"prerecored\",pre_node_record[element])\n",
    "                pre_node_record[element] = pre_node_record[element] + graph[node][element]['weight'] \n",
    "#                 print(pre_node_record[element])\n",
    "                if pre_node_record[element] >= graph.nodes[element]['thres']:\n",
    "#                     print(\">>>>>>>>>>>>>>>>>>node influeced:\",element)\n",
    "                    influnces.append(element)\n",
    "                    queue.append(element)\n",
    "#     influnce_num = len(influnces)\n",
    "#     print(\"Seed set:\",seeds,\"Activated nodes:\",influnces)\n",
    "    return influnces\n",
    "# linear_Threshold(GG,[45,29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def communityDiversityFunction(G, S, communities):\n",
    "#     activated = linear_Threshold(G, S)\n",
    "#     noofcommunity=0\n",
    "#     outersum=0\n",
    "#     for com in communities:\n",
    "#         innersum=0\n",
    "#         for rvj in com:\n",
    "#             if rvj in activated:\n",
    "#                 innersum=innersum+1\n",
    "#         outersum=outersum+math.sqrt(innersum)\n",
    "#     return outersum\n",
    "\n",
    "def communityDiversityFunction(G, S, communities):\n",
    "    activated = linear_Threshold(G, S)\n",
    "    noofcommunity=0\n",
    "    for com in communities:\n",
    "        if any(x in activated for x in com):\n",
    "            noofcommunity=noofcommunity+1\n",
    "    return noofcommunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Phi(G, S, communities, k):\n",
    "    if len(S)<1:\n",
    "        return 0\n",
    "    lambda_G = 0.5\n",
    "    \n",
    "    v_length = G.number_of_nodes()\n",
    "    diversity_V = communityDiversityFunction(G, list(G.nodes), communities)\n",
    "\n",
    "    IC_S = linear_Threshold(G, S)\n",
    "    activated_set_S_length = len(IC_S) \n",
    "    diversity_activated_set_S = communityDiversityFunction(G, IC_S, communities)\n",
    "    phi_S = ((1 - lambda_G)* (activated_set_S_length/v_length)) + (lambda_G * (diversity_activated_set_S/diversity_V))\n",
    "    return phi_S\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# obj=TBCD_mine()\n",
    "# perc=[20,40,60,80]\n",
    "# filename='200_0'\n",
    "# print(\"hello\")\n",
    "# obj.execute_TBCD_txt(filename,perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BocmOfr-H5po"
   },
   "outputs": [],
   "source": [
    "def findk(i):#dynamic calculation of k according the percentage of current dataset\n",
    "  k=(0.003*i)\n",
    "  if(i==0):\n",
    "    k=1\n",
    "  if(k>int(k)):\n",
    "   k=int(k)+1\n",
    "  return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmG7U7uqu8kV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayresult(result,itr,perc):\n",
    "    print(\"\\n\\n\\n\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "    linestylee=['dashdot','dashed','dotted','-', '--', ':','-.','dashdot','dashed','dotted','-', '--', ':','-.' ]\n",
    "    \n",
    "    markerss = ['o','v','s','*','+','x','D','d','X','P','x','D','d','X','P']\n",
    "#     descriptions = ['circle', 'triangle_down','square','star', 'plus','x','diamond', 'thin_diamond','x (filled)','plus (filled)']\n",
    "    communitiesITR=[]\n",
    "    Name=[]\n",
    "    activatednodesITR=[]\n",
    "    totalcomm=[]\n",
    "#     print(itr,result)\n",
    "    \n",
    "    for i in result:\n",
    "        for j in range(len(result[i])):\n",
    "            Name.append(result[i][j]['Name'])\n",
    "        break\n",
    "#     print(Name)\n",
    "    \n",
    "#     for i in result:\n",
    "# #         print(result[i])\n",
    "#         for j in range(len(result[i])):\n",
    "#             totalcomm.append(result[i][j]['Total communitites'])\n",
    "#             break\n",
    "#     print(\"Total communitites:\",totalcomm)\n",
    "    \n",
    "    for i in result:\n",
    "        active=[]\n",
    "        comm=[]\n",
    "        for j in range(len(result[i])):\n",
    "            active.append(result[i][j]['length of activated nodes'])\n",
    "            comm.append(result[i][j]['number of communities'])\n",
    "        activatednodesITR.append(active)\n",
    "        communitiesITR.append(comm)\n",
    "#     print(\"Activated nodes:\",activatednodesITR)\n",
    "#     print(\"Communities:\",communitiesITR)\n",
    "        \n",
    "    \n",
    "        \n",
    "#     print(\"hello\");\n",
    "    \n",
    "    plt.xlabel(\"Percentage\")\n",
    "    plt.ylabel(\"Activated nodes\")\n",
    "    plt.title(\"Activated nodes - LFR_1000_0.0 - Greedy - lemda=0.5 - Degree\")\n",
    "    for i in range(len(activatednodesITR[0])):\n",
    "        plt.plot(perc,[pt[i] for pt in activatednodesITR],label = '%s'%Name[i],linestyle='%s'%linestylee[i],marker='%s'%markerss[i])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.xlabel(\"Percentage\")\n",
    "    plt.ylabel(\"No of community\")\n",
    "    plt.title(\"Communitites - LFR_1000_0.0 - Greedy - lemda=0.5 - Degree\")\n",
    "    for i in range(len(communitiesITR[0])):\n",
    "        plt.plot(perc,[pt[i] for pt in communitiesITR],label = '%s'%Name[i],linestyle='%s'%linestylee[i],marker='%s'%markerss[i])\n",
    "#     plt.plot(perc,totalcomm,label='Total community',linestyle='%s'%linestylee[-1],marker='%s'%markerss[-1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findcommunity(G,seedset,k, comm,algoname):\n",
    "    print (\"Communities formed: \",len(comm))\n",
    "    activated = linear_Threshold(G, seedset)\n",
    "    noofcommunity=0\n",
    "    for com in comm:\n",
    "        if any(x in activated for x in com):\n",
    "            noofcommunity=noofcommunity+1\n",
    "    print(\"Community we got:\",noofcommunity)\n",
    "    upperBound_dict = {\n",
    "        'Name':algoname,\n",
    "        'k_nodes': k,\n",
    "#         'Total communitites':len(comm),\n",
    "        'number of communities':noofcommunity,\n",
    "        'length of activated nodes': len(activated),\n",
    "        \n",
    "        # 'length of communities': len(community_df['Unnamed: 1'].unique())\n",
    "    }\n",
    "    print(upperBound_dict)\n",
    "    return upperBound_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findresult(G,k,comm_set_final):\n",
    "    #TBCD\n",
    "    print('\\n\\n--------------- DCDIM -------------------')\n",
    "    R_seed = [getcand(G,int(k),comm_set_final)]\n",
    "    print(\"CDIM:\",R_seed)\n",
    "    R_seed = sum(R_seed, [])\n",
    "    CDIMresult=findcommunity(G,R_seed,k, comm_set_final,'DCDIM')\n",
    "    \n",
    "\n",
    "    print('\\n\\n--------------- FIMMOGA -------------------')\n",
    "    st=time.time()\n",
    "    FIMMOGA_seed = runFIMMOGA(G,k,comm_set_final)\n",
    "    FIMMOGAtime=time.time()-st\n",
    "    print(\"FIMMOGA seed:\",FIMMOGA_seed)\n",
    "    FIMMOGAresult=findcommunity(G,FIMMOGA_seed,k, comm_set_final,'FIMMOGA')\n",
    "\n",
    "\n",
    "    print('\\n\\n--------------- SSR -------------------')\n",
    "    st=time.time()\n",
    "    SSR_seed = runSSR(G,int(k))\n",
    "    SSRtime=time.time()-st\n",
    "    SSR_seed = list(SSR_seed)\n",
    "    print(\"SSR seed:\",SSR_seed)\n",
    "    SSRresult=findcommunity(G,SSR_seed,k, comm_set_final,'SSR-PEA')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('\\n\\n--------------- CSO-IM -------------------')\n",
    "    st=time.time()\n",
    "    G_new=G.copy()\n",
    "    CSO_SeedSet = CSOrun(G, k, comm_set_final, [], G_new)\n",
    "    endtime=time.time()-st\n",
    "    print(\"CSOseed set:\",CSO_SeedSet)\n",
    "    CSOResult=findcommunity(G,CSO_SeedSet,k, comm_set_final,'CSO')\n",
    "\n",
    "\n",
    "#     Greedy\n",
    "#     st=time.time()\n",
    "#     GreedyseedSet =Greedy(G,int(k))\n",
    "#     Greedytime=time.time()-st\n",
    "#     print('\\n\\n--------------- Greedy -------------------')\n",
    "#     Greedyresult=findcommunity(G,GreedyseedSet,k, comm_set_final,'Greedy')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #Single Degree Discount---Not Defined\n",
    "#     st=time.time()\n",
    "#     SingleDegreeSeedset = single_degree_discount(G,int(k))\n",
    "#     SingleDegreetime=time.time()-st\n",
    "#     print('\\n\\n--------------- Single Degree Discount -------------------')\n",
    "#     SingleDegreeresult=findcommunity(G,SingleDegreeSeedset,k, comm_set_final,'SingleDegreeDiscount')\n",
    "    \n",
    "    \n",
    "#     CELF\n",
    "    st=time.time()\n",
    "    CELFseedSet = celf(G,int(k))\n",
    "    CELFtime=time.time()-st\n",
    "    print('\\n\\n--------------- CELF -------------------')\n",
    "    CELFresult=findcommunity(G,CELFseedSet,k, comm_set_final,'CELF')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     PMIA\n",
    "#     st=time.time()\n",
    "#     PMIAseedSet = run_PMIA(G,int(k))\n",
    "#     PMIAtime=time.time()-st\n",
    "#     print('\\n\\n--------------- PMIA -------------------')\n",
    "#     PMIAresult=findcommunity(G,PMIAseedSet,k, comm_set_final,'PMIA')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #SIMPATH\n",
    "#     st=time.time()\n",
    "#     SIMPATHseedSet = run_SIMPATH(G,int(k))\n",
    "#     SIMPATHtime=time.time()-st\n",
    "#     print('\\n\\n--------------- SIMPATH -------------------')\n",
    "#     SIMPATHresult=findcommunity(G,SIMPATHseedSet,k, comm_set_final,'SIMPATH')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     PAGERANK\n",
    "    st=time.time()\n",
    "    pagerank = sorted(nx.pagerank(G).items(), key=lambda x: x[1], reverse=True)\n",
    "    pagerank_seed = [node for node, value in pagerank[0:k]]\n",
    "    PRtime=time.time()-st    \n",
    "    print('\\n\\n--------------- PAGERANK -------------------')\n",
    "    Pagerankresult=findcommunity(G,pagerank_seed,k, comm_set_final,'PageRank')\n",
    "\n",
    "#     DEGREE\n",
    "    st=time.time()\n",
    "    degree = sorted(G.degree(), key=lambda x: x[1], reverse=True)\n",
    "    degree_seed = [node for node, value in degree[0:k]]\n",
    "    Dtime=time.time()-st\n",
    "    print('\\n\\n--------------- DEGREE -------------------')\n",
    "    Degreeresult=findcommunity(G,degree_seed,k, comm_set_final,'Degree')\n",
    "\n",
    "\n",
    "#     #HUB\n",
    "#     st=time.time()\n",
    "#     hub, authority = nx.hits(G)     \n",
    "#     hub  = sorted(hub.items(), key=lambda x: x[1], reverse=True)\n",
    "#     hub_seed = [node for node, value in hub[0:k]]\n",
    "#     Hubtime=time.time()-st\n",
    "#     print('\\n\\n--------------- HUB -------------------')\n",
    "#     Hubresult=findcommunity(G,hub_seed,k, comm_set_final,'Hub')\n",
    "\n",
    "#     #AUTHORITY\n",
    "#     st=time.time()\n",
    "#     authority  = sorted(authority.items(), key=lambda x: x[1], reverse=True)\n",
    "#     authority_seed = [node for node, value in authority[0:k]]\n",
    "#     Atime=time.time()-st\n",
    "#     print('\\n\\n--------------- AUTHORITY -------------------')\n",
    "#     Authorityresult=findcommunity(G,authority_seed,k, comm_set_final,'Authority')\n",
    "\n",
    "#     #NBKCORE\n",
    "#     st=time.time()\n",
    "#     # neighborhood coreness\n",
    "#     node2nbcore =  {node: np.sum([G.degree(nb) for nb in G.neighbors(node)]) for node in G.nodes() }\n",
    "#     nbkcore = sorted(node2nbcore.items(), key=lambda x: x[1], reverse=True)\n",
    "#     nbkcore_seed = [node for node, value in nbkcore[0:k]]\n",
    "#     N2time=time.time()-st\n",
    "#     print('\\n\\n--------------- neighborhood coreness -------------------')\n",
    "#     Neighbourresult=findcommunity(G,nbkcore_seed,k, comm_set_final,'Neighbourhood')\n",
    "    \n",
    "\n",
    "#     resultt=[DDR,Greedyresult,CELFresult,Pagerankresult,Degreeresult,Hubresult,Authorityresult,Neighbourresult]\n",
    "#     resultt=[DDR,SingleDegreeresult,CELFresult,CELFPPresult,Pagerankresult,Degreeresult,Hubresult,Authorityresult,Neighbourresult]\n",
    "#     resultt=[DDR,Greedyresult,CELFresult,Pagerankresult,Degreeresult,Hubresult,Authorityresult,Neighbourresult]\n",
    "\n",
    "#     resultt=[TBCDresult,Greedyresult,CELFresult,Pagerankresult,PMIAresult,SIMPATHresult,Degreeresult]\n",
    "#     resultt=[TBCDresult,CELFresult,Pagerankresult,PMIAresult,SIMPATHresult,Degreeresult]\n",
    "    resultt=[CDIMresult,FIMMOGAresult,SSRresult,Degreeresult,Pagerankresult,CELFresult,CSOResult]\n",
    "\n",
    "    return resultt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeExcel(result,itr,perc):\n",
    "    timeITR=[]\n",
    "    Name=[]\n",
    "    activatednodesITR=[]\n",
    "    for i in result:\n",
    "        for j in range(len(result[i])):\n",
    "            Name.append(result[i][j]['Name'])\n",
    "        break\n",
    "    for i in result:\n",
    "        active=[]\n",
    "        timee=[]\n",
    "        for j in range(len(result[i])):\n",
    "            active.append(result[i][j]['length of activated nodes'])\n",
    "            timee.append(result[i][j]['number of communities'])\n",
    "        activatednodesITR.append(active)\n",
    "        timeITR.append(timee)\n",
    "    df=pd.DataFrame()\n",
    "    time=\"Community\"\n",
    "    for i in range(len(timeITR)):\n",
    "        timestr=time+\"_\"+str(itr[i])+\"_\"+str(perc[i])+\"%\"\n",
    "        df[timestr]=timeITR[i]\n",
    "#     print(\"After name\",df)\n",
    "    ICnodes=\"Activated_Nodes\"\n",
    "    for i in range(len(activatednodesITR)):\n",
    "        ICnodesstr=ICnodes+\"_\"+str(itr[i])+\"_\"+str(perc[i])+\"%\"\n",
    "        df[ICnodesstr]=activatednodesITR[i]\n",
    "    df.insert(0,\"Name of Algorithm\",Name)\n",
    "    return df\n",
    "\n",
    "# upperBound_dict = {\n",
    "#         'Name':algoname,\n",
    "#         'k_nodes': k,\n",
    "#         'Total communitites':len(comm),\n",
    "#         'number of communities':noofcommunity,\n",
    "#         'length of activated nodes': len(activated),\n",
    "        \n",
    "#         # 'length of communities': len(community_df['Unnamed: 1'].unique())\n",
    "#     }\n",
    "# result_df=pd.DataFrame()\n",
    "# for i in range(1):\n",
    "#     obj=TBCD_mine()\n",
    "#     perc=[10,20,30,40,50,60,70,80,90,100]\n",
    "#     filename='sample'\n",
    "#     dff=obj.execute_TBCD_txt(filename,perc)\n",
    "#     dff2=result_df\n",
    "#     result_df=dff\n",
    "#     result_df = dff2.append(dff,ignore_index = True)\n",
    "#     result_df.to_excel('Outputt.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "FUYNIegxE4lr"
   },
   "outputs": [],
   "source": [
    "class TBCD_mine(object):\n",
    "    \"\"\"\n",
    "        tree based community detection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename=None, g=nx.Graph(), ttl=float('inf'), obs=7, path=\"\",\n",
    "                 start=None, end=None, level_max = 6, window_size = 20, theta = 0.5,\n",
    "                 dataset = None, edge_list = None, comm_list = None,window_ratio = 0.10):\n",
    "        \"\"\"\n",
    "            Constructor\n",
    "            :param g: networkx graph\n",
    "            :param ttl: edge time to live (days)\n",
    "            :param obs: observation window (days)\n",
    "            :param path: Path where generate the results and find the edge file\n",
    "            :param start: starting date\n",
    "            :param end: ending date\n",
    "        \"\"\"\n",
    "        print(\"initialization\")\n",
    "        self.path = path\n",
    "        self.graph = g\n",
    "        self.removed = 0\n",
    "        self.added = 0\n",
    "        self.filename = filename\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.obs = obs\n",
    "        self.communities = {}\n",
    "        self.intra_conn = {}\n",
    "        self.inter_conn = {}\n",
    "        self.select = {}\n",
    "        self.cluster_head = set()\n",
    "        self.cluster_h = {}\n",
    "        self.level = {}\n",
    "        self.occurence = {}\n",
    "        self.level_max = level_max\n",
    "        self.w_temp = 0\n",
    "        self.window_size = window_size\n",
    "        self.theta = theta\n",
    "        self.dataset = \"\"\n",
    "        self.edge_list = \"\"\n",
    "        self.comm_list = \"\"\n",
    "        self.ground_truth_comm = {}\n",
    "        self.precision = -1\n",
    "        self.recall = -1\n",
    "        self.nmi = -1\n",
    "        self.f_measure = -1\n",
    "        self.purity = -1\n",
    "        self.ari = -1\n",
    "        self.entropy = -1\n",
    "        self.modularity = -1\n",
    "        self.coverage = -1\n",
    "        self.external_density = -1\n",
    "        self.average_isolability = -1\n",
    "        self.mat_file_adj = \"\"\n",
    "        self.mat_file_label = \"\"\n",
    "        self.row = 1\n",
    "        self.result_array = []\n",
    "        self.window_ratio = window_ratio\n",
    "\n",
    "\n",
    "    def detachability(self,label):\n",
    "\n",
    "#         print(\"calculating detachability for label = \"+str(label))\n",
    "        G = self.graph\n",
    "        internal = 0\n",
    "        external = 0\n",
    "        DZ = 0\n",
    "        '''cluster_h_set = set()\n",
    "        cluster_head_set = set()\n",
    "        for node in G :\n",
    "            cluster_h_set.add(self.cluster_h[node])\n",
    "            cluster_head_set.add(self.cluster_head[node])\n",
    "        count_h = len(cluster_h_set)\n",
    "        count_head = len(cluster_head_set)\n",
    "        print(\"\\n\\n\\n count h = \"+str(count_h))\n",
    "        print(\"\\n\\n\\n count head = \" + str(count_head))'''\n",
    "        # node and node neighbour only taken into account\n",
    "        for node in G:\n",
    "            if self.cluster_h[node] == label:\n",
    "                for node_neighbour in G.neighbors(node):\n",
    "                    if self.cluster_h[node_neighbour] == label:\n",
    "                        internal = internal + G[node][node_neighbour]['weight']\n",
    "                    else:\n",
    "                        external = external + G[node][node_neighbour]['weight']\n",
    "                '''internal += self.intra_conn[node]\n",
    "                external += self.inter_conn[node]'''\n",
    "        if internal + external != 0:\n",
    "            DZ = internal / (internal + external)\n",
    "#         print(\"detachbility = \"+str(DZ))\n",
    "        return DZ\n",
    "\n",
    "    def max_comm_label(self,node):\n",
    "        #removed influence part\n",
    "        G = self.graph\n",
    "        all_labels = set()\n",
    "        # print(\"initially for node \"+str(node)+\" label is \"+str(var_dict[node]))\n",
    "        for node_neighbour in G.neighbors(node):\n",
    "            all_labels.add(self.cluster_h[node_neighbour])\n",
    "        prob_actual = 1\n",
    "        label_actual = self.cluster_h[node]\n",
    "        for label in all_labels:\n",
    "            # print(\"for label \"+str(label))\n",
    "            '''prob_new = 1\n",
    "            for node_chk in G.neighbors(node):\n",
    "                # print(\"u is-\"+str(u)+\" v is-\"+str(v))\n",
    "                if self.cluster_h[node_chk] == label:\n",
    "                    # print(\"prob_new = \"+str(prob_new)+\" edge weight \"+str(G[node][node_chk]['weight']))\n",
    "                    chk = 0\n",
    "                    if G.has_edge(node, node_chk):\n",
    "                        chk = G[node][node_chk]['weight']\n",
    "                    if var_dict['influence'][node][node_chk] == 1:\n",
    "                        # print(\"influence and edge weight true for \"+str(node)+\"-\"+str(node_chk))\n",
    "                        prob_new = prob_new * (1 - chk)\n",
    "            if prob_new < prob_actual:\n",
    "                prob_actual = prob_new\n",
    "                label_actual = label\n",
    "                self.cluster_h[node] = label'''\n",
    "        # print(\"after max_comm_label for node \" + str(node) + \" label is \" + str(var_dict[node]))\n",
    "        return label_actual\n",
    "\n",
    "    def isolability_measure_single_label(self,label):\n",
    "\n",
    "        G = self.graph\n",
    "        isolability = 0\n",
    "        internal = 0\n",
    "        external = 0\n",
    "        for node in G:\n",
    "            if self.cluster_h[node] == label:\n",
    "                for node_neighbour in G.neighbors(node):\n",
    "                    if (self.cluster_h[node] == self.cluster_h[node_neighbour]):\n",
    "                        internal = internal + G[node][node_neighbour]['weight']\n",
    "                    else:\n",
    "                        external = external + G[node][node_neighbour]['weight']\n",
    "        if external != 0: isolability = internal / (internal+external)\n",
    "        return isolability\n",
    "\n",
    "    def external_density_eval(self):\n",
    "\n",
    "        G = self.graph\n",
    "        numerator = 0\n",
    "        n = len(G)\n",
    "        denominator = n * (n - 1)\n",
    "        total_labels = set()\n",
    "        for node in G:\n",
    "            total_labels.add(self.cluster_h[node])\n",
    "        for label in total_labels:\n",
    "            nodes_per_label = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label: nodes_per_label += 1\n",
    "            denominator -= (nodes_per_label * (nodes_per_label - 1))\n",
    "        for (node1, node2) in G.edges():\n",
    "            if self.cluster_h[node1] != self.cluster_h[node2] and node1 != node2: numerator += 1\n",
    "        if denominator != 0:\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def coverage_eval(self):\n",
    "\n",
    "        G = self.graph\n",
    "        numerator = 0\n",
    "        denominator = len(G.edges)\n",
    "        for (node1, node2) in G.edges():\n",
    "            if self.cluster_h[node1] == self.cluster_h[node2] and node1 != node2: numerator += 1\n",
    "        if denominator != 0:\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def modularity_eval(self):\n",
    "\n",
    "        G = self.graph\n",
    "        total_edges = len(G.edges)\n",
    "        total_labels = set()\n",
    "        for node in G:\n",
    "            total_labels.add(self.cluster_h[node])\n",
    "        modularity = 0\n",
    "        internal_final = 0\n",
    "        external_final = 0\n",
    "        for label in total_labels:\n",
    "            internal = 0\n",
    "            external = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label:\n",
    "                    for node_neighbour in G.neighbors(node):\n",
    "                        if self.cluster_h[node_neighbour] == label:\n",
    "                            internal += 1\n",
    "                        else:\n",
    "                            external += 1\n",
    "            '''internal_final_check = internal/total_edges\n",
    "            external_final_check = external/total_edges\n",
    "            if internal_final_check >= 1 : print(\"internal problem\")\n",
    "            if external_final_check >= 1: print(\"external problem\")\n",
    "            modularity_current_label = internal_final - external_final*external_final\n",
    "            if modularity_current_label >= 1 : print(\"modularity current label problem\")\n",
    "            modularity += ((internal/total_edges) - ((external*external)/(total_edges*total_edges)))'''\n",
    "            modularity += internal / len(G.edges) - ((external * external) / (len(G.edges) * len(G.edges)))\n",
    "            internal_final += internal\n",
    "            external_final += external\n",
    "        internal_final = internal_final / 2\n",
    "        external_final = external_final / 2\n",
    "        modularity_final = (internal_final / total_edges) - (\n",
    "                    (external_final * external_final) / (total_edges * total_edges))\n",
    "        modularity_final = modularity / 2\n",
    "        if (internal_final + external_final) == total_edges:\n",
    "            print(\"modularity edge check correct\")\n",
    "            print(str(modularity_final))\n",
    "        return modularity_final\n",
    "\n",
    "    def TBCD_dissol(self):\n",
    "\n",
    "        G = self.graph\n",
    "        print(\"Inside dissolution phase\")\n",
    "        labels = set()\n",
    "        for node in self.cluster_head: labels.add(node) \n",
    "#         print(\"labels:\",labels)\n",
    "        \n",
    "#         print(\"no of sets = \" + str(len(labels)))\n",
    "#         print(\"no of nodes = \" + str(len(G)))\n",
    "        for label in labels:\n",
    "            comm_set = []\n",
    "            for node in self.graph:\n",
    "                if self.cluster_h[node] == label:\n",
    "                    comm_set.append(node)\n",
    "                    comm_set.append(self.level[node])\n",
    "#                     print(\"Community:\",comm_set)\n",
    "                    \n",
    "        label_set = set()\n",
    "        for label in self.cluster_head:\n",
    "            label_set.add(label)\n",
    "#         print(\"label_set:\",label_set)\n",
    "        \n",
    "        for label in label_set :\n",
    "#             print(\"for label = \"+str(label))\n",
    "            detachabil = self.detachability(label)\n",
    "#             print(\"detachabil:\",detachabil,\"theta:\",self.theta)\n",
    "            \n",
    "            if detachabil < self.theta :\n",
    "                n_e = set()\n",
    "                for node in G :\n",
    "                    self.occurence[node] = 0\n",
    "                for node in G :\n",
    "                    if self.cluster_h[node] == label :\n",
    "                        for single in G.neighbors(node) :\n",
    "                            if self.cluster_h[single] != label :\n",
    "                                n_e.add(single)\n",
    "                                self.occurence[single] += 1\n",
    "#                 print(\"n_e set has = \"+str(len(n_e)))\n",
    "                max = -1\n",
    "                for node in G :\n",
    "                    if self.occurence[node] > max :\n",
    "                        max = self.occurence[node]\n",
    "                n_max = set()\n",
    "                for node in G :\n",
    "                    if self.occurence[node] == max :\n",
    "                        n_max.add(node)\n",
    "#                 print(\"n_max set has = \" + str(len(n_max)))\n",
    "                \n",
    "                c_s = set()\n",
    "                for node in n_max :\n",
    "                    c_s.add(self.cluster_h[node])\n",
    "#                 print(\"c_s set has = \" + str(len(c_s)))\n",
    "                mid = -999999\n",
    "                big_label = -1\n",
    "                for other_label in c_s :\n",
    "                    label_node_set = set()\n",
    "                    for node in G :\n",
    "                        if self.cluster_h[node] == label :\n",
    "                            label_node_set.add(node)\n",
    "                    for node in label_node_set :\n",
    "                        self.cluster_h[node] = other_label\n",
    "                    term1 = self.detachability(other_label)\n",
    "                    for node in label_node_set :\n",
    "                        self.cluster_h[node] = label\n",
    "                    term2 = self.detachability(other_label)\n",
    "                    tid = term1 - term2\n",
    "#                     print(\"for label = \"+str(other_label))\n",
    "#                     print(\"tid = \"+str(tid))\n",
    "#                     print(\"mid = \" + str(mid))\n",
    "                    if tid > mid :\n",
    "#                         print(\"big label is other label\")\n",
    "                        mid = tid\n",
    "                        big_label = other_label\n",
    "                to_be_removed = -9999\n",
    "                if len(self.cluster_head) != 1 :\n",
    "                    self.cluster_head.remove(label)\n",
    "                    to_be_removed = label\n",
    "#                     print(\"big label = \"+str(big_label))\n",
    "#                     print(\"to be removed label = \" + str(label))\n",
    "                    for node in G :\n",
    "                        if self.cluster_h[node] == to_be_removed :\n",
    "                            self.cluster_h[node] = big_label\n",
    "\n",
    "    def combin(self,n):\n",
    "\n",
    "        n = int(n)\n",
    "        term = n*(n-1)\n",
    "        if term > 2:\n",
    "            return term/2\n",
    "        else : return 0\n",
    "\n",
    "\n",
    "    def all_metric_nog(self):\n",
    "\n",
    "        G = self.graph\n",
    "\n",
    "        self.modularity = self.modularity_eval()\n",
    "        self.coverage = self.coverage_eval()\n",
    "        self.external_density = self.external_density_eval()\n",
    "        print(\"\\n\\n\\n\\nwithout ground truth\\n\\n\\n\\n\")\n",
    "        print(\"modularity = \" + str(self.modularity))\n",
    "        print(\"coverage = \" + str(self.coverage))\n",
    "        print(\"external density = \" + str(self.external_density))\n",
    "        total_labels_mine = set()\n",
    "        for node in G:\n",
    "            total_labels_mine.add(self.cluster_h[node])\n",
    "        total_isolability = 0\n",
    "        for label in total_labels_mine:\n",
    "            total_isolability = total_isolability + self.isolability_measure_single_label(label)\n",
    "        average_isolability = total_isolability / len(total_labels_mine)\n",
    "        self.average_isolability = average_isolability\n",
    "        print(\"average isolability = \" + str(average_isolability))\n",
    "\n",
    "        self.result_array.append([self.row,len(total_labels_mine),self.modularity,self.coverage,self.external_density,\n",
    "                                  self.average_isolability])\n",
    "        self.row += 1\n",
    "\n",
    "    def all_metric(self):\n",
    "\n",
    "        G = self.graph\n",
    "\n",
    "        self.modularity = self.modularity_eval()\n",
    "        self.coverage = self.coverage_eval()\n",
    "        self.external_density = self.external_density_eval()\n",
    "        print(\"\\n\\n\\n\\nwithout ground truth\\n\\n\\n\\n\")\n",
    "        print(\"modularity = \" + str(self.modularity))\n",
    "        print(\"coverage = \" + str(self.coverage))\n",
    "        print(\"external density = \" + str(self.external_density))\n",
    "        total_labels_mine = set()\n",
    "        total_labels_ground = set()\n",
    "        for node in G:\n",
    "            total_labels_mine.add(self.cluster_h[node])\n",
    "            #total_labels_ground.add(self.ground_truth_comm[node])\n",
    "        total_isolability = 0\n",
    "        for label in total_labels_mine:\n",
    "            total_isolability = total_isolability + self.isolability_measure_single_label(label)\n",
    "        average_isolability = total_isolability / len(total_labels_mine)\n",
    "        self.average_isolability = average_isolability\n",
    "        print(\"average isolability = \" + str(average_isolability))\n",
    "\n",
    "        print(\"calculating ground truth metric\")\n",
    "        comm_list = open(self.comm_list)\n",
    "        print(\"before reading community text file\")\n",
    "\n",
    "        no_nodes = 0\n",
    "        ground_truth_comm_set = set()\n",
    "        for l in comm_list:\n",
    "            l = l.split(\" \")\n",
    "            node = int(l[0])\n",
    "            comm_label = int(l[1])\n",
    "            #print(\"for node x : \" + str(node) + \" community is  : \" + str(comm_label))\n",
    "            ground_truth_comm_set.add(comm_label)\n",
    "            self.ground_truth_comm[node] = comm_label\n",
    "            no_nodes += 1\n",
    "\n",
    "        print(\"no of communities in ground truth = \"+str(len(ground_truth_comm_set)))\n",
    "        #print(str(ground_truth_comm_set))\n",
    "        #for node in range(no_nodes): print(str(node)+\" \"+str(self.ground_truth_comm[node]))\n",
    "\n",
    "        algo_comm_set = set()\n",
    "        for node in G :\n",
    "            algo_comm_set.add(self.cluster_h[node])\n",
    "        print(\"no of communities from algo = \"+str(len(algo_comm_set)))\n",
    "\n",
    "\n",
    "        ground_truth_comm_list = list(ground_truth_comm_set)\n",
    "        algo_comm_list = list(algo_comm_set)\n",
    "        common_matrix = np.zeros((len(ground_truth_comm_set),len(algo_comm_set)))\n",
    "        for i in range(len(ground_truth_comm_set)) :\n",
    "            for j in range(len(algo_comm_set)) :\n",
    "                for node in G :\n",
    "                    if self.ground_truth_comm[node] == ground_truth_comm_list[i] and \\\n",
    "                            self.cluster_h[node] == algo_comm_list[j] :\n",
    "                        common_matrix[i][j] += 1\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        true_neg = 0\n",
    "        false_neg = 0\n",
    "        for node1 in G :\n",
    "            for node2 in G :\n",
    "                if self.cluster_h[node1] == self.cluster_h[node2] :\n",
    "                    if self.ground_truth_comm[node1] == self.ground_truth_comm[node2] :\n",
    "                        true_pos += 1\n",
    "                    else :\n",
    "                        false_pos += 1\n",
    "                else :\n",
    "                    if self.ground_truth_comm[node1] != self.ground_truth_comm[node2] :\n",
    "                        true_neg += 1\n",
    "                    else :\n",
    "                        false_neg += 1\n",
    "\n",
    "        self.precision = true_pos/(true_pos+false_pos)\n",
    "        self.recall = true_pos/(true_pos+false_neg)\n",
    "\n",
    "        ground_truth_comm_no = {}\n",
    "        ground_truth_comm_no_index = np.zeros(len(ground_truth_comm_set))\n",
    "        algo_comm_no = {}\n",
    "        algo_comm_no_index = np.zeros(len(algo_comm_set))\n",
    "        index = -1\n",
    "        for label in ground_truth_comm_set :\n",
    "            index += 1\n",
    "            ground_truth_comm_no[label] = 0\n",
    "            ground_truth_comm_no_index[index] = 0\n",
    "            for node in range(no_nodes) :\n",
    "                if self.ground_truth_comm[node] == label :\n",
    "                    #print(\"counting ground truth increase\")\n",
    "                    ground_truth_comm_no[label] += 1\n",
    "                    ground_truth_comm_no_index[index] += 1\n",
    "\n",
    "        index = -1\n",
    "        for label in algo_comm_set:\n",
    "            index += 1\n",
    "            algo_comm_no[label] = 0\n",
    "            algo_comm_no_index[index] = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label:\n",
    "                    algo_comm_no[label] += 1\n",
    "                    algo_comm_no_index[index] += 1\n",
    "\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "\n",
    "        for i in range(len(ground_truth_comm_set)):\n",
    "            if ground_truth_comm_no_index[i] == 0 : print(\"error in ground truth label counting\")\n",
    "\n",
    "\n",
    "        for i in range(len(ground_truth_comm_set)) :\n",
    "            for j in range(len(algo_comm_set)) :\n",
    "                if common_matrix[i][j] != 0 :\n",
    "                    numerator += common_matrix[i][j] * math.log((common_matrix[i][j]*no_nodes)/(algo_comm_no_index[j]*ground_truth_comm_no_index[i]))\n",
    "\n",
    "        numerator *= -2\n",
    "\n",
    "        for i in range(len(ground_truth_comm_set)):\n",
    "            denominator += ground_truth_comm_no_index[i] * math.log(ground_truth_comm_no_index[i]/no_nodes)\n",
    "\n",
    "        for j in range(len(algo_comm_set)):\n",
    "            denominator += algo_comm_no_index[j] * math.log(algo_comm_no_index[j]/no_nodes)\n",
    "\n",
    "        self.nmi = numerator/denominator\n",
    "\n",
    "        self.f_measure = 2 * (self.precision * self.recall)/(self.precision + self.recall)\n",
    "\n",
    "        self.purity = 0\n",
    "\n",
    "        for ground_truth_label in ground_truth_comm_set:\n",
    "            max = -1\n",
    "            for algo_label in algo_comm_set:\n",
    "                count = 0\n",
    "                for node in G :\n",
    "                    if self.cluster_h[node] == algo_label and self.ground_truth_comm[node] == ground_truth_label :\n",
    "                        count += 1\n",
    "                if count > max : max = count\n",
    "\n",
    "            self.purity += max\n",
    "\n",
    "        self.purity = self.purity/no_nodes\n",
    "\n",
    "        self.entropy = 0\n",
    "\n",
    "        for label_mine in algo_comm_set :\n",
    "            n_c = algo_comm_no[label_mine]\n",
    "            n = no_nodes\n",
    "            m = len(ground_truth_comm_set)\n",
    "            term2 = 0\n",
    "            for label_ground in ground_truth_comm_set :\n",
    "                n_i_j = 0\n",
    "                for node in G :\n",
    "                    if self.cluster_h[node] == label_mine and self.ground_truth_comm[node] == label_ground :\n",
    "                        n_i_j += 1\n",
    "                if n_i_j != 0 :\n",
    "                    term2 += (n_i_j/n_c)* math.log(n_i_j/n_c)\n",
    "            self.entropy += (n_c/n)*(1/math.log(m))*term2*(-1)\n",
    "\n",
    "        print(\"entropy = \" + str(self.entropy))\n",
    "\n",
    "        self.ari = 0\n",
    "        term_common = 0\n",
    "        for i in range(len(ground_truth_comm_set)):\n",
    "            for j in range(len(algo_comm_set)):\n",
    "                term_common += self.combin(common_matrix[i][j])\n",
    "\n",
    "        term_ground = 0\n",
    "\n",
    "        print(\"ground truth label set\")\n",
    "        print(str(ground_truth_comm_set))\n",
    "        for label in ground_truth_comm_set:\n",
    "            count = 0\n",
    "            for node in G:\n",
    "                if self.ground_truth_comm[node] == label: count += 1\n",
    "            term_ground += self.combin(count)\n",
    "\n",
    "        print(\"algo label set\")\n",
    "        print(str(algo_comm_set))\n",
    "        term_algo = 0\n",
    "        for label in algo_comm_set:\n",
    "            count = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label: count += 1\n",
    "            term_algo += self.combin(count)\n",
    "\n",
    "\n",
    "        term_sum = (term_ground + term_algo) / 2\n",
    "        term_prod = (term_ground * term_algo) / self.combin(no_nodes)\n",
    "\n",
    "        '''print(\"term_common = \"+str(term_common))\n",
    "        print(\"term_ground = \" + str(term_ground))\n",
    "        print(\"term_algo = \" + str(term_algo))\n",
    "        print(\"term_sum = \" + str(term_sum))\n",
    "        print(\"term_prod = \" + str(term_prod))'''\n",
    "\n",
    "        self.ari = (term_common - term_prod) / (term_sum - term_prod)\n",
    "\n",
    "        self.result_array.append([self.row,len(algo_comm_set),self.modularity,self.coverage,self.external_density,self.average_isolability,self.f_measure,self.nmi,self.purity,self.entropy,self.ari])\n",
    "        self.row += 1\n",
    "        \n",
    "    def calldiverified(self):\n",
    "        print(self.graph)\n",
    "        return 1,3\n",
    "    \n",
    "\n",
    "    def execute_TBCD_txt(self,file_name,perc):\n",
    "        \"\"\"\n",
    "            Execute tree based community detection algorithm from static dataset\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"reading static edge list with file name : \"+str(file_name))\n",
    "#         self.edge_list = './datasets_txt/' + file_name + '_edge_list.txt'\n",
    "#         self.comm_list = './datasets_txt/' + file_name + '_comm_list.txt'\n",
    "        self.edge_list = file_name + '_edge_list.txt'\n",
    "#         self.comm_list =  file_name + '_comm_list.txt'\n",
    "    \n",
    "        edge_list = open(self.edge_list)\n",
    "#         print(\"counting edges to set window size\")\n",
    "        window_counter = 0\n",
    "        count = 0\n",
    "        for l in edge_list:\n",
    "            count += 1\n",
    "#         print(\"Count:\",count,\"Window ratio:\",self.window_ratio)\n",
    "        \n",
    "        self.window_size = int(count*self.window_ratio)\n",
    "        # print(\"Window size:\",self.window_size)\n",
    "        \n",
    "        self.graph = nx.Graph()\n",
    "        # print(self.graph)\n",
    "        \n",
    "#       print(\"before reading edge list text file\")\n",
    "        no_nodes = 0\n",
    "        w = 0\n",
    "        edge_list = open(self.edge_list)\n",
    "        R_seed=[]\n",
    "        seedSet=[]\n",
    "        pagerank=[]\n",
    "        G = nx.DiGraph()\n",
    "#         MultiDiGraph\n",
    "        # print(\"------------------------------------------------------------------------------------------\")\n",
    "        \n",
    "        print(count)\n",
    "        itr=[]\n",
    "        for i in range(len(perc)):\n",
    "            vv=int((perc[i]/100)*count)\n",
    "            itr.append(vv)\n",
    "        print(itr)\n",
    "        totalnodes=0\n",
    "        resultt={}\n",
    "        \n",
    "        for l in edge_list:\n",
    "#             print(l)\n",
    "            totalnodes=totalnodes+1\n",
    "            l = l.split(\" \")\n",
    "            s=int(l[0])\n",
    "            t=int(l[1])\n",
    "            value = round(random.uniform(0.01,1.0),2)\n",
    "            G.add_edge(s,t,weight=value)\n",
    "            G.nodes[s]['thres']=(G.in_degree(s)/2)\n",
    "            G.nodes[t]['thres']=(G.in_degree(t)/2)\n",
    "            k=int(findk(len(G.nodes)))\n",
    "            \n",
    "            self.added += 1\n",
    "            e = {}\n",
    "            x = int(l[0])\n",
    "            y = int(l[1])\n",
    "#             print(\"for edge x : \"+str(x)+\" y : \"+str(y))\n",
    "#             print(\".....................................................\")\n",
    "            if x == y:\n",
    "#                 print(\"self edge found\")\n",
    "                if totalnodes in itr:\n",
    "                    self.TBCD_dissol()\n",
    "                    w = 0\n",
    "                    print(\"_______________________________________________________________________________________________\")\n",
    "                    print(\"Get result at edge:\",totalnodes)\n",
    "                    comm_set_final = detect_comm(self.graph, self.cluster_head, self.cluster_h)\n",
    "                    resultt[totalnodes]=findresult(G,k,comm_set_final)\n",
    "                continue\n",
    "\n",
    "            if not self.graph.has_node(x) :\n",
    "#                 print(\"Graph Does not have node x.\")\n",
    "                self.graph.add_node(x)\n",
    "#                 print(\"intra connection of x:\",self.intra_conn[x])\n",
    "                self.intra_conn[x] = -1\n",
    "                self.inter_conn[x] = -1\n",
    "                self.select[x] = 0\n",
    "                no_nodes = no_nodes + 1\n",
    "#                 print(\"new node added\")\n",
    "                # print(\"no nodes = \"+str(no_nodes))\n",
    "\n",
    "            if not self.graph.has_node(y):\n",
    "#                 print(\"Graph Does not have node y.\")\n",
    "                self.graph.add_node(y)\n",
    "#                 print(\"intra connection of y:\",self.intra_conn[y])\n",
    "                self.intra_conn[y] = -1\n",
    "                self.inter_conn[y] = -1\n",
    "                self.select[y] = 0\n",
    "                no_nodes = no_nodes + 1\n",
    "#                 print(\"new node added\")\n",
    "                # print(\"no nodes = \" + str(no_nodes))\n",
    "\n",
    "            self.graph.add_edge(x, y, weight=1.0)\n",
    "\n",
    "            if self.select[x] == 0 and self.select[y] == 0:\n",
    "#                 print(\"If both are new.\")\n",
    "#                 print(\"select[x]==select[y]\")\n",
    "                # print(type(self.cluster_head))\n",
    "#                 print(\"Cluster head:\",self.cluster_head)\n",
    "                self.cluster_head.add(x)\n",
    "#                 print(\"Cluster head after added:\",self.cluster_head)\n",
    "                self.cluster_h[x] = x\n",
    "                self.level[x] = 0\n",
    "                self.level[y] = 1\n",
    "                self.cluster_h[y] = x\n",
    "                self.intra_conn[x] = 1\n",
    "                self.intra_conn[y] = 1\n",
    "                self.select[x] = 1\n",
    "                self.select[y] = 1\n",
    "\n",
    "            elif self.select[y] == 0 or self.select[x] == 0 :\n",
    "\n",
    "                if self.select[x] == 0 :\n",
    "                    term = y\n",
    "                    y = x\n",
    "                    x = term\n",
    "#                 print(\"Max level:\",self.level_max)\n",
    "                \n",
    "                \n",
    "                if self.level[x] <= self.level_max - 1:\n",
    "                    self.level[y] = self.level[x] + 1\n",
    "                    self.intra_conn[x] += 1\n",
    "                    self.intra_conn[y] = 1\n",
    "                    self.cluster_h[y] = self.cluster_h[x]\n",
    "                    self.select[y] = 1\n",
    "\n",
    "                else:\n",
    "                    self.cluster_head.add(x)\n",
    "                    self.cluster_h[x] = x\n",
    "                    self.level[x] = 0\n",
    "                    self.level[y] = 1\n",
    "                    self.cluster_h[y] = x\n",
    "                    self.inter_conn[x] = self.intra_conn[x]\n",
    "                    self.intra_conn[x] = 1\n",
    "                    self.intra_conn[y] = 1\n",
    "                    self.select[x] = 1\n",
    "                    self.select[y] = 1\n",
    "\n",
    "            else:  #If clusters are same\n",
    "#                 print(\"If clusters are same...,\")\n",
    "                if self.cluster_h[x] == self.cluster_h[y]:\n",
    "                    self.intra_conn[x] += 1\n",
    "                    self.intra_conn[y] += 1\n",
    "                else:\n",
    "                    self.inter_conn[x] += 1\n",
    "                    self.inter_conn[y] += 1\n",
    "#             print(\"cluster_h x:\",self.cluster_h[x],\"cluster_h y:\",self.cluster_h[y])\n",
    "#             print(\"Expansion phase started.\")\n",
    "            \n",
    "#             labels = set()\n",
    "#             for node in self.cluster_head: labels.add(node)\n",
    "#             comm_set_final = []\n",
    "#             for label in labels:\n",
    "#                 comm_set = []\n",
    "#                 for node in self.graph :\n",
    "#                     if self.cluster_h[node] == label :\n",
    "#                         comm_set.append(node)\n",
    "#                 comm_set_final.append(comm_set)\n",
    "            w += 1\n",
    "            if w == self.window_size or totalnodes==count:\n",
    "                print(\"**************************************************************************\")\n",
    "                window_counter += 1  \n",
    "                self.TBCD_dissol()\n",
    "                w = 0\n",
    "            if totalnodes in itr:\n",
    "                self.TBCD_dissol()\n",
    "                w = 0\n",
    "                print(\"_______________________________________________________________________________________________\")\n",
    "                print(\"Get result at node:\",totalnodes)\n",
    "                comm_set_final = detect_comm(self.graph, self.cluster_head, self.cluster_h)\n",
    "                resultt[totalnodes]=findresult(G,k,comm_set_final)\n",
    "        \n",
    "        displayresult(resultt,itr,perc)\n",
    "        dff = makeExcel(resultt,itr,perc)\n",
    "        return dff\n",
    "\n",
    "            \n",
    "    def checkingg(self):\n",
    "        print(\"666\")\n",
    "        \n",
    "        \n",
    "def detect_comm(graph, cluster_head, cluster_h):\n",
    "    labels = set()\n",
    "    for node in cluster_head:\n",
    "        labels.add(node)\n",
    "    comm_set_final = []\n",
    "    for label in labels:\n",
    "        comm_set = []\n",
    "        for node in graph :\n",
    "            if cluster_h[node] == label :\n",
    "                comm_set.append(node)\n",
    "        comm_set_final.append(comm_set)\n",
    "    return comm_set_final\n",
    "# obj=TBCD_mine()\n",
    "# perc=[20,40,60,80,100]\n",
    "# filename='LFR_500_0.0'\n",
    "# obj.execute_TBCD_txt(filename,perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj=TBCD_mine()\n",
    "# perc=[20,40,60,80,100]\n",
    "# filename='LFR_500_0.0'\n",
    "# obj.execute_TBCD_txt(filename,perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3W9mywEit_nM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello.\n"
     ]
    }
   ],
   "source": [
    "print(\"hello.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "haTniG2Lt_nM",
    "outputId": "957ade05-8092-4df7-d054-c3cf8dd5040c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization\n",
      "47594\n",
      "[38075, 42834, 47594]\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "Inside dissolution phase\n",
      "_______________________________________________________________________________________________\n",
      "Get result at node: 38075\n",
      "\n",
      "\n",
      "--------------- DCDIM -------------------\n"
     ]
    }
   ],
   "source": [
    "def executeDiv():\n",
    "    result_df=pd.DataFrame()\n",
    "    for i in range(1):\n",
    "        obj=TBCD_mine()\n",
    "#         perc=[10,20,30,40,50,60,70,80,90,100]\n",
    "        perc=[80,90,100]\n",
    "        filename='cond_mat2001'\n",
    "        dff=obj.execute_TBCD_txt(filename,perc)\n",
    "        dff2=result_df\n",
    "        result_df=dff\n",
    "        result_df = pd.concat([dff2, dff], ignore_index=True)\n",
    "        result_df.to_excel('Outputt.xlsx')\n",
    "def processResults():\n",
    "    df = pd.read_excel('Outputt.xlsx')\n",
    "    df=df.drop(columns='Unnamed: 0')\n",
    "    print(df.shape)\n",
    "#     print(df.head(8))\n",
    "    TBCD = df.loc[df['Name of Algorithm']=='TBCD']\n",
    "    DCDIM = df.loc[df['Name of Algorithm']=='DCDIM']\n",
    "    Greedy = df.loc[df['Name of Algorithm']=='Greedy']\n",
    "    CELF = df.loc[df['Name of Algorithm']=='CELF']\n",
    "    PMIA = df.loc[df['Name of Algorithm']=='PMIA']\n",
    "    FIMMOGA = df.loc[df['Name of Algorithm']=='FIMMOGA']\n",
    "    cso = df.loc[df['Name of Algorithm']=='CSO']\n",
    "    SSRPEA = df.loc[df['Name of Algorithm']=='SSR-PEA']\n",
    "    SIMPATH = df.loc[df['Name of Algorithm']=='SIMPATH']\n",
    "    DEGREE = df.loc[df['Name of Algorithm']=='Degree']\n",
    "    PAGERANK = df.loc[df['Name of Algorithm']=='PageRank']\n",
    "    \n",
    "    cols= df.columns\n",
    "\n",
    "    ddris = pd.DataFrame(TBCD.iloc[:,1:].mean(),columns = ['TBCD'])\n",
    "    dcdim = pd.DataFrame(DCDIM.iloc[:,1:].mean(),columns = ['DCDIM'])\n",
    "    greedy = pd.DataFrame(Greedy.iloc[:,1:].mean(),columns = ['Greedy'])\n",
    "    celf = pd.DataFrame(CELF.iloc[:,1:].mean(),columns = ['CELF'])\n",
    "    pmia = pd.DataFrame(PMIA.iloc[:,1:].mean(),columns = ['PMIA'])\n",
    "    simpath = pd.DataFrame(SIMPATH.iloc[:,1:].mean(),columns = ['SIMPATH'])\n",
    "    degree = pd.DataFrame(DEGREE.iloc[:,1:].mean(),columns = ['DEGREE'])\n",
    "    pagerank = pd.DataFrame(PAGERANK.iloc[:,1:].mean(),columns = ['PageRank'])\n",
    "    CSO = pd.DataFrame(cso.iloc[:,1:].mean(),columns = ['CSO'])\n",
    "    FIMMOGA = pd.DataFrame(FIMMOGA.iloc[:,1:].mean(),columns = ['FIMMOGA'])\n",
    "    SSRPEA = pd.DataFrame(SSRPEA.iloc[:,1:].mean(),columns = ['SSRPEA'])\n",
    "    \n",
    "    \n",
    "    resultant = pd.concat([dcdim,celf,pmia,degree,pagerank,CSO, FIMMOGA,SSRPEA], axis='columns')\n",
    "#     resultant = pd.concat([ddris,pmia,simpath,degree,pagerank], axis='columns')\n",
    "#     print(\"Resultant:\",resultant)\n",
    "    \n",
    "    result=resultant.iloc[0:]\n",
    "#     print(\"Result:\",result)\n",
    "    \n",
    "    result.to_excel('cond_mat2001_l05s003_Processed.xlsx')\n",
    "    \n",
    "    \n",
    "executeDiv()\n",
    "processResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
