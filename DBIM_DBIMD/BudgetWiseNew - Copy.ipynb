{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "lOpWXRCkSeNe"
   },
   "outputs": [],
   "source": [
    "# %pip install 'networkx<2.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "Z3Qlb0a7x8Jb"
   },
   "outputs": [],
   "source": [
    "# !pip install cdlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21971,
     "status": "ok",
     "timestamp": 1706072220779,
     "user": {
      "displayName": "Ashish Rathee",
      "userId": "11020670938656878428"
     },
     "user_tz": -330
    },
    "id": "lb7T4shHa0Rc",
    "outputId": "fa7c6a62-8823-460d-e646-fe8862108ce5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad6C-xYR03II"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "oZjyJyegav3I"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import heapq\n",
    "import argparse\n",
    "import threading\n",
    "import multiprocessing\n",
    "import sys\n",
    "import queue\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import math, time\n",
    "from copy import deepcopy\n",
    "import multiprocessing, json\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "import community\n",
    "\n",
    "#importing libraries that will be used\n",
    "# import networkx as nx#for creating network\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt#for plotting plots\n",
    "# import random\n",
    "# import time#claculating time\n",
    "# import math\n",
    "# from collections import Counter\n",
    "# from itertools import permutations\n",
    "# from itertools import combinations\n",
    "\n",
    "# from collections import defaultdict\n",
    "# from scipy.io import mmread# to read dataset\n",
    "# import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "OD7RKjsuuzaY"
   },
   "outputs": [],
   "source": [
    "#importing libraries that will be used\n",
    "import networkx as nx#for creating network\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt#for plotting plots\n",
    "\n",
    "import random\n",
    "import time#claculating time\n",
    "import math\n",
    "from collections import Counter\n",
    "from itertools import permutations\n",
    "from itertools import combinations\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.io import mmread# to read dataset\n",
    "import pandas as pd\n",
    "\n",
    "# from cdlib import algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JA9ixQytav3K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhAzXlPQav3L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0e9ikroav3L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "ewVhu3gBav3M"
   },
   "outputs": [],
   "source": [
    "'''Implementation of SimPath algorithm'''\n",
    "\n",
    "class CELFQueue:\n",
    "    # create if not exist\n",
    "    nodes = None\n",
    "    q = None\n",
    "    nodes_gain = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.q = []\n",
    "        self.nodes_gain = {}\n",
    "\n",
    "    def put(self, node, marginalgain):\n",
    "        self.nodes_gain[node] = marginalgain\n",
    "        heapq.heappush(self.q, (-marginalgain, node))\n",
    "\n",
    "    def update(self, node, marginalgain):\n",
    "        self.remove(node)\n",
    "        self.put(node, marginalgain)\n",
    "\n",
    "    def remove(self, node):\n",
    "        self.q.remove((-self.nodes_gain[node], node))\n",
    "        self.nodes_gain[node] = None\n",
    "        heapq.heapify(self.q)\n",
    "\n",
    "    def topn(self, n):\n",
    "        top = heapq.nsmallest(n, self.q)\n",
    "        top_ = list()\n",
    "        for t in top:\n",
    "            top_.append(t[1])\n",
    "        return top_\n",
    "\n",
    "    def get_gain(self, node):\n",
    "        return self.nodes_gain[node]\n",
    "\n",
    "\n",
    "\n",
    "def init_D(graph):\n",
    "    D = {}\n",
    "#     for i in range(graph.node_num + 1):\n",
    "    for i in graph.nodes:\n",
    "        D[i]=[]\n",
    "    return D\n",
    "\n",
    "class Graph:\n",
    "    nodes = None\n",
    "    edges = None\n",
    "    children = None\n",
    "    parents = None\n",
    "    node_num = None\n",
    "    edge_num = None\n",
    "\n",
    "    def __init__(self, nodes, edges, children, parents, node_num, edge_num):\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "        self.children = children\n",
    "        self.parents = parents\n",
    "        self.node_num = node_num\n",
    "        self.edge_num = edge_num\n",
    "#         print(\"Numnodes:\",self.node_num)\n",
    "    def get_children(self, node):\n",
    "        ch = self.children.get(node)\n",
    "        if ch is None:\n",
    "            self.children[node] = []\n",
    "        return self.children[node]\n",
    "\n",
    "    def get_parents(self, node):\n",
    "        pa = self.parents.get(node)\n",
    "        if pa is None:\n",
    "            self.parents[node] = []\n",
    "        return self.parents[node]\n",
    "\n",
    "    def get_weight(self, src, dest):\n",
    "        weight = self.edges.get((src, dest))\n",
    "        if weight is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return weight\n",
    "\n",
    "    # return true if node1 is parent of node 2 , else return false\n",
    "    def is_parent_of(self, node1, node2):\n",
    "        if self.get_weight(node1, node2) != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # return true if node1 is child of node 2 , else return false\n",
    "    def is_child_of(self, node1, node2):\n",
    "        return self.is_parent_of(node2, node1)\n",
    "\n",
    "    def get_out_degree(self, node):\n",
    "        return len(self.get_children(node))\n",
    "\n",
    "    def get_in_degree(self, node):\n",
    "        return len(self.get_parents(node))\n",
    "\n",
    "\n",
    "def read_graph_info(path):\n",
    "    if os.path.exists(path):\n",
    "        parents = {}\n",
    "        children = {}\n",
    "        edges = {}\n",
    "        nodes = set()\n",
    "\n",
    "        try:\n",
    "            f = open(path, 'r')\n",
    "            txt = f.readlines()\n",
    "            header = str.split(txt[0])\n",
    "            node_num = int(header[0])\n",
    "            edge_num = int(header[1])\n",
    "\n",
    "            for line in txt[1:]:\n",
    "#                 print(\"line:\",line)\n",
    "                row = str.split(line)\n",
    "#                 print(row)\n",
    "                src = int(row[0])\n",
    "                des = int(row[1])\n",
    "                nodes.add(src)\n",
    "                nodes.add(des)\n",
    "#                 print(src,des)\n",
    "                if children.get(src) is None:\n",
    "                    children[src] = []\n",
    "                if parents.get(des) is None:\n",
    "                    parents[des] = []\n",
    "\n",
    "#                 weight = float(row[2])\n",
    "                weight=round(random.uniform(0.0,1.0),2)\n",
    "                edges[(src, des)] = weight\n",
    "                children[src].append(des)\n",
    "                parents[des].append(src)\n",
    "\n",
    "            return list(nodes), edges, children, parents, node_num, edge_num\n",
    "        except IOError:\n",
    "            print('IOError')\n",
    "    else:\n",
    "        print('file can not found')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_vertex_cover(graph):\n",
    "#     # dv[i] out degree of node i+1\n",
    "#     dv = np.zeros(graph.node_num)\n",
    "#     # e[i,j] = 0: edge (i+1,j+1),(j+1,i+1) checked\n",
    "#     check_array = np.zeros((graph.node_num, graph.node_num))\n",
    "#     checked = 0\n",
    "# #     print(range(graph.node_num))\n",
    "#     for i in range(graph.node_num):\n",
    "#         # for a edge (i,j) and (j,i) may be count twice but the algorithm is to find a vertex cover. it doesn't mater\n",
    "#         dv[i] = graph.get_out_degree(i + 1) + graph.get_in_degree(i + 1)\n",
    "#     # V: Vertex cover\n",
    "#     V = set()\n",
    "# #     print(dv)\n",
    "#     while checked < graph.edge_num:\n",
    "#         s = dv.argmax() + 1\n",
    "#         V.add(s)\n",
    "#         # make sure that never to select this node again\n",
    "#         children = graph.get_children(s)\n",
    "#         parents = graph.get_parents(s)\n",
    "#         for child in children:\n",
    "#             if check_array[s - 1][child - 1] == 0:\n",
    "#                 check_array[s - 1][child - 1] = 1\n",
    "#                 checked = checked + 1\n",
    "#         for parent in parents:\n",
    "#             if check_array[parent - 1][s - 1] == 0:\n",
    "#                 check_array[parent - 1][s - 1] = 1\n",
    "#                 checked = checked + 1\n",
    "#         dv[s - 1] = -1\n",
    "#     return list(V)\n",
    "\n",
    "\n",
    "\n",
    "def forward(Q, D, spd, pp, r, W, U, spdW_u, graph):\n",
    "    x = Q[-1]\n",
    "    if U is None:\n",
    "        U = []\n",
    "    children = graph.get_children(x)\n",
    "    count = 0\n",
    "    while True:\n",
    "        # any suitable chid is ok\n",
    "\n",
    "#         for child in range(count, len(children)):\n",
    "        flag=1\n",
    "        for y in children:\n",
    "\n",
    "#             print(\"y,D[x],type:\",y,D[x],type(D[x]))\n",
    "#             if(y not in D[x]):\n",
    "#                 print(\"YES\")\n",
    "\n",
    "            if (y in W) and (y not in Q) and (y not in D[x]):\n",
    "#                 y = children[child]\n",
    "                flag=0\n",
    "                break\n",
    "#             count = count + 1\n",
    "\n",
    "        # no such child:\n",
    "        if flag==1:\n",
    "            return Q, D, spd, pp\n",
    "\n",
    "        if pp * graph.get_weight(x, y) < r:\n",
    "            D[x].append(y)\n",
    "        else:\n",
    "            Q.append(y)\n",
    "            pp = pp * graph.get_weight(x, y)\n",
    "            spd = spd + pp\n",
    "            D[x].append(y)\n",
    "            x = Q[-1]\n",
    "            for v in U:\n",
    "                if v not in Q:\n",
    "                    spdW_u[v] = spdW_u[v] + pp\n",
    "            children = graph.get_children(x)\n",
    "            count = 0\n",
    "\n",
    "\n",
    "\n",
    "def backtrack(u, r, W, U, spdW_, graph):\n",
    "    Q = [u]\n",
    "    spd = 1\n",
    "    pp = 1\n",
    "    D = init_D(graph)\n",
    "\n",
    "    while len(Q) != 0:\n",
    "        Q, D, spd, pp = forward(Q, D, spd, pp, r, W, U, spdW_, graph)\n",
    "        u = Q.pop()\n",
    "#         print(\"In backtrack:type,Q,u\",type(Q),Q,u)\n",
    "        D[u] = []\n",
    "        if len(Q) != 0:\n",
    "            v = Q[-1]\n",
    "            pp = pp / graph.get_weight(v, u)\n",
    "    return spd\n",
    "\n",
    "\n",
    "\n",
    "def simpath_spread(S, r, U, graph, spdW_=None):\n",
    "    spread = 0\n",
    "    # W: V-S\n",
    "    W = set(graph.nodes).difference(S)\n",
    "    if U is None or spdW_ is None:\n",
    "        spdW_={}\n",
    "        for i in graph.nodes:\n",
    "            spdW_[i]=0\n",
    "#         spdW_ = np.zeros(graph.node_num + 1)\n",
    "        # print 'U None'\n",
    "    for u in S:\n",
    "        W.add(u)\n",
    "        # print spdW_[u]\n",
    "        spread = spread + backtrack(u, r, W, U, spdW_[u], graph)\n",
    "        # print spdW_[u]\n",
    "        W.remove(u)\n",
    "    return spread\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simpath(graph, k, r, l):\n",
    "    C = set(get_vertex_cover(graph))\n",
    "    V = set(graph.nodes)\n",
    "\n",
    "    V_C = V.difference(C)\n",
    "    # spread[x] is spd of S + x\n",
    "#     spread = np.zeros(graph.node_num + 1)\n",
    "    spread={}\n",
    "    for i in graph.nodes:\n",
    "        spread[i]=0\n",
    "#     spdV_ = np.ones((graph.node_num + 1, graph.node_num + 1))\n",
    "    spdV_={}\n",
    "    for i in graph.nodes:\n",
    "        dd={}\n",
    "        for j in graph.nodes:\n",
    "            dd[j]=0\n",
    "        spdV_[i]=dd\n",
    "\n",
    "\n",
    "    for u in C:\n",
    "        U = V_C.intersection(set(graph.get_parents(u)))\n",
    "        spread[u] = simpath_spread(set([u]), r, U, graph, spdV_)\n",
    "    for v in V_C:\n",
    "        v_children = graph.get_children(v)\n",
    "        for child in v_children:\n",
    "            spread[v] = spread[v] + spdV_[child][v] * graph.get_weight(v, child)\n",
    "        spread[v] = spread[v] + 1\n",
    "    celf = CELFQueue()\n",
    "    # put all nodes into celf queqe\n",
    "    # spread[v] is the marginal gain at this time\n",
    "\n",
    "#     for node in range(1, graph.node_num + 1):\n",
    "#         celf.put(node, spread[node])\n",
    "    for node in graph.nodes:\n",
    "        celf.put(node, spread[node])\n",
    "\n",
    "    S = set()\n",
    "    W = V\n",
    "    spd = 0\n",
    "    # mark the node that checked before during the same Si\n",
    "#     checked = np.zeros(graph.node_num + 1)\n",
    "    checked={}\n",
    "    for i in graph.nodes:\n",
    "        checked[i]=0\n",
    "\n",
    "    while len(S) < k:\n",
    "        U = celf.topn(l)\n",
    "#         spdW_ = np.ones((graph.node_num + 1, graph.node_num + 1))\n",
    "#         spdV_x = np.zeros(graph.node_num + 1)\n",
    "\n",
    "        spdW_={}\n",
    "        spdV_x={}\n",
    "        for i in graph.nodes:\n",
    "            dd={}\n",
    "            spdV_x[i]=0\n",
    "            for j in graph.nodes:\n",
    "                dd[j]=1\n",
    "            spdW_[i]=dd\n",
    "\n",
    "        simpath_spread(S, r, U, graph, spdW_=spdW_)\n",
    "        for x in U:\n",
    "            for s in S:\n",
    "                spdV_x[x] = spdV_x[x] + spdW_[s][x]\n",
    "        for x in U:\n",
    "            if checked[x] != 0:\n",
    "                S.add(x)\n",
    "                W = W.difference(set([x]))\n",
    "                spd = spread[x]\n",
    "                # print spread[x],simpath_spread(S,r,None,None)\n",
    "#                 checked = np.zeros(graph.node_num + 1)\n",
    "#                 for i in graph.nodes:\n",
    "#                     checked[i]=0\n",
    "                celf.remove(x)\n",
    "                break\n",
    "            else:\n",
    "                spread[x] = backtrack(x, r, W, None, None, graph) + spdV_x[x]\n",
    "                checked[x] = 1\n",
    "                celf.update(x, spread[x] - spd)\n",
    "    return S\n",
    "\n",
    "\n",
    "\n",
    "# node,edges,children,parents,nodenum,edge_num=read_graph_info('dolphins.txt') #Input dataset with 'total nodes' and 'total edges' in the 'first line'\n",
    "# graph = Graph(node,edges,children,parents,nodenum,edge_num)\n",
    "# seeds = simpath(graph, 3, 0.5, 4)\n",
    "# print(\"seed:\",seeds)\n",
    "\n",
    "\n",
    "\n",
    "def SIMPATH_setup(G):\n",
    "    parents = {}\n",
    "    children = {}\n",
    "    edges = {}\n",
    "    nodes = set()\n",
    "    node_num = len(G.nodes())\n",
    "    edge_num = len(G.edges())\n",
    "#     print(node_num,edge_num)\n",
    "    for src,des in G.edges():\n",
    "        nodes.add(src)\n",
    "        nodes.add(des)\n",
    "        if children.get(src) is None:\n",
    "            children[src] = []\n",
    "        if parents.get(des) is None:\n",
    "            parents[des] = []\n",
    "        weight=G[src][des]['weight']\n",
    "        edges[(src, des)] = weight\n",
    "        children[src].append(des)\n",
    "        parents[des].append(src)\n",
    "    return list(nodes), edges, children, parents, node_num, edge_num\n",
    "\n",
    "def run_SIMPATH(G,k):\n",
    "    node,edges,children,parents,nodenum,edge_num=SIMPATH_setup(G)\n",
    "    graph = Graph(node,edges,children,parents,nodenum,edge_num)\n",
    "    seeds = simpath(graph, k, 0.5, k+1)\n",
    "    return list(seeds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vertex_cover(graph):\n",
    "#     dv = np.zeros(graph.node_num)\n",
    "    dv=dict()\n",
    "#     check_array = np.zeros((graph.node_num, graph.node_num))\n",
    "    check_array={}\n",
    "    for i in graph.nodes:\n",
    "        dd={}\n",
    "        for j in graph.nodes:\n",
    "            dd[j]=0\n",
    "        check_array[i]=dd\n",
    "\n",
    "#     checked = 0\n",
    "\n",
    "#     for i in range(graph.node_num):\n",
    "#         dv[i] =graph.get_out_degree(i + 1) + graph.get_in_degree(i + 1)\n",
    "    for i in graph.nodes:\n",
    "        dv[i]=graph.get_out_degree(i) + graph.get_in_degree(i)\n",
    "\n",
    "    # V: Vertex cover\n",
    "    V = set()\n",
    "#     while checked < graph.edge_num:\n",
    "    for checked in graph.nodes:\n",
    "#         s = dv.argmax() + 1\n",
    "        _,s=max(zip(dv.values(), dv.keys()))\n",
    "        V.add(s)\n",
    "        # make sure that never to select this node again\n",
    "        children = graph.get_children(s)\n",
    "        parents = graph.get_parents(s)\n",
    "        for child in children:\n",
    "            if check_array[s][child] == 0:\n",
    "                check_array[s][child] = 1\n",
    "#                 checked = checked + 1\n",
    "        for parent in parents:\n",
    "            if check_array[parent][s] == 0:\n",
    "                check_array[parent][s] = 1\n",
    "#                 checked = checked + 1\n",
    "        dv[s] = -1\n",
    "#     print(\"In Vertex conver V is:\",V)\n",
    "    return list(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "2NjSkFqDav3O"
   },
   "outputs": [],
   "source": [
    "''' Implementation of PMIA algorithm [1].\n",
    "[1] -- Scalable Influence Maximization for Prevalent Viral Marketing in Large-Scale Social Networks.\n",
    "'''\n",
    "\n",
    "\n",
    "def updateAP(ap, S, PMIIAv, PMIIA_MIPv, Ep):\n",
    "    ''' Assumption: PMIIAv is a directed tree, which is a subgraph of general G.\n",
    "    PMIIA_MIPv -- dictionary of MIP from nodes in PMIIA\n",
    "    PMIIAv is rooted at v.\n",
    "    '''\n",
    "    # going from leaves to root\n",
    "    sorted_MIPs = sorted(PMIIA_MIPv.items(), key = lambda MIP: len(MIP), reverse = True)\n",
    "#     print(\"Edges:\",PMIIAv.nodes)\n",
    "#     for e in PMIIAv.edges:\n",
    "#         print(e)\n",
    "#     for u,_ in sorted_MIPs:\n",
    "#         print(u,_)\n",
    "\n",
    "\n",
    "    for u, _ in sorted_MIPs:\n",
    "        if u in S:\n",
    "            ap[(u, PMIIAv)] = 1\n",
    "        elif not PMIIAv.in_edges([u]):\n",
    "#             print(\"\\n\\nGoin in elif\")\n",
    "            ap[(u, PMIIAv)] = 0\n",
    "        else:\n",
    "            in_edges = PMIIAv.in_edges([u], data=True)\n",
    "            prod = 1\n",
    "            for w, _, edata in in_edges:\n",
    "                # p = (1 - (1 - Ep[(w, u)])**edata[\"weight\"])\n",
    "\n",
    "                print(\"In updateAP\")\n",
    "                print(\"hello:\",w,PMIIAv)\n",
    "                if (w,PMIIAv) in ap.keys():\n",
    "                    print(\"Key present\")\n",
    "                else:\n",
    "                    print(\"Key is NOT present\")\n",
    "#                 print(\"keys:\",\n",
    "\n",
    "                p = Ep[(w,u)]\n",
    "                prod *= 1 - ap[(w, PMIIAv)]*p\n",
    "#             print(ap,u, PMIIAv)\n",
    "            ap[(u, PMIIAv)] = 1 - prod\n",
    "\n",
    "def updateAlpha(alpha, v, S, PMIIAv, PMIIA_MIPv, Ep, ap):\n",
    "    # going from root to leaves\n",
    "    sorted_MIPs =  sorted(PMIIA_MIPv.items(), key = lambda MIP: len(MIP))\n",
    "    for u, mip in sorted_MIPs:\n",
    "        if u == v:\n",
    "            alpha[(PMIIAv, u)] = 1\n",
    "        else:\n",
    "            out_edges = PMIIAv.out_edges([u])\n",
    "            assert len(out_edges) == 1, \"node u=%s must have exactly one neighbor, got %s instead\" %(u, len(out_edges))\n",
    "            out_edges=list(out_edges)\n",
    "#             print(\"out_edges:\",out_edges,type(out_edges))\n",
    "\n",
    "            w = out_edges[0][1]\n",
    "            if w in S:\n",
    "                alpha[(PMIIAv, u)] = 0\n",
    "            else:\n",
    "                in_edges = PMIIAv.in_edges([w], data=True)\n",
    "                prod = 1\n",
    "                for up, _, edata in in_edges:\n",
    "                    if up != u:\n",
    "                        # pp_upw = 1 - (1 - Ep[(up, w)])**edata[\"weight\"]\n",
    "                        pp_upw = Ep[(up, w)]\n",
    "                        prod *= (1 - ap[up]*pp_upw)\n",
    "                # alpha[(PMIIAv, u)] = alpha[(PMIIAv, w)]*(1 - (1 - Ep[(u,w)])**PMIIAv[u][w][\"weight\"])*prod\n",
    "                alpha[(PMIIAv, u)] = alpha[(PMIIAv, w)]*(Ep[(u,w)])*prod\n",
    "\n",
    "def computePMIOA(G, u, theta, S, Ep):\n",
    "    '''\n",
    "     Compute PMIOA -- subgraph of G that's rooted at u.\n",
    "     Uses Dijkstra's algorithm until length of path doesn't exceed -log(theta)\n",
    "     or no more nodes can be reached.\n",
    "    '''\n",
    "    # initialize PMIOA\n",
    "    PMIOA = nx.DiGraph()\n",
    "    PMIOA.add_node(u)\n",
    "    PMIOA_MIP = {u: [u]} # MIP(u,v) for v in PMIOA\n",
    "\n",
    "    crossing_edges = set([out_edge for out_edge in G.out_edges([u]) if out_edge[1] not in S + [u]])\n",
    "    edge_weights = dict()\n",
    "    dist = {u: 0} # shortest paths from the root u\n",
    "\n",
    "    # grow PMIOA\n",
    "    while crossing_edges:\n",
    "        # Dijkstra's greedy criteria\n",
    "        min_dist = float(\"Inf\")\n",
    "        sorted_crossing_edges = sorted(crossing_edges) # to break ties consistently\n",
    "        for edge in sorted_crossing_edges:\n",
    "            if edge not in edge_weights:\n",
    "                # edge_weights[edge] = -math.log(1 - (1 - Ep[edge])**G[edge[0]][edge[1]][\"weight\"])\n",
    "                edge_weights[edge] = -math.log(Ep[edge])\n",
    "            edge_weight = edge_weights[edge]\n",
    "            if dist[edge[0]] + edge_weight < min_dist:\n",
    "                min_dist = dist[edge[0]] + edge_weight\n",
    "                min_edge = edge\n",
    "        # check stopping criteria\n",
    "        if min_dist < -math.log(theta):\n",
    "            dist[min_edge[1]] = min_dist\n",
    "            # PMIOA.add_edge(min_edge[0], min_edge[1], {\"weight\": G[min_edge[0]][min_edge[1]][\"weight\"]})\n",
    "            PMIOA.add_edge(min_edge[0], min_edge[1])\n",
    "            PMIOA_MIP[min_edge[1]] = PMIOA_MIP[min_edge[0]] + [min_edge[1]]\n",
    "            # update crossing edges\n",
    "            crossing_edges.difference_update(G.in_edges(min_edge[1]))\n",
    "            crossing_edges.update([out_edge for out_edge in G.out_edges(min_edge[1])\n",
    "                                   if (out_edge[1] not in PMIOA) and (out_edge[1] not in S)])\n",
    "        else:\n",
    "            break\n",
    "    return PMIOA, PMIOA_MIP\n",
    "\n",
    "def updateIS(IS, S, u, PMIOA, PMIIA):\n",
    "    for v in PMIOA[u]:\n",
    "        for si in S:\n",
    "            # if seed node is effective and it's blocked by u\n",
    "            # then it becomes ineffective\n",
    "            if (si in PMIIA[v]) and (si not in IS[v]) and (u in PMIIA[v][si]):\n",
    "                    IS[v].append(si)\n",
    "\n",
    "def computePMIIA(G, ISv, v, theta, S, Ep):\n",
    "\n",
    "    # initialize PMIIA\n",
    "    PMIIA = nx.DiGraph()\n",
    "    PMIIA.add_node(v)\n",
    "    PMIIA_MIP = {v: [v]} # MIP(u,v) for u in PMIIA\n",
    "\n",
    "    crossing_edges = set([in_edge for in_edge in G.in_edges([v]) if in_edge[0] not in ISv + [v]])\n",
    "    edge_weights = dict()\n",
    "    dist = {v: 0} # shortest paths from the root u\n",
    "\n",
    "    # grow PMIIA\n",
    "    while crossing_edges:\n",
    "        # Dijkstra's greedy criteria\n",
    "        min_dist = float(\"Inf\")\n",
    "        sorted_crossing_edges = sorted(crossing_edges) # to break ties consistently\n",
    "        for edge in sorted_crossing_edges:\n",
    "            if edge not in edge_weights:\n",
    "                # edge_weights[edge] = -math.log(1 - (1 - Ep[edge])**G[edge[0]][edge[1]][\"weight\"])\n",
    "#                 edgevalue=Ep[edge]\n",
    "#                 print(\"Ep[edge]:\",Ep[edge],edgevalue)\n",
    "                edge_weights[edge] = -(math.log(Ep[edge]))\n",
    "#                 edge_weights[edge] = -(math.log(edgevalue))\n",
    "\n",
    "            edge_weight = edge_weights[edge]\n",
    "            if dist[edge[1]] + edge_weight < min_dist:\n",
    "                min_dist = dist[edge[1]] + edge_weight\n",
    "                min_edge = edge\n",
    "        # check stopping criteria\n",
    "        # print min_edge, ':', min_dist, '-->', -math.log(theta)\n",
    "        if min_dist < -math.log(theta):\n",
    "            dist[min_edge[0]] = min_dist\n",
    "            # PMIIA.add_edge(min_edge[0], min_edge[1], {\"weight\": G[min_edge[0]][min_edge[1]][\"weight\"]})\n",
    "            PMIIA.add_edge(min_edge[0], min_edge[1])\n",
    "            PMIIA_MIP[min_edge[0]] = PMIIA_MIP[min_edge[1]] + [min_edge[0]]\n",
    "            # update crossing edges\n",
    "            crossing_edges.difference_update(G.out_edges(min_edge[0]))\n",
    "            if min_edge[0] not in S:\n",
    "                crossing_edges.update([in_edge for in_edge in G.in_edges(min_edge[0])\n",
    "                                       if (in_edge[0] not in PMIIA) and (in_edge[0] not in ISv)])\n",
    "        else:\n",
    "            break\n",
    "    return PMIIA, PMIIA_MIP\n",
    "\n",
    "def PMIA(G, k, theta, Ep):\n",
    "    start = time.time()\n",
    "    # initialization\n",
    "    S = []\n",
    "    IncInf = dict(zip(G.nodes(), [0]*len(G)))\n",
    "    PMIIA = dict() # node to tree\n",
    "    PMIOA = dict()\n",
    "    PMIIA_MIP = dict() # node to MIPs (dict)\n",
    "    PMIOA_MIP = dict()\n",
    "    ap = dict()\n",
    "    alpha = dict()\n",
    "    IS = dict()\n",
    "    for v in G:\n",
    "        IS[v] = []\n",
    "        PMIIA[v], PMIIA_MIP[v] = computePMIIA(G, IS[v], v, theta, S, Ep)\n",
    "        for u in PMIIA[v]:\n",
    "            ap[u] = 0 # ap of u node in PMIIA[v]\n",
    "        updateAlpha(alpha, v, S, PMIIA[v], PMIIA_MIP[v], Ep, ap)\n",
    "        for u in PMIIA[v]:\n",
    "            IncInf[u] += alpha[(PMIIA[v], u)]*(1 - ap[u])\n",
    "#     print('Finished initialization')\n",
    "#     print(time.time() - start)\n",
    "\n",
    "    # main loop\n",
    "    for i in range(k):\n",
    "#         print(IncInf)\n",
    "#         u, _ = max(IncInf.items(), key = lambda dk, dv: dv)\n",
    "        _,u=max(zip(IncInf.values(), IncInf.keys()))\n",
    "        IncInf.pop(u) # exclude node u for next iterations\n",
    "        PMIOA[u], PMIOA_MIP[u] = computePMIOA(G, u, theta, S, Ep)\n",
    "        for v in PMIOA[u]:\n",
    "            for w in PMIIA[v]:\n",
    "                if w not in S + [u]:\n",
    "                    IncInf[w] -= alpha[(PMIIA[v],w)]*(1 - ap[w])\n",
    "\n",
    "        updateIS(IS, S, u, PMIOA_MIP, PMIIA_MIP)\n",
    "\n",
    "        S.append(u)\n",
    "\n",
    "        for v in PMIOA[u]:\n",
    "            if v != u:\n",
    "                PMIIA[v], PMIIA_MIP[v] = computePMIIA(G, IS[v], v, theta, S, Ep)\n",
    "\n",
    "\n",
    "                for uu in PMIIA[v].nodes:\n",
    "                    if uu in S:\n",
    "#                         print(\"IN s\")\n",
    "                        ap[u] = 1\n",
    "                    elif not PMIIA[v].in_edges([uu]):\n",
    "#                         print(\"Second else if\")\n",
    "            #             print(\"\\n\\nGoin in elif\")\n",
    "                        ap[uu] = 0\n",
    "                    else:\n",
    "                        in_edges = PMIIA[v].in_edges([uu], data=True)\n",
    "                        prod = 1\n",
    "#                         print(\"inEdgessss:\",in_edges)\n",
    "                        for w, _, edata in in_edges:\n",
    "                            # p = (1 - (1 - Ep[(w, u)])**edata[\"weight\"]\n",
    "                            p = Ep[(w,uu)]\n",
    "                            prod *= 1 - ap[w]*p\n",
    "                        ap[uu] = 1 - prod\n",
    "\n",
    "#                 updateAP(ap, S, PMIIA[v], PMIIA_MIP[v], Ep)\n",
    "                updateAlpha(alpha, v, S, PMIIA[v], PMIIA_MIP[v], Ep, ap)\n",
    "                # add new incremental influence\n",
    "                for w in PMIIA[v]:\n",
    "                    if w not in S:\n",
    "                        IncInf[w] += alpha[(PMIIA[v], w)]*(1 - ap[w])\n",
    "\n",
    "    return S\n",
    "\n",
    "def getCoverage(G, S, Ep):\n",
    "    return IC(G, S)\n",
    "\n",
    "\n",
    "def run_PMIA(GG,k):\n",
    "    Ep = dict()\n",
    "    G=nx.DiGraph()\n",
    "    for edgee in GG.edges():\n",
    "        s=edgee[0]\n",
    "        t=edgee[1]\n",
    "        Ep[(int(s), int(t))] = GG[s][t]['weight']\n",
    "        G.add_edge(s,t,weight=GG[s][t]['weight'])\n",
    "        G.nodes[s]['thres']=(G.degree(s)/2)\n",
    "        G.nodes[t]['thres']=(G.degree(t)/2)\n",
    "    theta = 1.0/20\n",
    "    S = PMIA(G, k, theta, Ep)\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0tilfBsav3Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "j86wDWWIP_YB"
   },
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "  file1 = open(path,'r')\n",
    "  sender = list()\n",
    "  receiver = list()\n",
    "\n",
    "  for i in file1.readlines():\n",
    "    sender.append(int(i.split(' ')[0]))\n",
    "    receiver.append(int(i.split(' ')[1].split('\\n')[0]))\n",
    "\n",
    "  df = pd.DataFrame(list(zip(sender,receiver)),columns =['source', 'target'])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "OLnRTTTVav3R"
   },
   "outputs": [],
   "source": [
    "def gmltotxt(filename):\n",
    "    import networkx as nx\n",
    "    import pandas as pd\n",
    "    g = nx.read_gml('airlines.gml')\n",
    "    nx.write_edgelist(g, 'edgelistFile.csv', delimiter=',')\n",
    "    df = pd.read_csv('edgelistFile.csv')\n",
    "    file = open(\"myfile.txt\",\"w\")\n",
    "    for i in range(len(df)):\n",
    "        x=df.iloc[i][0]\n",
    "        y=df.iloc[i][1]\n",
    "        file.write(str(x)+\" \"+str(y)+\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "2_C0G0gyav3R"
   },
   "outputs": [],
   "source": [
    "def getcand(G,k, comm):#Take df and  all new nodes as input and return a seed node.\n",
    "    s=[]\n",
    "    s=GreedyDiv(G,k, comm)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeA9FfXZvXSN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhJ1sm7yt_nD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hsLLGd4av3S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZn0MKQet_nE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "ER71qO5zt_nE"
   },
   "outputs": [],
   "source": [
    "def linear_Threshold(graph, seeds):\n",
    "    seeds=list(seeds)\n",
    "    influnces = seeds[:]\n",
    "    queue = influnces[:]\n",
    "    pre_node_record = defaultdict(float)\n",
    "#     print(\"Queue:\",queue)\n",
    "\n",
    "#     print(\"Influences:\",influnces)\n",
    "    while len(queue) != 0:\n",
    "        node = queue.pop(0)\n",
    "#         print(\"----------------------------------------------------------------------\")\n",
    "#         print(\"Take node:\",node)\n",
    "#         print(\"Neighbour:\",graph[node])\n",
    "        for element in graph[node]:\n",
    "            if element not in influnces:\n",
    "#                 print(\"Element:\",element,\"prerecored\",pre_node_record[element])\n",
    "                pre_node_record[element] = pre_node_record[element] + graph[node][element]['weight']\n",
    "#                 print(pre_node_record[element])\n",
    "                if pre_node_record[element] >= graph.nodes[element]['thres']:\n",
    "#                     print(\">>>>>>>>>>>>>>>>>>node influeced:\",element)\n",
    "                    influnces.append(element)\n",
    "                    queue.append(element)\n",
    "#     influnce_num = len(influnces)\n",
    "#     print(\"Seed set:\",seeds,\"Activated nodes:\",influnces)\n",
    "    return influnces\n",
    "# linear_Threshold(GG,[45,29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rw3IxheNav3S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "oPQG_C69DRBL"
   },
   "outputs": [],
   "source": [
    "def communityDiversityFunction(graph, activated_set, community_list):\n",
    "    num_communities = len(community_list)\n",
    "    #activated_set = activated_nodes\n",
    "\n",
    "    diversity_sum = 0\n",
    "    num_activated_communities = 0\n",
    "\n",
    "    for community in community_list:\n",
    "        community_set = set(community)\n",
    "        common_elements = community_set.intersection(activated_set)\n",
    "\n",
    "        if common_elements:\n",
    "            current_diversity = len(common_elements) / len(community_set)\n",
    "            diversity_sum += current_diversity\n",
    "            num_activated_communities += 1\n",
    "    if num_activated_communities==0:\n",
    "        num_activated_communities=1\n",
    "\n",
    "    return diversity_sum * (num_activated_communities / num_communities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9RIw9O-DRBL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "g4vcQ64yICZF"
   },
   "outputs": [],
   "source": [
    "def compute_Phi_dbim(G, S, communities):\n",
    "    if len(S)<1:\n",
    "        return 0\n",
    "    lambda_G = 0.5\n",
    "    v_length = G.number_of_nodes()\n",
    "\n",
    "    IC_S = IC(G, S)\n",
    "    activated_set_S_length = len(IC_S)\n",
    "    diversity_activated_set_S = communityDiversityFunction(G, IC_S, communities)\n",
    "#     diversity_V = len(communities)\n",
    "#     diversity_V = activated_set_S_length\n",
    "#     print(\"diversity_activated_set_S: \", diversity_activated_set_S)\n",
    "#     print(\"diversity_V: \", diversity_V)\n",
    "    phi_S = ((1 - lambda_G)* (activated_set_S_length/v_length)) + (lambda_G * (diversity_activated_set_S))\n",
    "    return phi_S,IC_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "Z884A4ODav3T"
   },
   "outputs": [],
   "source": [
    "def compute_Phi(G, S, communities):\n",
    "    if len(S)<1:\n",
    "        return 0\n",
    "    lambda_G = 0.5\n",
    "    v_length = G.number_of_nodes()\n",
    "\n",
    "    IC_S = IC(G, S)\n",
    "    activated_set_S_length = len(IC_S)\n",
    "    diversity_activated_set_S = communityDiversityFunction(G, IC_S, communities)\n",
    "#     diversity_V = len(communities)\n",
    "#     diversity_V = activated_set_S_length\n",
    "#     print(\"diversity_activated_set_S: \", diversity_activated_set_S)\n",
    "#     print(\"diversity_V: \", diversity_V)\n",
    "    phi_S = ((1 - lambda_G)* (activated_set_S_length/v_length)) + (lambda_G * (diversity_activated_set_S))\n",
    "    if( phi_S>1):\n",
    "        print(\"yes\")\n",
    "    return phi_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "g6CDAFBLav3T"
   },
   "outputs": [],
   "source": [
    "\n",
    "# obj=TBCD_mine()\n",
    "# perc=[20,40,60,80]\n",
    "# filename='200_0'\n",
    "# print(\"hello\")\n",
    "# obj.execute_TBCD_txt(filename,perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zmXLBf2av3T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "BocmOfr-H5po"
   },
   "outputs": [],
   "source": [
    "def findk(i):#dynamic calculation of k according the percentage of current dataset\n",
    "  k=(0.01*i)\n",
    "  if(i==0):\n",
    "    k=1\n",
    "  if(k>int(k)):\n",
    "   k=int(k)+1\n",
    "  return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmG7U7uqu8kV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "jFp7JuUxav3U"
   },
   "outputs": [],
   "source": [
    "def displayresult(result,itr,perc):\n",
    "    print(\"\\n\\n\\n\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "    linestylee=['dashdot','dashed','dotted','-', '--', ':','-.','dashed','dotted','-', '--', ':','-.' ]\n",
    "\n",
    "    markerss = ['o','v','s','*','+','x','D','d','X','P','+','x','D','d','X','P']\n",
    "#     descriptions = ['circle', 'triangle_down','square','star', 'plus','x','diamond', 'thin_diamond','x (filled)','plus (filled)']\n",
    "    communitiesITR=[]\n",
    "    Name=[]\n",
    "    activatednodesITR=[]\n",
    "    totalcomm=[]\n",
    "    timeITR=[]\n",
    "    tt=[]\n",
    "#     print(itr,result)\n",
    "\n",
    "    for i in result:\n",
    "        for j in range(len(result[i])):\n",
    "            Name.append(result[i][j]['Name'])\n",
    "        break\n",
    "#     print(Name)\n",
    "\n",
    "#     for i in result:\n",
    "# #         print(result[i])\n",
    "#         for j in range(len(result[i])):\n",
    "#             totalcomm.append(result[i][j]['Total communitites'])\n",
    "#             break\n",
    "#     print(\"Total communitites:\",totalcomm)\n",
    "\n",
    "    for i in result:\n",
    "        active=[]\n",
    "        comm=[]\n",
    "        tt=[]\n",
    "        for j in range(len(result[i])):\n",
    "            active.append(result[i][j]['length of activated nodes'])\n",
    "            comm.append(result[i][j]['number of communities'])\n",
    "            tt.append(result[i][j]['time'])\n",
    "        activatednodesITR.append(active)\n",
    "        communitiesITR.append(comm)\n",
    "        timeITR.append(tt)\n",
    "#     print(\"Activated nodes:\",activatednodesITR)\n",
    "#     print(\"Communities:\",communitiesITR)\n",
    "\n",
    "\n",
    "\n",
    "#     print(\"hello\");\n",
    "\n",
    "    plt.xlabel(\"Percentage\")\n",
    "    plt.ylabel(\"Activated nodes\")\n",
    "    plt.title(\"Activated nodes\")\n",
    "    for i in range(len(activatednodesITR[0])):\n",
    "        plt.plot(perc,[pt[i] for pt in activatednodesITR],label = '%s'%Name[i],linestyle='%s'%linestylee[i],marker='%s'%markerss[i])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Percentage\")\n",
    "    plt.ylabel(\"No of community\")\n",
    "    plt.title(\"Communitites\")\n",
    "    for i in range(len(communitiesITR[0])):\n",
    "        plt.plot(perc,[pt[i] for pt in communitiesITR],label = '%s'%Name[i],linestyle='%s'%linestylee[i],marker='%s'%markerss[i])\n",
    "#     plt.plot(perc,totalcomm,label='Total community',linestyle='%s'%linestylee[-1],marker='%s'%markerss[-1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"communities:\",len(communitiesITR[0]))\n",
    "    print(\"Time:\",len(timeITR[0]))\n",
    "#     print(\"Percentage:\",)\n",
    "    #plt.figure(figsize=(12,8))\n",
    "    plt.xlabel(\"Percentage\")\n",
    "    plt.ylabel(\"Execution time:\")\n",
    "    plt.title(\"Execution time\")\n",
    "    for i in range(len(timeITR[0])):\n",
    "        plt.plot(perc,[pt[i] for pt in timeITR],label = '%s'%Name[i],linestyle='%s'%linestylee[i],marker='%s'%markerss[i])\n",
    "#     plt.plot(perc,totalcomm,label='Total community',linestyle='%s'%linestylee[-1],marker='%s'%markerss[-1])\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qazMNQdav3U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "JbPpwt2Aav3U"
   },
   "outputs": [],
   "source": [
    "def IC(graph, seed_set, threshold=0.5):\n",
    "    activated_nodes = set(seed_set)\n",
    "    new_nodes = set(seed_set)\n",
    "\n",
    "    while new_nodes:\n",
    "        current_nodes = list(new_nodes)\n",
    "        new_nodes = set()\n",
    "\n",
    "        for node in current_nodes:\n",
    "            neighbors = set(graph.neighbors(node)) - activated_nodes\n",
    "            for neighbor in neighbors:\n",
    "                edge_weight = graph[node][neighbor]['weight']\n",
    "                if edge_weight >= threshold:\n",
    "                    new_nodes.add(neighbor)\n",
    "                    activated_nodes.add(neighbor)\n",
    "\n",
    "    return activated_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "qODJ2EQKav3U"
   },
   "outputs": [],
   "source": [
    "def findcommunity(G,seedset,budget, comm,algoname,time):\n",
    "\n",
    "    print (\"Communities formed: \",len(comm))\n",
    "    activated = IC(G, seedset)\n",
    "    noofcommunity=0\n",
    "    for com in comm:\n",
    "        if any(x in activated for x in com):\n",
    "            noofcommunity=noofcommunity+1\n",
    "    print(\"Community we got:\",noofcommunity)\n",
    "\n",
    "    upperBound_dict = {\n",
    "        'Name':algoname,\n",
    "        'budget': budget,\n",
    "#         'Total communitites':len(comm),\n",
    "        'number of communities':noofcommunity,\n",
    "        'length of activated nodes': len(activated),\n",
    "        'time': time\n",
    "\n",
    "        # 'length of communities': len(community_df['Unnamed: 1'].unique())\n",
    "    }\n",
    "    print(upperBound_dict)\n",
    "    return upperBound_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "y9bkDX6-av3V"
   },
   "outputs": [],
   "source": [
    "def Greedy(G, budget, candidate_nodes):\n",
    "    Dict = {}\n",
    "    mySet1 = []\n",
    "    cost = 0\n",
    "\n",
    "    while cost < budget:\n",
    "        for v in (set(candidate_nodes) - set(mySet1)):\n",
    "            if cost + G.nodes[v]['cost'] > budget:\n",
    "                continue\n",
    "            mySet1.append(v)\n",
    "            a = IC(G, mySet1)\n",
    "            Dict[v] = len(a)  # influence as value and current node as key\n",
    "            mySet1.remove(v)  # remove current node from mySet for rest nodes to go for IC\n",
    "\n",
    "        if not Dict:\n",
    "            break\n",
    "\n",
    "        Keymax = max(zip(Dict.values(), Dict.keys()))[1]  # finding node with max influence\n",
    "        mySet1.append(Keymax)\n",
    "        cost = cost + G.nodes[Keymax]['cost']\n",
    "        Dict.clear()\n",
    "\n",
    "    return set(mySet1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "G3dbtXr8av3W"
   },
   "outputs": [],
   "source": [
    "def cgim_IC(graph,budget,candidate_nodes):\n",
    "    S = set()\n",
    "    total_cost = 0\n",
    "    influence = 0\n",
    "    cnt=0\n",
    "    influenced_nodes=[]\n",
    "    while total_cost < budget:\n",
    "        max_marginal_gain = -1\n",
    "        chosen_node = None\n",
    "        cnt+=1\n",
    "        temp_inodes=[]\n",
    "\n",
    "        for node in candidate_nodes:\n",
    "            if (total_cost + graph.nodes[node].get('cost', 0)) < budget:\n",
    "                S.add(node)\n",
    "                temp_inodes = IC(graph, S)\n",
    "                temp_influence=len(temp_inodes)\n",
    "                S.remove(node)\n",
    "                marginal_gain = temp_influence - influence\n",
    "\n",
    "                if marginal_gain > max_marginal_gain:\n",
    "                    influenced_nodes=temp_inodes\n",
    "                    chosen_node = node\n",
    "                    max_marginal_gain = marginal_gain\n",
    "\n",
    "        if chosen_node==None:\n",
    "          break\n",
    "\n",
    "        # Removing influenced Node\n",
    "        for node in influenced_nodes:\n",
    "          if node in candidate_nodes:\n",
    "            candidate_nodes.remove(node)\n",
    "\n",
    "        S.add(chosen_node)\n",
    "        total_cost = total_cost + graph.nodes[chosen_node].get('cost', 0)\n",
    "        influence = influence + max_marginal_gain\n",
    "\n",
    "        #candidate_nodes.remove(chosen_node)\n",
    "\n",
    "    return S\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "8g0bD_8WSUg7"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def celf(graph, budget):\n",
    "    # Initialize variables\n",
    "    nodes = []\n",
    "    heap = []\n",
    "    marg_gains = {}\n",
    "    total_cost = 0\n",
    "\n",
    "    # Calculate the marginal gain for each node\n",
    "    for node in graph.nodes():\n",
    "        # Run Monte Carlo simulations to estimate the influence of each node\n",
    "        sim_res = linear_Threshold(graph, nodes + [node])\n",
    "        marg_gains[node] = len(sim_res) - len(nodes)\n",
    "        # Add the node to the heap with its marginal gain as key\n",
    "        heapq.heappush(heap, (-marg_gains[node], node))\n",
    "\n",
    "    # Select nodes until the budget is exhausted\n",
    "    while total_cost < budget and heap:\n",
    "        # Get the node with the highest marginal gain\n",
    "        _, node = heapq.heappop(heap)\n",
    "\n",
    "        # Check if adding the node exceeds the budget\n",
    "        if total_cost + graph.nodes[node]['cost'] <= budget:\n",
    "            # Recalculate the marginal gain of the selected node\n",
    "            sim_res = linear_Threshold(graph, nodes + [node])\n",
    "            marg_gains[node] = len(sim_res) - len(nodes)\n",
    "            # Add the node to the list of selected nodes\n",
    "            nodes.append(node)\n",
    "            # Update the total cost\n",
    "            total_cost += graph.nodes[node]['cost']\n",
    "            # Update the heap with the new marginal gains for neighbors\n",
    "            for n in graph.neighbors(node):\n",
    "                if n not in nodes:\n",
    "                    heapq.heappush(heap, (-marg_gains[n], n))\n",
    "\n",
    "    return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "ZlWtdYSuav3W"
   },
   "outputs": [],
   "source": [
    "# from diffusion_dynamic import IndependentCascade\n",
    "\n",
    "\n",
    "# from diffusion_dynamic import IndependentCascade\n",
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "import heapq\n",
    "\n",
    "population_size = 50\n",
    "mutation_rate = 0.3\n",
    "crossover_rate = 1.0\n",
    "elite_number = 2\n",
    "mc = 1\n",
    "\n",
    "def roulette_wheel_selection(population, fitnesses):\n",
    "    total_fitness = sum(fitnesses)\n",
    "    pick = random.uniform(0, total_fitness)\n",
    "    current = 0\n",
    "    for i in range(len(population)):\n",
    "        current += fitnesses[i]\n",
    "        if current > pick:\n",
    "            return population[i]\n",
    "\n",
    "def ordered_crossover(parent1, parent2):\n",
    "    size = len(parent1)\n",
    "    start, end = sorted(random.sample(range(size), 2))\n",
    "    \n",
    "    child = [None] * size\n",
    "    child[start:end] = parent1[start:end]\n",
    "    for gene in parent2:\n",
    "        if gene not in child:\n",
    "            for i in range(size):\n",
    "                if child[i] is None:\n",
    "                    child[i] = gene\n",
    "                    break\n",
    "    return child\n",
    "\n",
    "def mutation(child, V):\n",
    "    m = random.sample(sorted(set(V).difference(child)), len(child))\n",
    "    for i in range(len(child)):\n",
    "        r = random.random()\n",
    "        if r < mutation_rate:\n",
    "            child[i] = m[i]\n",
    "    return child\n",
    "            \n",
    "def fitness(child, diffuse, t0, duration):\n",
    "    f = diffuse.diffuse_mc(child, mc=mc, t0=t0, duration=duration)\n",
    "    return f\n",
    "\n",
    "def find_max_indices(arr, k):\n",
    "    max_values = heapq.nlargest(k, arr)\n",
    "    indices = [i for i, num in enumerate(arr) if num in max_values]\n",
    "    return indices\n",
    "    \n",
    "\n",
    "def genetic_algorithm(graph, diffuse, k, t0, duration):\n",
    "    V = sorted(set(graph.nodes()))\n",
    "    print(type(V))\n",
    "    population = [random.sample(V, k) for _ in range(population_size)]\n",
    "    generations = population_size * k\n",
    "    print(\"evolution start, total generations: \" + str(generations))\n",
    "    for gen in range(generations):\n",
    "#         print(\"generation: \" + str(gen))\n",
    "        #fitness\n",
    "        fitnesses = [fitness(child, diffuse, t0, duration) for child in population]\n",
    "        \n",
    "        #select\n",
    "        selected = [roulette_wheel_selection(population, fitnesses) for _ in range(population_size)]\n",
    "        \n",
    "        #crossover\n",
    "        offspring = []\n",
    "        for i in range(0, population_size, 2):\n",
    "            parent1, parent2 = selected[i], selected[i + 1]\n",
    "            if (random.random() < crossover_rate):\n",
    "                child1, child2 = ordered_crossover(parent1, parent2), ordered_crossover(parent2, parent1)\n",
    "            else:\n",
    "                child1, child2 = parent1, parent2\n",
    "            offspring += [child1, child2]\n",
    "        \n",
    "        #mutation\n",
    "        mutated_offspring = [mutation(child, V) for child in offspring]\n",
    "        \n",
    "        #elitism\n",
    "        indices = find_max_indices(fitnesses, elite_number)\n",
    "        for i in indices:\n",
    "            mutated_offspring[i] = population[i]\n",
    "        #update population\n",
    "        population = mutated_offspring\n",
    "    \n",
    "    #best seed\n",
    "    fitnesses = [fitness(child, diffuse, t0, duration) for child in population]\n",
    "    indices = find_max_indices(fitnesses, 1)\n",
    "    return population[indices[0]], fitnesses[indices[0]],population,fitnesses\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "def diffuse_dynamic0(graph, edge_probs, edge_count_map_list, reverse_edge_idx, act_nodes, t0, duration):\n",
    "    new_act_nodes = set(act_nodes)\n",
    "    live_edges = np.zeros([1, len(edge_probs)], dtype = np.int8)\n",
    "    t1 = min(len(edge_count_map_list), t0 + duration)\n",
    "    for t in range(t0, t1):\n",
    "        edge_count_map = edge_count_map_list[t]\n",
    "        inc_nodes = set()\n",
    "        for idx in edge_count_map.keys():\n",
    "            edge = reverse_edge_idx[idx]\n",
    "            count = edge_count_map[idx]\n",
    "            if edge[0] not in new_act_nodes:\n",
    "                continue\n",
    "            r = np.random.random()\n",
    "            if r > edge_probs[edge] ** count:\n",
    "                live_edges[0][idx] = 1\n",
    "                inc_nodes.add(edge[1])\n",
    "        new_act_nodes.update(inc_nodes)\n",
    "    return len(new_act_nodes)\n",
    "\n",
    "class IndependentCascade(object):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        graph: networkx.DiGraph()\n",
    "        edge_idx: {(u, v): i for i, (u, v) in enumerate(graph.edges())}\n",
    "        temporal[t][i]: At time t, the edge (u, v) is effctive whose edge_idx == temporal[t][i]. \n",
    "        Given t, if there are many i to make temporal[t][i] the same, then the probability increases in power for edge (u, v).\n",
    "        If temporal == None, then the network is static.\n",
    "    \"\"\"\n",
    "    def __init__(self, graph, edge_idx, temporal = None):\n",
    "        self.graph = graph\n",
    "        self.sampled_graph = graph.copy()\n",
    "        self.edge_idx = {(u, v): i for i, (u, v) in enumerate(self.graph.edges())}\n",
    "        self.edge_probs = {(u, v): d['prob'] for u, v, d in graph.edges().data()}\n",
    "        self.temporal = temporal\n",
    "        self.reverse_edge_idx = {i: e for e, i in self.edge_idx.items()}\n",
    "        self.prob_matrix = [self.graph.edges[self.reverse_edge_idx[i][0], self.reverse_edge_idx[i][1]]['prob'] for i in sorted(self.reverse_edge_idx.keys())]\n",
    "        self.edge_count_map_list = []\n",
    "        for t in range(len(temporal)):\n",
    "            edge_idx_list = temporal[t]\n",
    "            edge_count_map = {}\n",
    "            for idx in edge_idx_list:\n",
    "                if idx in edge_count_map:\n",
    "                    edge_count_map[idx] += 1\n",
    "                else:\n",
    "                    edge_count_map[idx] = 1\n",
    "            self.edge_count_map_list.append(edge_count_map)\n",
    "    \n",
    "    def sample_live_graph_mc(self, act_nodes, mc):\n",
    "        edge_probs = {(u, v): d['prob'] for u, v, d in self.graph.edges().data()}\n",
    "        probs = np.random.uniform(size=(mc, len(edge_probs)))\n",
    "        self.sampled_graphs = []\n",
    "        for p in probs:\n",
    "            live_edges = np.array([p > self.prob_matrix]).astype(np.int8)\n",
    "            self.sampled_graphs.append(live_edges)\n",
    "    \n",
    "    def diffuse_dynamic(self, act_nodes, t0, duration):\n",
    "        return diffuse_dynamic0(self.graph, self.edge_probs, self.edge_count_map_list, self.reverse_edge_idx, act_nodes, t0, duration)\n",
    "        \n",
    "    def sample_live_graph(self, mcount):\n",
    "        removed_edges_idx = np.where(self.sampled_graphs[mcount] == 0)[1].tolist()\n",
    "        removed_edges = [self.reverse_edge_idx[i] for i in removed_edges_idx]\n",
    "        Gp = self.graph.copy()\n",
    "        Gp.remove_edges_from(removed_edges)\n",
    "        self.sampled_graph = Gp\n",
    "\n",
    "    def diffusion_iter(self, act_nodes):\n",
    "        new_act_nodes = set(act_nodes)\n",
    "        for node in act_nodes:\n",
    "            for node2 in nx.algorithms.bfs_tree(self.sampled_graph, node).nodes():\n",
    "                new_act_nodes.add(node2)\n",
    "        for node in new_act_nodes:\n",
    "            self.sampled_graph.nodes[node]['is_active'] = True\n",
    "\n",
    "    def diffuse(self, act_nodes, mcount):\n",
    "        self.sample_live_graph(mcount)\n",
    "        nx.set_node_attributes(self.sampled_graph, False, name='is_active')\n",
    "\n",
    "        for node in act_nodes:\n",
    "            self.sampled_graph.nodes[node]['is_active'] = True\n",
    "        \n",
    "        self.diffusion_iter(act_nodes)\n",
    "        active_nodes = [n for n, v in self.sampled_graph.nodes.data() if v['is_active']]\n",
    "        self.graph.total_activated_nodes.append(len(active_nodes))\n",
    "\n",
    "    def diffuse_mc(self, act_nodes, mc=10, t0=0, duration=30):\n",
    "        if self.temporal == None:\n",
    "            self.sample_live_graph_mc(act_nodes, mc)\n",
    "            self.graph.total_activated_nodes = []\n",
    "            for i in range(mc):\n",
    "                self.diffuse(act_nodes, i)\n",
    "            return sum(self.graph.total_activated_nodes) / float(mc)\n",
    "        else:\n",
    "            count_list = [self.diffuse_dynamic(act_nodes, t0, duration) for _ in range(mc)]\n",
    "            return sum(count_list) / float(mc)\n",
    "\n",
    "    def shapely_iter(self, act_nodes):\n",
    "        nx.set_node_attributes(self.sampled_graph, False, name='is_active')\n",
    "\n",
    "        for node in act_nodes:\n",
    "            self.sampled_graph.nodes[node]['is_active'] = True\n",
    "\n",
    "        self.diffusion_iter(act_nodes)\n",
    "        active_nodes = [n for n, v in self.sampled_graph.nodes.data() if v['is_active']]\n",
    "        return active_nodes\n",
    "\n",
    "    def shapely_diffuse(self, nodes, mc=10, t0=0, duration = 30):\n",
    "        self.sample_live_graph_mc(nodes, mc, t0, duration)\n",
    "        for node in nodes:\n",
    "            self.graph.nodes[node]['tmp'] = 0\n",
    "\n",
    "        for c in tqdm(range(mc), desc='Shapely Monte Carlo', leave=False):\n",
    "            self.sample_live_graph(c)\n",
    "            active_nodes_with = []\n",
    "            active_nodes_without = []\n",
    "            for i in tqdm(range(len(nodes)), desc='Shapely Iter', leave=False):\n",
    "                if i in active_nodes_with:\n",
    "                    self.graph.nodes[node]['tmp'] = 0\n",
    "                    continue\n",
    "                active_nodes_with = self.shapely_iter(nodes[:i+1])\n",
    "                active_nodes_without = self.shapely_iter(nodes[:i])\n",
    "                self.graph.nodes[nodes[i]]['tmp'] +=  len(active_nodes_with) - len(active_nodes_without)\n",
    "\n",
    "        for i in range(len(nodes)):\n",
    "            self.graph.nodes[node]['tmp'] /= float(mc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def greedy(graph, diffuse, k, t0, duration):\n",
    "# \tS = set()\n",
    "# \tA = set(graph.nodes)\n",
    "# \twhile len(S) < k:\n",
    "# \t\tnode_diffusion = {}\n",
    "# \t\tfor node in A:\n",
    "# \t\t\tS.add(node)\n",
    "# \t\t\tnode_diffusion[node] = diffuse.diffuse_mc(S, mc=1, t0=t0, duration=duration)\n",
    "# \t\t\tS.remove(node)\n",
    "# \t\tmax_node = max(node_diffusion.items(), key=lambda x: x[1])[0]\n",
    "# \t\tS.add(max_node)\n",
    "# \t\tA.remove(max_node)\n",
    "# \treturn S\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def runDIG(InputG,budget):\n",
    "\n",
    "#     G = nx.DiGraph()\n",
    "#     day = 86400\n",
    "#     # f = open(\"dataset/CollegeMsg.txt\");\n",
    "#     f = open(\"dataset/dolphin_edge_list.txt\");\n",
    "    G=nx.DiGraph()\n",
    "#     lines = [l.split() for l in f.readlines() if l.strip()]\n",
    "    lines=InputG.edges()\n",
    "#     print(lines)\n",
    "    for i in lines:\n",
    "#         print(int(i[0]),int(i[1]))\n",
    "        G.add_edge(int(i[0]), int(i[1]), prob = 0.9)\n",
    "    edge_idx = {(u, v): i for i, (u, v) in enumerate(G.edges())}\n",
    "    temporal = []\n",
    "    for _ in range(1):\n",
    "        temporal.append([])\n",
    "#     print(\"temporal:\",len(temporal))\n",
    "    for i in lines:\n",
    "        idx = edge_idx[(int(i[0]), int(i[1]))]\n",
    "        temporal[0].append(idx)\n",
    "#     print(temporal)\n",
    "    sorted_temporal = [sorted(i) for i in temporal]\n",
    "    # print(\"sorted_temporal:\",sorted_temporal)\n",
    "    diffuse = IndependentCascade(G, edge_idx, temporal = temporal)\n",
    "    k = 5\n",
    "    t0 =0\n",
    "    duration = 30\n",
    "    \n",
    "    S, expand, pop,fitnesses = genetic_algorithm(G, diffuse, k, t0, duration)\n",
    "    seedset=findSeedDIG(InputG,budget,pop,fitnesses)\n",
    "   \n",
    "    return seedset\n",
    "#     print(S)\n",
    "#     print(expand)\n",
    "\n",
    "\n",
    "    \n",
    "def find_max_indicess(arr, k):\n",
    "    max_values = heapq.nlargest(k, arr)\n",
    "    indices = [i for i, num in enumerate(arr) if num in max_values]\n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_seed_set(G,budget,populations, fitness_list):\n",
    "    # Combine populations and fitness into a single list of tuples\n",
    "    # Each tuple is (fitness, population) and sorted by fitness in descending order\n",
    "    pop_fit_pairs = sorted(zip(fitness_list, populations), key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    seed_set = []\n",
    "    total_cost = 0\n",
    "    \n",
    "    # Iterate through populations starting from the one with highest fitness\n",
    "    for fitness, population in pop_fit_pairs:\n",
    "        # Sort the population by cost to add the most affordable nodes first\n",
    "        population = sorted(population, key=lambda node: G.nodes[node]['cost'])\n",
    "        \n",
    "        # Add nodes from this population while under budget\n",
    "        for node in population:\n",
    "            cost = G.nodes[node]['cost']\n",
    "            if total_cost + cost <= budget:\n",
    "                seed_set.append(node)\n",
    "                total_cost += cost\n",
    "            else:\n",
    "                break  # Break the loop if adding more nodes exceeds the budget\n",
    "#     print(\"Seed set:\",seed_set)\n",
    "    return seed_set\n",
    "\n",
    "\n",
    "\n",
    "def findSeedDIG(InputG,budget,pop,fitnesses):\n",
    "#     print(\"-----------\")\n",
    "    seedSetDIG=select_seed_set(InputG,budget,pop,fitnesses)\n",
    "#     print(pop,fitnesses)\n",
    "#     print(\"seed set:\",seedSetDIG)\n",
    "    return seedSetDIG\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def DIG(G,budget):\n",
    "#     print(G)\n",
    "    seedset=runDIG(G,budget)\n",
    "    return seedset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "BridgePreviousSeedsetManually=[]\n",
    "ratiolevel0bridge=10\n",
    "def BridgeAlgo(G,budget,comm_set_final,Previous_seed_set,G_new):\n",
    "#     print(\"In bridge algorithm...\")\n",
    "        \n",
    "#     community_weight=WeightAssign(G,k,comm_set_final,Previous_seed_set)\n",
    "\n",
    "#     comm_set_final=[[1,2,3,4,5,6,7,8],[9,10,11,12],[13,14,15,16,17,18,19,20],[21,22,23,24,25]]\n",
    "\n",
    "    community_weight=[]    \n",
    "    seedSet,Bridge_seed_nodes,Core_seedset=findSeetset(G,budget,community_weight,Previous_seed_set,G_new,comm_set_final)\n",
    "    BridgePreviousSeedsetManually=seedSet\n",
    "    return seedSet\n",
    "\n",
    "# BridgeDic={}\n",
    "def GreedyDivv(G,k, comm,Bridge_Graph):\n",
    "    Dict={}\n",
    "    mySet1=[]\n",
    "    V=set()\n",
    "#     print(comm)\n",
    "    for e in Bridge_Graph.edges():\n",
    "        V.add(e[0])\n",
    "#         print(V)\n",
    "    mySet1.clear()\n",
    "#     print(\"Budget:\",k)\n",
    "    totalcost=0\n",
    "    if len(V)==0:\n",
    "#         print(\"Hello\")\n",
    "        return [],totalcost\n",
    "#     kk=ass\n",
    "    while totalcost<=k:\n",
    "#         print(\"In while loop\")\n",
    "        for v in (V-set(mySet1)):\n",
    "#             print(\"Cost:\",G.nodes[v]['cost'])\n",
    "            if totalcost+G.nodes[v]['cost']>k:\n",
    "                V.remove(v)\n",
    "#                 print(V)\n",
    "                continue;\n",
    "            mySet1.append(v)\n",
    "            a=compute_Phi(G,mySet1,comm,1)\n",
    "            Dict[v]=a#influence as value and current node as key\n",
    "            mySet1.remove(v)#remove crrent node from mySet for rest nodes to go for IC\n",
    "        if(len(Dict)!=0):\n",
    "            Keymax = max(zip(Dict.values(), Dict.keys()))[1]# finding node with max influence\n",
    "            Dict.clear()\n",
    "            mySet1.append(Keymax)\n",
    "            totalcost+=G.nodes[Keymax]['cost']\n",
    "#             print(\"Bridge seed set:\",mySet1)\n",
    "        else:\n",
    "            break\n",
    "    return list(mySet1),totalcost\n",
    "\n",
    "def findSeetset(G,budget,community_weight,Previous_seed_set,G_new,comm_set_final,ratiolevel0bridge=10):\n",
    "#     Display_Communities(G,comm_set_final)\n",
    "    seeds=Previous_seed_set\n",
    "    degrees = G_new.out_degree()  # Compute degrees of all nodes\n",
    "\n",
    "    sorted_nodes = sorted(degrees, key=lambda x: x[1], reverse=True)  # Sort nodes based on degree in descending order\n",
    "    top_nodes = [node for node, _ in sorted_nodes[:50]]\n",
    "    seeds=Previous_seed_set+top_nodes\n",
    "    temp_cand=list(set(seeds))\n",
    "    \n",
    "#     bridge_size,core_size=SetCore_Bridge_size(ratiolevel0bridge,ksize)\n",
    "    bridge_size=round((ratiolevel0bridge/100)*budget)\n",
    "    core_size=budget-bridge_size\n",
    "    \n",
    "#     print(\"coresize:\",core_size,\" bridge size:\",bridge_size)\n",
    "    \n",
    "    Bridge_seed_nodes,Bridgecost=find_Bridge_function(G,bridge_size,community_weight,Previous_seed_set,G_new,comm_set_final)\n",
    "    \n",
    "    core_size=core_size+(bridge_size-Bridgecost)\n",
    "#     print(\"coresize:\",core_size,\" bridge size:\",bridge_size)\n",
    "#     print(\"Bridge seed nodes---------\",Bridge_seed_nodes)\n",
    "    \n",
    "    Core_seedset=GreedyDivv_find_core_nodes_function(G,core_size,comm_set_final,temp_cand,Bridge_seed_nodes)\n",
    "#     print(\"Bridge seed nodes---------\",Bridge_seed_nodes)\n",
    "#     print(\"Core seed nodes---------\",Core_seedset)\n",
    "    \n",
    "    seedset=Bridge_seed_nodes+Core_seedset\n",
    "#     print(\"Overall seed nodes---------\",len(seedset),type(seedset),seedset)\n",
    "\n",
    "    \n",
    "    return seedset,Bridge_seed_nodes,Core_seedset\n",
    "    \n",
    "\n",
    "def find_Bridge_function(G,bridge_size,community_weight,Previous_seed_set,G_new,comm_set_final):\n",
    "#     print(\"--------------For bridge nodes------------------------------\")\n",
    "    BridgeGraph=nx.DiGraph()\n",
    "    \n",
    "    for edge in G.edges():\n",
    "        s=edge[0]\n",
    "        t=edge[1]\n",
    "        for com in comm_set_final:\n",
    "            if s in com:\n",
    "                if t not in com:\n",
    "                    BridgeGraph.add_edge(s,t,weight=G[s][t]['weight'])\n",
    "                break;\n",
    "#     print(\"Bridge graph:\",G,BridgeGraph)\n",
    "    BridgeSeedset,Bridgecost=GreedyDivv(G,bridge_size,comm_set_final,BridgeGraph)\n",
    "    return BridgeSeedset,Bridgecost\n",
    "    \n",
    "\n",
    "def SetCore_Bridge_size(ratiolevel0bridge,ksize):\n",
    "    bridge_size=round((ratiolevel0bridge/100)*ksize)\n",
    "    core_size=ksize-bridge_size  \n",
    "\n",
    "    # CoreDic={}\n",
    "\n",
    "def GreedyDivv_find_core_nodes_function(G,k, comm,temp_cand,Bridge_seed_nodes):\n",
    "#     Display_Communities(G,comm)\n",
    "#     print(\"In core---------------\")\n",
    "    Dict={}\n",
    "    mySet1=set()\n",
    "    temp=list(set(temp_cand)-set(Bridge_seed_nodes))\n",
    "    V=set(temp)\n",
    "    mySet1.clear()\n",
    "    totalcost=0\n",
    "    while totalcost<=k:\n",
    "        for v in (V-mySet1):\n",
    "            if totalcost+G.nodes[v]['cost']>k:\n",
    "                V.remove(v)\n",
    "                continue;\n",
    "            mySet1.add(v)\n",
    "            a=compute_Phi(G,list(mySet1),comm,1)\n",
    "            Dict[v]=a#influence as value and current node as key\n",
    "            mySet1.remove(v)#remove crrent node from mySet for rest nodes to go for IC\n",
    "        if(len(Dict)!=0):\n",
    "            Keymax = max(zip(Dict.values(), Dict.keys()))[1]# finding node with max influence\n",
    "            Dict.clear()\n",
    "            mySet1.add(Keymax)\n",
    "            totalcost+=G.nodes[Keymax]['cost']\n",
    "        else:\n",
    "            break;\n",
    "#         print(\"Core nodes:\",mySet1)\n",
    "    return list(mySet1)\n",
    "\n",
    "def compute_Phi(G, S, communities, k):\n",
    "    if len(S)<1:\n",
    "        return 0\n",
    "    lambda_G = 0.5\n",
    "    \n",
    "    v_length = G.number_of_nodes()\n",
    "    diversity_V = communityDiversityFunction(G, list(G.nodes), communities)\n",
    "\n",
    "    IC_S = linear_Threshold(G, S)\n",
    "    activated_set_S_length = len(IC_S) \n",
    "    diversity_activated_set_S = communityDiversityFunction(G, IC_S, communities)\n",
    "    phi_S = ((1 - lambda_G)* (activated_set_S_length/v_length)) + (lambda_G * (diversity_activated_set_S/diversity_V))\n",
    "    return phi_S\n",
    "    \n",
    "    \n",
    "def communityDiversityFunction(G, S, communities):\n",
    "\n",
    "    activated = linear_Threshold(G, S)\n",
    "    noofcommunity=0\n",
    "    for com in communities:\n",
    "#         print(com)\n",
    "        if any(x in activated for x in com):\n",
    "            noofcommunity=noofcommunity+1\n",
    "#     print(\"Community we got:\",noofcommunity)\n",
    "    \n",
    "    \n",
    "    return noofcommunity\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "yRKR7OkRzwFX"
   },
   "outputs": [],
   "source": [
    "def GreedyDiv(G, budget, candidate_nodes):\n",
    "    mySet1 = set()\n",
    "    total_cost = 0\n",
    "\n",
    "    while total_cost < budget:\n",
    "        best_node = None\n",
    "        max_marginal_gain = -1\n",
    "\n",
    "        for v in (set(candidate_nodes) - mySet1):\n",
    "            mySet1.add(v)\n",
    "            influence_after_addition = len(linear_Threshold(G, mySet1))\n",
    "            mySet1.remove(v)\n",
    "\n",
    "            cost = G.nodes[v].get('cost', 0)\n",
    "\n",
    "            # Calculate the marginal gain in influence\n",
    "            marginal_gain = influence_after_addition - len(linear_Threshold(G, mySet1))\n",
    "\n",
    "            # Check if the node is a valid candidate and adding it doesn't violate the budget constraint\n",
    "            if v in candidate_nodes and total_cost + cost < budget and marginal_gain > max_marginal_gain:\n",
    "                best_node = v\n",
    "                max_marginal_gain = marginal_gain\n",
    "\n",
    "        if best_node is not None:\n",
    "            mySet1.add(best_node)\n",
    "            total_cost += G.nodes[best_node].get('cost', 0)\n",
    "        else:\n",
    "            # No valid candidate node found that satisfies the budget constraint\n",
    "            break\n",
    "\n",
    "    return mySet1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "mzXMZi6jDRBO"
   },
   "outputs": [],
   "source": [
    "def remove_neighbors(graph, node, distance, candidate_nodes):\n",
    "    if distance == 0:\n",
    "        return\n",
    "\n",
    "    neighbors_to_remove = set(graph.neighbors(node))\n",
    "    candidate_nodes.difference_update(neighbors_to_remove)\n",
    "\n",
    "    for neighbor in neighbors_to_remove:\n",
    "        remove_neighbors(graph, neighbor, distance - 1, candidate_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "wAOidZHNzpFN"
   },
   "outputs": [],
   "source": [
    "def DBIM(graph, community_nodes, budget, candidate_nodes):\n",
    "    S = set()\n",
    "    total_cost = 0\n",
    "    influence = 0\n",
    "    influenced_nodes=[]\n",
    "    while total_cost < budget:\n",
    "        max_marginal_gain = -1\n",
    "        chosen_node = None\n",
    "        temp_inodes=[]\n",
    "\n",
    "        for node in candidate_nodes:\n",
    "            if (total_cost + graph.nodes[node].get('cost', 0)) < budget:\n",
    "                S.add(node)\n",
    "                temp_influence,temp_inodes = compute_Phi_dbim(graph, S, community_nodes)\n",
    "                S.remove(node)\n",
    "                marginal_gain = temp_influence - influence\n",
    "\n",
    "                if marginal_gain > max_marginal_gain:\n",
    "                    influenced_nodes=temp_inodes\n",
    "                    chosen_node = node\n",
    "                    max_marginal_gain = marginal_gain\n",
    "\n",
    "        if chosen_node==None:\n",
    "          break\n",
    "\n",
    "        # Remove neighbors of the chosen node from candidate_nodes\n",
    "        neighbors_to_remove = set(graph.neighbors(chosen_node))\n",
    "        candidate_nodes -= neighbors_to_remove\n",
    "\n",
    "        # Removing influenced Node\n",
    "        for node in influenced_nodes:\n",
    "          if node in candidate_nodes:\n",
    "            candidate_nodes.remove(node)\n",
    "\n",
    "        S.add(chosen_node)\n",
    "        total_cost = total_cost + graph.nodes[chosen_node].get('cost', 0)\n",
    "        influence = influence + max_marginal_gain\n",
    "\n",
    "        #candidate_nodes.remove(chosen_node)\n",
    "\n",
    "\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "ZS1H5Re1av3X"
   },
   "outputs": [],
   "source": [
    "def DBIM_1(graph, community_nodes , budget, candidate_nodes):\n",
    "    S = set()\n",
    "    total_cost = 0\n",
    "    influence = 0\n",
    "\n",
    "    influence_increase = defaultdict(int)\n",
    "    candidate_nodes = list(candidate_nodes)\n",
    "\n",
    "    while total_cost < budget:\n",
    "        for node in candidate_nodes:\n",
    "            if (total_cost + graph.nodes[node]['cost']) > budget:\n",
    "                candidate_nodes.remove(node)\n",
    "                continue\n",
    "\n",
    "            temp_seed_set = S.union({node})\n",
    "            temp_influence = compute_Phi(graph, temp_seed_set, community_nodes)\n",
    "            influence_increase[node] = temp_influence - influence\n",
    "\n",
    "        chosen_node = max(candidate_nodes, key=lambda node: influence_increase.get(node, 0), default=None)\n",
    "\n",
    "        if not chosen_node:\n",
    "            break\n",
    "\n",
    "        #chosen_node = sorted_candidates[0]\n",
    "        S.add(chosen_node)\n",
    "        total_cost = total_cost+ graph.nodes[chosen_node]['cost']\n",
    "        influence = influence+influence_increase[chosen_node]\n",
    "        influence_increase.clear()\n",
    "        candidate_nodes.remove(chosen_node)\n",
    "\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcHIMIOMav3X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OI_IGCYiJC-y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "hL9Tcj7hav3Y"
   },
   "outputs": [],
   "source": [
    "def PAGERANK(G,budget):\n",
    "    pagerank = sorted(nx.pagerank(G).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    pagerank_seed = set()\n",
    "    totalcost=0\n",
    "    for node,value in pagerank:\n",
    "\n",
    "        if totalcost+G.nodes[node]['cost']>budget:\n",
    "            continue\n",
    "\n",
    "        pagerank_seed.add(node)\n",
    "        totalcost += G.nodes[node]['cost']\n",
    "    return pagerank_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "gNTc9xADav3Y"
   },
   "outputs": [],
   "source": [
    "def DEGREE(G,budget):\n",
    "\n",
    "    degree = sorted(G.degree(), key=lambda x: x[1], reverse=True)\n",
    "    degree_seed = set()\n",
    "    totalcost=0\n",
    "    for node,value in degree:\n",
    "        if totalcost+G.nodes[node]['cost']>budget:\n",
    "            continue\n",
    "\n",
    "        degree_seed.add(node)\n",
    "        totalcost += G.nodes[node]['cost']\n",
    "    return degree_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "QsjhXA5-av3Z"
   },
   "outputs": [],
   "source": [
    "def findresult(G,comm_set_final,Delta,seedsetlist,budget):\n",
    "\n",
    "\n",
    "    #new_nodes=set(Delta.nodes())\n",
    "    #budget=setBudget(G,new_nodes)\n",
    "\n",
    "\n",
    "    sorted_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)\n",
    "    half_nodes = sorted_nodes[:len(G.nodes())//2]\n",
    "    new_nodes = set(node for node, _ in half_nodes)\n",
    "\n",
    "\n",
    "\n",
    "    #DBIM\n",
    "    st=time.time()\n",
    "    DBIMseedSet = DBIM(G,comm_set_final,budget, seedsetlist['DBIM']|new_nodes )\n",
    "    DBIMtime=time.time()-st\n",
    "    print('\\n\\n--------------- DBIM -------------------')\n",
    "    DBIMresult=findcommunity(G,DBIMseedSet,budget, comm_set_final,'DBIM',DBIMtime)\n",
    "    seedsetlist['DBIM']=DBIMseedSet\n",
    "    print(DBIMseedSet)\n",
    "\n",
    "    # #Greedy\n",
    "    st=time.time()\n",
    "    Greedyseedset=GreedyDiv(G,budget,seedsetlist['GREEDY']|new_nodes)\n",
    "    Greedytime=time.time()-st\n",
    "    print('\\n\\n--------------- Greedy -------------------')\n",
    "    Greedyresult=findcommunity(G,Greedyseedset,budget, comm_set_final,'Greedy',Greedytime)\n",
    "    seedsetlist['GREEDY']=Greedyseedset\n",
    "    print(Greedyseedset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #CELF\n",
    "    st=time.time()\n",
    "    #CELFseedSet = celf(G,budget,seedsetlist['CELF']|new_nodes)\n",
    "    CELFseedSet = celf(G,budget)\n",
    "    CELFtime=time.time()-st\n",
    "    print('\\n\\n--------------- CELF -------------------')\n",
    "    CELFresult=findcommunity(G,CELFseedSet,budget, comm_set_final,'CELF',CELFtime)\n",
    "    seedsetlist['CELF']=CELFseedSet\n",
    "    print(CELFseedSet)\n",
    "\n",
    "    #CGIM_IC\n",
    "    st=time.time()\n",
    "    CGIM_ICseedSet = cgim_IC(G,budget,seedsetlist['CGIM_IC']|new_nodes)\n",
    "    CGIM_ICtime=time.time()-st\n",
    "    print('\\n\\n--------------- \"Cost-Greedy Influence Maximization -------------------')\n",
    "    CGIM_ICresult=findcommunity(G,CGIM_ICseedSet,budget, comm_set_final,'CGIM_IC',CGIM_ICtime)\n",
    "    seedsetlist['CGIM_IC']=CGIM_ICseedSet\n",
    "    print(CGIM_ICseedSet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #PAGERANK\n",
    "    st=time.time()\n",
    "    #pagerank_seed = [node for node, value in pagerank[0:k]]\n",
    "    pagerank_seed=PAGERANK(G,budget)\n",
    "    PRtime=time.time()-st\n",
    "    print('\\n\\n--------------- PAGERANK -------------------')\n",
    "    Pagerankresult=findcommunity(G,pagerank_seed,budget, comm_set_final,'PageRank',PRtime)\n",
    "    seedsetlist['PAGERANK']=pagerank_seed\n",
    "\n",
    "    #DEGREE\n",
    "    st=time.time()\n",
    "    #degree_seed = [node for node, value in degree[0:k]]\n",
    "    degree_seed =DEGREE(G,budget)\n",
    "    Dtime=time.time()-st\n",
    "    print('\\n\\n--------------- DEGREE -------------------')\n",
    "    Degreeresult=findcommunity(G,degree_seed,budget, comm_set_final,'Degree',Dtime)\n",
    "    seedsetlist['DEGREE']=degree_seed\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "     #CELF\n",
    "#     print('\\n\\n--------------- DIG -------------------')\n",
    "    st=time.time()\n",
    "    DIGseedSet = DIG(G,budget)\n",
    "    DIGtime=time.time()-st\n",
    "    print('\\n\\n--------------- DGA -------------------',DIGseedSet)\n",
    "    DGAresult=findcommunity(G,DIGseedSet,budget, comm_set_final,'DGA',DIGtime)\n",
    "    seedsetlist['DGA']=DIGseedSet\n",
    "    print(DIGseedSet)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('\\n\\n--------------- Bridge -------------------')\n",
    "    BBridgeSeedSet=[]\n",
    "    st=time.time()\n",
    "    BBridgeSeedSet = BridgeAlgo(G,budget,comm_set_final,BridgePreviousSeedsetManually,Delta)\n",
    "    endtime=time.time()-st\n",
    "    BridgeResult=findcommunity(G,BBridgeSeedSet,budget, comm_set_final,'Bridge',endtime)\n",
    "    seedsetlist['Bridge']=BBridgeSeedSet\n",
    "    \n",
    "    \n",
    "\n",
    "    resultt=[DBIMresult,CGIM_ICresult,Greedyresult,CELFresult,Degreeresult,Pagerankresult,DGAresult,BridgeResult]\n",
    "    #resultt=[CELFresult,Degreeresult,DBIMresult,CGIM_ICresult,Pagerankresult]\n",
    "\n",
    "    return resultt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDAx6ONUav3Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "D1sinODnav3a"
   },
   "outputs": [],
   "source": [
    "def makeExcel(result,itr,perc):\n",
    "    timeITR=[]\n",
    "    Name=[]\n",
    "    activatednodesITR=[]\n",
    "    ttt_itr=[]\n",
    "    for i in result:\n",
    "        for j in range(len(result[i])):\n",
    "            Name.append(result[i][j]['Name'])\n",
    "        break\n",
    "    for i in result:\n",
    "        active=[]\n",
    "        timee=[]\n",
    "        tt=[]\n",
    "        for j in range(len(result[i])):\n",
    "            active.append(result[i][j]['length of activated nodes'])\n",
    "            timee.append(result[i][j]['number of communities'])\n",
    "            tt.append(result[i][j]['time'])\n",
    "        activatednodesITR.append(active)\n",
    "        timeITR.append(timee)\n",
    "        ttt_itr.append(tt)\n",
    "    df=pd.DataFrame()\n",
    "    time=\"Community\"\n",
    "    for i in range(len(timeITR)):\n",
    "        timestr=time+\"_\"+str(itr)+\"_\"+str(perc[i])+\"%\"\n",
    "        df[timestr]=timeITR[i]\n",
    "#     print(\"After name\",df)\n",
    "    ICnodes=\"Activated_Nodes\"\n",
    "    for i in range(len(activatednodesITR)):\n",
    "        ICnodesstr=ICnodes+\"_\"+str(itr)+\"_\"+str(perc[i])+\"%\"\n",
    "        df[ICnodesstr]=activatednodesITR[i]\n",
    "\n",
    "    et=\"time\"\n",
    "    for i in range(len(ttt_itr)):\n",
    "        t=et+\"_\"+str(itr)+\"_\"+str(perc[i])+\"%\"\n",
    "        df[t]=ttt_itr[i]\n",
    "\n",
    "    df.insert(0,\"Name of Algorithm\",Name)\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "Idw5P3fMav3a"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_graphs(result, verbose=True):\n",
    "    itr = range(1, 11)  # Assuming itr is defined somewhere\n",
    "\n",
    "    algo_results = [(result.iloc[i, 0], color, result.iloc[range(i, result.shape[0], 6), 1:].to_numpy()) for i, color in enumerate(['blue', 'orange', 'green', 'red', 'purple', 'brown'])]\n",
    "\n",
    "    for i, suffix in zip(range(0, result.shape[1]-1, len(itr)),  ['Found(Mean)', 'Taken(Mean)', 'Nodes(Mean)']):\n",
    "        for algo, color, res in algo_results:\n",
    "            y_values = res[:, i:i + len(itr)]\n",
    "            name = result.columns[i + 1].split('_')[0] + ' ' + suffix\n",
    "            plt.title(name)\n",
    "            plt.xlabel('Percentage')\n",
    "            plt.ylabel(name)\n",
    "            if verbose:\n",
    "                plt.plot(itr, y_values.T, c=color, alpha=0.3)\n",
    "            plt.plot(itr, np.mean(y_values, axis=0), c=color, label=algo + ' Mean')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming result is a DataFrame with appropriate data\n",
    "# generate_graphs(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "FUYNIegxE4lr"
   },
   "outputs": [],
   "source": [
    "class TBCD_mine(object):\n",
    "    \"\"\"\n",
    "        tree based community detection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename=None, g=nx.Graph(), ttl=float('inf'), obs=7, path=\"\",\n",
    "                 start=None, end=None, level_max = 6, window_size = 20, theta = 0.5,\n",
    "                 dataset = None, edge_list = None, comm_list = None,window_ratio = 0.10):\n",
    "        \"\"\"\n",
    "            Constructor\n",
    "            :param g: networkx graph\n",
    "            :param ttl: edge time to live (days)\n",
    "            :param obs: observation window (days)\n",
    "            :param path: Path where generate the results and find the edge file\n",
    "            :param start: starting date\n",
    "            :param end: ending date\n",
    "        \"\"\"\n",
    "        print(\"initialization\")\n",
    "        self.path = path\n",
    "        self.graph = g\n",
    "        self.removed = 0\n",
    "        self.added = 0\n",
    "        self.filename = filename\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.obs = obs\n",
    "        self.communities = {}\n",
    "        self.intra_conn = {}\n",
    "        self.inter_conn = {}\n",
    "        self.select = {}\n",
    "        self.cluster_head = set()\n",
    "        self.cluster_h = {}\n",
    "        self.level = {}\n",
    "        self.occurence = {}\n",
    "        self.level_max = level_max\n",
    "        self.w_temp = 0\n",
    "        self.window_size = window_size\n",
    "        self.theta = theta\n",
    "        self.dataset = \"\"\n",
    "        self.edge_list = \"\"\n",
    "        self.comm_list = \"\"\n",
    "        self.ground_truth_comm = {}\n",
    "        self.precision = -1\n",
    "        self.recall = -1\n",
    "        self.nmi = -1\n",
    "        self.f_measure = -1\n",
    "        self.purity = -1\n",
    "        self.ari = -1\n",
    "        self.entropy = -1\n",
    "        self.modularity = -1\n",
    "        self.coverage = -1\n",
    "        self.external_density = -1\n",
    "        self.average_isolability = -1\n",
    "        self.mat_file_adj = \"\"\n",
    "        self.mat_file_label = \"\"\n",
    "        self.row = 1\n",
    "        self.result_array = []\n",
    "        self.window_ratio = window_ratio\n",
    "\n",
    "\n",
    "    def detachability(self,label):\n",
    "\n",
    "#         print(\"calculating detachability for label = \"+str(label))\n",
    "        G = self.graph\n",
    "        internal = 0\n",
    "        external = 0\n",
    "        DZ = 0\n",
    "        '''cluster_h_set = set()\n",
    "        cluster_head_set = set()\n",
    "        for node in G :\n",
    "            cluster_h_set.add(self.cluster_h[node])\n",
    "            cluster_head_set.add(self.cluster_head[node])\n",
    "        count_h = len(cluster_h_set)\n",
    "        count_head = len(cluster_head_set)\n",
    "        print(\"\\n\\n\\n count h = \"+str(count_h))\n",
    "        print(\"\\n\\n\\n count head = \" + str(count_head))'''\n",
    "        # node and node neighbour only taken into account\n",
    "        for node in G:\n",
    "            if self.cluster_h[node] == label:\n",
    "                for node_neighbour in G.neighbors(node):\n",
    "                    if self.cluster_h[node_neighbour] == label:\n",
    "                        internal = internal + G[node][node_neighbour]['weight']\n",
    "                    else:\n",
    "                        external = external + G[node][node_neighbour]['weight']\n",
    "                '''internal += self.intra_conn[node]\n",
    "                external += self.inter_conn[node]'''\n",
    "        if internal + external != 0:\n",
    "            DZ = internal / (internal + external)\n",
    "#         print(\"detachbility = \"+str(DZ))\n",
    "        return DZ\n",
    "\n",
    "    def max_comm_label(self,node):\n",
    "        #removed influence part\n",
    "        G = self.graph\n",
    "        all_labels = set()\n",
    "        # print(\"initially for node \"+str(node)+\" label is \"+str(var_dict[node]))\n",
    "        for node_neighbour in G.neighbors(node):\n",
    "            all_labels.add(self.cluster_h[node_neighbour])\n",
    "        prob_actual = 1\n",
    "        label_actual = self.cluster_h[node]\n",
    "        for label in all_labels:\n",
    "            # print(\"for label \"+str(label))\n",
    "            '''prob_new = 1\n",
    "            for node_chk in G.neighbors(node):\n",
    "                # print(\"u is-\"+str(u)+\" v is-\"+str(v))\n",
    "                if self.cluster_h[node_chk] == label:\n",
    "                    # print(\"prob_new = \"+str(prob_new)+\" edge weight \"+str(G[node][node_chk]['weight']))\n",
    "                    chk = 0\n",
    "                    if G.has_edge(node, node_chk):\n",
    "                        chk = G[node][node_chk]['weight']\n",
    "                    if var_dict['influence'][node][node_chk] == 1:\n",
    "                        # print(\"influence and edge weight true for \"+str(node)+\"-\"+str(node_chk))\n",
    "                        prob_new = prob_new * (1 - chk)\n",
    "            if prob_new < prob_actual:\n",
    "                prob_actual = prob_new\n",
    "                label_actual = label\n",
    "                self.cluster_h[node] = label'''\n",
    "        # print(\"after max_comm_label for node \" + str(node) + \" label is \" + str(var_dict[node]))\n",
    "        return label_actual\n",
    "\n",
    "    def isolability_measure_single_label(self,label):\n",
    "\n",
    "        G = self.graph\n",
    "        isolability = 0\n",
    "        internal = 0\n",
    "        external = 0\n",
    "        for node in G:\n",
    "            if self.cluster_h[node] == label:\n",
    "                for node_neighbour in G.neighbors(node):\n",
    "                    if (self.cluster_h[node] == self.cluster_h[node_neighbour]):\n",
    "                        internal = internal + G[node][node_neighbour]['weight']\n",
    "                    else:\n",
    "                        external = external + G[node][node_neighbour]['weight']\n",
    "        if external != 0: isolability = internal / (internal+external)\n",
    "        return isolability\n",
    "\n",
    "    def external_density_eval(self):\n",
    "\n",
    "        G = self.graph\n",
    "        numerator = 0\n",
    "        n = len(G)\n",
    "        denominator = n * (n - 1)\n",
    "        total_labels = set()\n",
    "        for node in G:\n",
    "            total_labels.add(self.cluster_h[node])\n",
    "        for label in total_labels:\n",
    "            nodes_per_label = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label: nodes_per_label += 1\n",
    "            denominator -= (nodes_per_label * (nodes_per_label - 1))\n",
    "        for (node1, node2) in G.edges():\n",
    "            if self.cluster_h[node1] != self.cluster_h[node2] and node1 != node2: numerator += 1\n",
    "        if denominator != 0:\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def coverage_eval(self):\n",
    "\n",
    "        G = self.graph\n",
    "        numerator = 0\n",
    "        denominator = len(G.edges)\n",
    "        for (node1, node2) in G.edges():\n",
    "            if self.cluster_h[node1] == self.cluster_h[node2] and node1 != node2: numerator += 1\n",
    "        if denominator != 0:\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def modularity_eval(self):\n",
    "\n",
    "        G = self.graph\n",
    "        total_edges = len(G.edges)\n",
    "        total_labels = set()\n",
    "        for node in G:\n",
    "            total_labels.add(self.cluster_h[node])\n",
    "        modularity = 0\n",
    "        internal_final = 0\n",
    "        external_final = 0\n",
    "        for label in total_labels:\n",
    "            internal = 0\n",
    "            external = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label:\n",
    "                    for node_neighbour in G.neighbors(node):\n",
    "                        if self.cluster_h[node_neighbour] == label:\n",
    "                            internal += 1\n",
    "                        else:\n",
    "                            external += 1\n",
    "            '''internal_final_check = internal/total_edges\n",
    "            external_final_check = external/total_edges\n",
    "            if internal_final_check >= 1 : print(\"internal problem\")\n",
    "            if external_final_check >= 1: print(\"external problem\")\n",
    "            modularity_current_label = internal_final - external_final*external_final\n",
    "            if modularity_current_label >= 1 : print(\"modularity current label problem\")\n",
    "            modularity += ((internal/total_edges) - ((external*external)/(total_edges*total_edges)))'''\n",
    "            modularity += internal / len(G.edges) - ((external * external) / (len(G.edges) * len(G.edges)))\n",
    "            internal_final += internal\n",
    "            external_final += external\n",
    "        internal_final = internal_final / 2\n",
    "        external_final = external_final / 2\n",
    "        modularity_final = (internal_final / total_edges) - (\n",
    "                    (external_final * external_final) / (total_edges * total_edges))\n",
    "        modularity_final = modularity / 2\n",
    "        if (internal_final + external_final) == total_edges:\n",
    "            print(\"modularity edge check correct\")\n",
    "            print(str(modularity_final))\n",
    "        return modularity_final\n",
    "\n",
    "    def TBCD_dissol(self):\n",
    "\n",
    "        G = self.graph\n",
    "        print(\"Inside dissolution phase\")\n",
    "        labels = set()\n",
    "        for node in self.cluster_head: labels.add(node)\n",
    "#         print(\"labels:\",labels)\n",
    "\n",
    "#         print(\"no of sets = \" + str(len(labels)))\n",
    "#         print(\"no of nodes = \" + str(len(G)))\n",
    "        for label in labels:\n",
    "            comm_set = []\n",
    "            for node in self.graph:\n",
    "                if self.cluster_h[node] == label:\n",
    "                    comm_set.append(node)\n",
    "                    comm_set.append(self.level[node])\n",
    "#                     print(\"Community:\",comm_set)\n",
    "\n",
    "        label_set = set()\n",
    "        for label in self.cluster_head:\n",
    "            label_set.add(label)\n",
    "#         print(\"label_set:\",label_set)\n",
    "\n",
    "        for label in label_set :\n",
    "#             print(\"for label = \"+str(label))\n",
    "            detachabil = self.detachability(label)\n",
    "#             print(\"detachabil:\",detachabil,\"theta:\",self.theta)\n",
    "\n",
    "            if detachabil < self.theta :\n",
    "                n_e = set()\n",
    "                for node in G :\n",
    "                    self.occurence[node] = 0\n",
    "                for node in G :\n",
    "                    if self.cluster_h[node] == label :\n",
    "                        for single in G.neighbors(node) :\n",
    "                            if self.cluster_h[single] != label :\n",
    "                                n_e.add(single)\n",
    "                                self.occurence[single] += 1\n",
    "#                 print(\"n_e set has = \"+str(len(n_e)))\n",
    "                max = -1\n",
    "                for node in G :\n",
    "                    if self.occurence[node] > max :\n",
    "                        max = self.occurence[node]\n",
    "                n_max = set()\n",
    "                for node in G :\n",
    "                    if self.occurence[node] == max :\n",
    "                        n_max.add(node)\n",
    "#                 print(\"n_max set has = \" + str(len(n_max)))\n",
    "\n",
    "                c_s = set()\n",
    "                for node in n_max :\n",
    "                    c_s.add(self.cluster_h[node])\n",
    "#                 print(\"c_s set has = \" + str(len(c_s)))\n",
    "                mid = -999999\n",
    "                big_label = -1\n",
    "                for other_label in c_s :\n",
    "                    label_node_set = set()\n",
    "                    for node in G :\n",
    "                        if self.cluster_h[node] == label :\n",
    "                            label_node_set.add(node)\n",
    "                    for node in label_node_set :\n",
    "                        self.cluster_h[node] = other_label\n",
    "                    term1 = self.detachability(other_label)\n",
    "                    for node in label_node_set :\n",
    "                        self.cluster_h[node] = label\n",
    "                    term2 = self.detachability(other_label)\n",
    "                    tid = term1 - term2\n",
    "#                     print(\"for label = \"+str(other_label))\n",
    "#                     print(\"tid = \"+str(tid))\n",
    "#                     print(\"mid = \" + str(mid))\n",
    "                    if tid > mid :\n",
    "#                         print(\"big label is other label\")\n",
    "                        mid = tid\n",
    "                        big_label = other_label\n",
    "                to_be_removed = -9999\n",
    "                if len(self.cluster_head) != 1 :\n",
    "                    self.cluster_head.remove(label)\n",
    "                    to_be_removed = label\n",
    "#                     print(\"big label = \"+str(big_label))\n",
    "#                     print(\"to be removed label = \" + str(label))\n",
    "                    for node in G :\n",
    "                        if self.cluster_h[node] == to_be_removed :\n",
    "                            self.cluster_h[node] = big_label\n",
    "\n",
    "    def combin(self,n):\n",
    "\n",
    "        n = int(n)\n",
    "        term = n*(n-1)\n",
    "        if term > 2:\n",
    "            return term/2\n",
    "        else : return 0\n",
    "\n",
    "\n",
    "    def all_metric_nog(self):\n",
    "\n",
    "        G = self.graph\n",
    "\n",
    "        self.modularity = self.modularity_eval()\n",
    "        self.coverage = self.coverage_eval()\n",
    "        self.external_density = self.external_density_eval()\n",
    "        print(\"\\n\\n\\n\\nwithout ground truth\\n\\n\\n\\n\")\n",
    "        print(\"modularity = \" + str(self.modularity))\n",
    "        print(\"coverage = \" + str(self.coverage))\n",
    "        print(\"external density = \" + str(self.external_density))\n",
    "        total_labels_mine = set()\n",
    "        for node in G:\n",
    "            total_labels_mine.add(self.cluster_h[node])\n",
    "        total_isolability = 0\n",
    "        for label in total_labels_mine:\n",
    "            total_isolability = total_isolability + self.isolability_measure_single_label(label)\n",
    "        average_isolability = total_isolability / len(total_labels_mine)\n",
    "        self.average_isolability = average_isolability\n",
    "        print(\"average isolability = \" + str(average_isolability))\n",
    "\n",
    "        self.result_array.append([self.row,len(total_labels_mine),self.modularity,self.coverage,self.external_density,\n",
    "                                  self.average_isolability])\n",
    "        self.row += 1\n",
    "\n",
    "    def all_metric(self):\n",
    "\n",
    "        G = self.graph\n",
    "\n",
    "        self.modularity = self.modularity_eval()\n",
    "        self.coverage = self.coverage_eval()\n",
    "        self.external_density = self.external_density_eval()\n",
    "        print(\"\\n\\n\\n\\nwithout ground truth\\n\\n\\n\\n\")\n",
    "        print(\"modularity = \" + str(self.modularity))\n",
    "        print(\"coverage = \" + str(self.coverage))\n",
    "        print(\"external density = \" + str(self.external_density))\n",
    "        total_labels_mine = set()\n",
    "        total_labels_ground = set()\n",
    "        for node in G:\n",
    "            total_labels_mine.add(self.cluster_h[node])\n",
    "            #total_labels_ground.add(self.ground_truth_comm[node])\n",
    "        total_isolability = 0\n",
    "        for label in total_labels_mine:\n",
    "            total_isolability = total_isolability + self.isolability_measure_single_label(label)\n",
    "        average_isolability = total_isolability / len(total_labels_mine)\n",
    "        self.average_isolability = average_isolability\n",
    "        print(\"average isolability = \" + str(average_isolability))\n",
    "\n",
    "        print(\"calculating ground truth metric\")\n",
    "        comm_list = open(self.comm_list)\n",
    "        print(\"before reading community text file\")\n",
    "\n",
    "        no_nodes = 0\n",
    "        ground_truth_comm_set = set()\n",
    "        for l in comm_list:\n",
    "            l = l.split(\" \")\n",
    "            node = int(l[0])\n",
    "            comm_label = int(l[1])\n",
    "            #print(\"for node x : \" + str(node) + \" community is  : \" + str(comm_label))\n",
    "            ground_truth_comm_set.add(comm_label)\n",
    "            self.ground_truth_comm[node] = comm_label\n",
    "            no_nodes += 1\n",
    "\n",
    "        print(\"no of communities in ground truth = \"+str(len(ground_truth_comm_set)))\n",
    "        #print(str(ground_truth_comm_set))\n",
    "        #for node in range(no_nodes): print(str(node)+\" \"+str(self.ground_truth_comm[node]))\n",
    "\n",
    "        algo_comm_set = set()\n",
    "        for node in G :\n",
    "            algo_comm_set.add(self.cluster_h[node])\n",
    "        print(\"no of communities from algo = \"+str(len(algo_comm_set)))\n",
    "\n",
    "\n",
    "        ground_truth_comm_list = list(ground_truth_comm_set)\n",
    "        algo_comm_list = list(algo_comm_set)\n",
    "        common_matrix = np.zeros((len(ground_truth_comm_set),len(algo_comm_set)))\n",
    "        for i in range(len(ground_truth_comm_set)) :\n",
    "            for j in range(len(algo_comm_set)) :\n",
    "                for node in G :\n",
    "                    if self.ground_truth_comm[node] == ground_truth_comm_list[i] and \\\n",
    "                            self.cluster_h[node] == algo_comm_list[j] :\n",
    "                        common_matrix[i][j] += 1\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        true_neg = 0\n",
    "        false_neg = 0\n",
    "        for node1 in G :\n",
    "            for node2 in G :\n",
    "                if self.cluster_h[node1] == self.cluster_h[node2] :\n",
    "                    if self.ground_truth_comm[node1] == self.ground_truth_comm[node2] :\n",
    "                        true_pos += 1\n",
    "                    else :\n",
    "                        false_pos += 1\n",
    "                else :\n",
    "                    if self.ground_truth_comm[node1] != self.ground_truth_comm[node2] :\n",
    "                        true_neg += 1\n",
    "                    else :\n",
    "                        false_neg += 1\n",
    "\n",
    "        self.precision = true_pos/(true_pos+false_pos)\n",
    "        self.recall = true_pos/(true_pos+false_neg)\n",
    "\n",
    "        ground_truth_comm_no = {}\n",
    "        ground_truth_comm_no_index = np.zeros(len(ground_truth_comm_set))\n",
    "        algo_comm_no = {}\n",
    "        algo_comm_no_index = np.zeros(len(algo_comm_set))\n",
    "        index = -1\n",
    "        for label in ground_truth_comm_set :\n",
    "            index += 1\n",
    "            ground_truth_comm_no[label] = 0\n",
    "            ground_truth_comm_no_index[index] = 0\n",
    "            for node in range(no_nodes) :\n",
    "                if self.ground_truth_comm[node] == label :\n",
    "                    #print(\"counting ground truth increase\")\n",
    "                    ground_truth_comm_no[label] += 1\n",
    "                    ground_truth_comm_no_index[index] += 1\n",
    "\n",
    "        index = -1\n",
    "        for label in algo_comm_set:\n",
    "            index += 1\n",
    "            algo_comm_no[label] = 0\n",
    "            algo_comm_no_index[index] = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label:\n",
    "                    algo_comm_no[label] += 1\n",
    "                    algo_comm_no_index[index] += 1\n",
    "\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "\n",
    "        for i in range(len(ground_truth_comm_set)):\n",
    "            if ground_truth_comm_no_index[i] == 0 : print(\"error in ground truth label counting\")\n",
    "\n",
    "\n",
    "        for i in range(len(ground_truth_comm_set)) :\n",
    "            for j in range(len(algo_comm_set)) :\n",
    "                if common_matrix[i][j] != 0 :\n",
    "                    numerator += common_matrix[i][j] * math.log((common_matrix[i][j]*no_nodes)/(algo_comm_no_index[j]*ground_truth_comm_no_index[i]))\n",
    "\n",
    "        numerator *= -2\n",
    "\n",
    "        for i in range(len(ground_truth_comm_set)):\n",
    "            denominator += ground_truth_comm_no_index[i] * math.log(ground_truth_comm_no_index[i]/no_nodes)\n",
    "\n",
    "        for j in range(len(algo_comm_set)):\n",
    "            denominator += algo_comm_no_index[j] * math.log(algo_comm_no_index[j]/no_nodes)\n",
    "\n",
    "        self.nmi = numerator/denominator\n",
    "\n",
    "        self.f_measure = 2 * (self.precision * self.recall)/(self.precision + self.recall)\n",
    "\n",
    "        self.purity = 0\n",
    "\n",
    "        for ground_truth_label in ground_truth_comm_set:\n",
    "            max = -1\n",
    "            for algo_label in algo_comm_set:\n",
    "                count = 0\n",
    "                for node in G :\n",
    "                    if self.cluster_h[node] == algo_label and self.ground_truth_comm[node] == ground_truth_label :\n",
    "                        count += 1\n",
    "                if count > max : max = count\n",
    "\n",
    "            self.purity += max\n",
    "\n",
    "        self.purity = self.purity/no_nodes\n",
    "\n",
    "        self.entropy = 0\n",
    "\n",
    "        for label_mine in algo_comm_set :\n",
    "            n_c = algo_comm_no[label_mine]\n",
    "            n = no_nodes\n",
    "            m = len(ground_truth_comm_set)\n",
    "            term2 = 0\n",
    "            for label_ground in ground_truth_comm_set :\n",
    "                n_i_j = 0\n",
    "                for node in G :\n",
    "                    if self.cluster_h[node] == label_mine and self.ground_truth_comm[node] == label_ground :\n",
    "                        n_i_j += 1\n",
    "                if n_i_j != 0 :\n",
    "                    term2 += (n_i_j/n_c)* math.log(n_i_j/n_c)\n",
    "            self.entropy += (n_c/n)*(1/math.log(m))*term2*(-1)\n",
    "\n",
    "        print(\"entropy = \" + str(self.entropy))\n",
    "\n",
    "        self.ari = 0\n",
    "        term_common = 0\n",
    "        for i in range(len(ground_truth_comm_set)):\n",
    "            for j in range(len(algo_comm_set)):\n",
    "                term_common += self.combin(common_matrix[i][j])\n",
    "\n",
    "        term_ground = 0\n",
    "\n",
    "        print(\"ground truth label set\")\n",
    "        print(str(ground_truth_comm_set))\n",
    "        for label in ground_truth_comm_set:\n",
    "            count = 0\n",
    "            for node in G:\n",
    "                if self.ground_truth_comm[node] == label: count += 1\n",
    "            term_ground += self.combin(count)\n",
    "\n",
    "        print(\"algo label set\")\n",
    "        print(str(algo_comm_set))\n",
    "        term_algo = 0\n",
    "        for label in algo_comm_set:\n",
    "            count = 0\n",
    "            for node in G:\n",
    "                if self.cluster_h[node] == label: count += 1\n",
    "            term_algo += self.combin(count)\n",
    "\n",
    "\n",
    "        term_sum = (term_ground + term_algo) / 2\n",
    "        term_prod = (term_ground * term_algo) / self.combin(no_nodes)\n",
    "\n",
    "        '''print(\"term_common = \"+str(term_common))\n",
    "        print(\"term_ground = \" + str(term_ground))\n",
    "        print(\"term_algo = \" + str(term_algo))\n",
    "        print(\"term_sum = \" + str(term_sum))\n",
    "        print(\"term_prod = \" + str(term_prod))'''\n",
    "\n",
    "        self.ari = (term_common - term_prod) / (term_sum - term_prod)\n",
    "\n",
    "        self.result_array.append([self.row,len(algo_comm_set),self.modularity,self.coverage,self.external_density,self.average_isolability,self.f_measure,self.nmi,self.purity,self.entropy,self.ari])\n",
    "        self.row += 1\n",
    "\n",
    "    def calldiverified(self):\n",
    "        print(self.graph)\n",
    "        return 1,3\n",
    "\n",
    "\n",
    "    def execute_TBCD_txt(self,file_name,perc):\n",
    "        \"\"\"\n",
    "            Execute tree based community detection algorithm from static dataset\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"reading static edge list with file name : \"+str(file_name))\n",
    "#         self.edge_list = './datasets_txt/' + file_name + '_edge_list.txt'\n",
    "#         self.comm_list = './datasets_txt/' + file_name + '_comm_list.txt'\n",
    "        self.edge_list = file_name + '_edge_list.txt'\n",
    "#         self.comm_list =  file_name + '_comm_list.txt'\n",
    "\n",
    "        edge_list = open(self.edge_list)\n",
    "#         print(\"counting edges to set window size\")\n",
    "        window_counter = 0\n",
    "        count = 0\n",
    "        for l in edge_list:\n",
    "            count += 1\n",
    "#         print(\"Count:\",count,\"Window ratio:\",self.window_ratio)\n",
    "\n",
    "        self.window_size = int(count*self.window_ratio)\n",
    "        # print(\"Window size:\",self.window_size)\n",
    "\n",
    "        self.graph = nx.Graph()\n",
    "        # print(self.graph)\n",
    "\n",
    "#       print(\"before reading edge list text file\")\n",
    "        no_nodes = 0\n",
    "        w = 0\n",
    "        edge_list = open(self.edge_list)\n",
    "        R_seed=[]\n",
    "        seedSet=[]\n",
    "        pagerank=[]\n",
    "        G = nx.DiGraph()\n",
    "        Delta = nx.DiGraph()\n",
    "        seedsetlist = {\n",
    "            'CELF': set(),\n",
    "            'PAGERANK': set(),\n",
    "            'DEGREE': set(),\n",
    "            'DBIM': set(),\n",
    "            'CGIM_IC': set(),\n",
    "            'GREEDY':set()\n",
    "            }\n",
    "        temp=1\n",
    "\n",
    "#         MultiDiGraph\n",
    "        # print(\"------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        print(count)\n",
    "        itr=[count]\n",
    "        #for i in range(len(perc)):\n",
    "         #   vv=int((perc[i]/100)*count)\n",
    "        #    itr.append(vv)\n",
    "        print(itr)\n",
    "        totalnodes=0\n",
    "        resultt={}\n",
    "\n",
    "        for l in edge_list:\n",
    "#             print(l)\n",
    "            totalnodes=totalnodes+1\n",
    "            l = l.split(\" \")\n",
    "            s=int(l[0])\n",
    "            t=int(l[1])\n",
    "            value = round(random.uniform(0.01,1.0),2)\n",
    "            G.add_edge(s,t,weight=value)\n",
    "            G.nodes[s]['thres']=(G.in_degree(s)/2)\n",
    "            G.nodes[t]['thres']=(G.in_degree(t)/2)\n",
    "\n",
    "            Delta.add_edge(s,t,weight=value)\n",
    "            #Delta.nodes[s]['thres']=(Delta.in_degree(s)/2)\n",
    "            #Delta.nodes[t]['thres']=(Delta.in_degree(t)/2)\n",
    "\n",
    "            #k=int(findk(len(G.nodes)))\n",
    "\n",
    "            self.added += 1\n",
    "            e = {}\n",
    "            x = int(l[0])\n",
    "            y = int(l[1])\n",
    "#             print(\"for edge x : \"+str(x)+\" y : \"+str(y))\n",
    "#             print(\".....................................................\")\n",
    "            if x == y:\n",
    "#                 print(\"self edge found\")\n",
    "                if totalnodes in itr:\n",
    "                    self.TBCD_dissol()\n",
    "                    w = 0\n",
    "                    print(\"_________________________________________________________________________________________\")\n",
    "                    print(\"Iteration   :   \")\n",
    "                    print(temp)\n",
    "                    temp=temp+1\n",
    "                    print(\"_________________________________________________________________________________________\")\n",
    "                    print(\"Get result at edge:\",totalnodes,itr)\n",
    "                    comm_set_final = detect_comm(self.graph, self.cluster_head, self.cluster_h)\n",
    "                    resultt=findresult(G,comm_set_final,Delta,seedsetlist)\n",
    "                    Delta.clear()\n",
    "                continue\n",
    "\n",
    "            if not self.graph.has_node(x) :\n",
    "#                 print(\"Graph Does not have node x.\")\n",
    "                self.graph.add_node(x)\n",
    "#                 print(\"intra connection of x:\",self.intra_conn[x])\n",
    "                self.intra_conn[x] = -1\n",
    "                self.inter_conn[x] = -1\n",
    "                self.select[x] = 0\n",
    "                no_nodes = no_nodes + 1\n",
    "#                 print(\"new node added\")\n",
    "                # print(\"no nodes = \"+str(no_nodes))\n",
    "\n",
    "            if not self.graph.has_node(y):\n",
    "#                 print(\"Graph Does not have node y.\")\n",
    "                self.graph.add_node(y)\n",
    "#                 print(\"intra connection of y:\",self.intra_conn[y])\n",
    "                self.intra_conn[y] = -1\n",
    "                self.inter_conn[y] = -1\n",
    "                self.select[y] = 0\n",
    "                no_nodes = no_nodes + 1\n",
    "#                 print(\"new node added\")\n",
    "                # print(\"no nodes = \" + str(no_nodes))\n",
    "\n",
    "            self.graph.add_edge(x, y, weight=1.0)\n",
    "\n",
    "            if self.select[x] == 0 and self.select[y] == 0:\n",
    "#                 print(\"If both are new.\")\n",
    "#                 print(\"select[x]==select[y]\")\n",
    "                # print(type(self.cluster_head))\n",
    "#                 print(\"Cluster head:\",self.cluster_head)\n",
    "                self.cluster_head.add(x)\n",
    "#                 print(\"Cluster head after added:\",self.cluster_head)\n",
    "                self.cluster_h[x] = x\n",
    "                self.level[x] = 0\n",
    "                self.level[y] = 1\n",
    "                self.cluster_h[y] = x\n",
    "                self.intra_conn[x] = 1\n",
    "                self.intra_conn[y] = 1\n",
    "                self.select[x] = 1\n",
    "                self.select[y] = 1\n",
    "\n",
    "            elif self.select[y] == 0 or self.select[x] == 0 :\n",
    "\n",
    "                if self.select[x] == 0 :\n",
    "                    term = y\n",
    "                    y = x\n",
    "                    x = term\n",
    "#                 print(\"Max level:\",self.level_max)\n",
    "\n",
    "\n",
    "                if self.level[x] <= self.level_max - 1:\n",
    "                    self.level[y] = self.level[x] + 1\n",
    "                    self.intra_conn[x] += 1\n",
    "                    self.intra_conn[y] = 1\n",
    "                    self.cluster_h[y] = self.cluster_h[x]\n",
    "                    self.select[y] = 1\n",
    "\n",
    "                else:\n",
    "                    self.cluster_head.add(x)\n",
    "                    self.cluster_h[x] = x\n",
    "                    self.level[x] = 0\n",
    "                    self.level[y] = 1\n",
    "                    self.cluster_h[y] = x\n",
    "                    self.inter_conn[x] = self.intra_conn[x]\n",
    "                    self.intra_conn[x] = 1\n",
    "                    self.intra_conn[y] = 1\n",
    "                    self.select[x] = 1\n",
    "                    self.select[y] = 1\n",
    "\n",
    "            else:  #If clusters are same\n",
    "#                 print(\"If clusters are same...,\")\n",
    "                if self.cluster_h[x] == self.cluster_h[y]:\n",
    "                    self.intra_conn[x] += 1\n",
    "                    self.intra_conn[y] += 1\n",
    "                else:\n",
    "                    self.inter_conn[x] += 1\n",
    "                    self.inter_conn[y] += 1\n",
    "#             print(\"cluster_h x:\",self.cluster_h[x],\"cluster_h y:\",self.cluster_h[y])\n",
    "#             print(\"Expansion phase started.\")\n",
    "\n",
    "#             labels = set()\n",
    "#             for node in self.cluster_head: labels.add(node)\n",
    "#             comm_set_final = []\n",
    "#             for label in labels:\n",
    "#                 comm_set = []\n",
    "#                 for node in self.graph :\n",
    "#                     if self.cluster_h[node] == label :\n",
    "#                         comm_set.append(node)\n",
    "#                 comm_set_final.append(comm_set)\n",
    "\n",
    "            w += 1\n",
    "            # print(\"w:\",w,\" window size:\",self.window_size)\n",
    "#             print(itr,totalnodes)\n",
    "            if w == self.window_size or totalnodes==count:\n",
    "                print(\"**************************************************************************\")\n",
    "                window_counter += 1\n",
    "#                 print(\"Window sizeee\",totalnodes)\n",
    "                self.TBCD_dissol()\n",
    "#                 seedset,activatednodes=self.calldiverified()\n",
    "#                 self.all_metric()\n",
    "                w = 0\n",
    "#                 findresult(G,k,comm_set_final):\n",
    "\n",
    "            if totalnodes in itr:\n",
    "                self.TBCD_dissol()\n",
    "                w = 0\n",
    "                print(\"_________________________________________________________________________________________\")\n",
    "                print(\"Iteration   :   \")\n",
    "                print(temp)\n",
    "                temp=temp+1\n",
    "                print(\"_________________________________________________________________________________________\")\n",
    "\n",
    "                print(\"Get result at node:\",totalnodes)\n",
    "                comm_set_final = detect_comm(self.graph, self.cluster_head, self.cluster_h)\n",
    "                new_nodes=set(G.nodes())\n",
    "                budgets=setBudget(G,new_nodes)\n",
    "                for i in range(len(budgets)):\n",
    "                    resultt[i]=findresult(G,comm_set_final,Delta,seedsetlist,budgets[i])\n",
    "                #Delta.clear()\n",
    "                displayresult(resultt,itr,budgets)\n",
    "                dff = makeExcel(resultt,itr,budgets)\n",
    "                dff.to_excel('Outputt.xlsx')\n",
    "                processResults()\n",
    "        return dff\n",
    "\n",
    "\n",
    "    def checkingg(self):\n",
    "        print(\"666\")\n",
    "\n",
    "    \n",
    "def detect_comm(graph, cluster_head, cluster_h):\n",
    "    labels = set()\n",
    "    for node in cluster_head:\n",
    "        labels.add(node)\n",
    "    comm_set_final = []\n",
    "    for label in labels:\n",
    "        comm_set = []\n",
    "        for node in graph :\n",
    "            if cluster_h[node] == label :\n",
    "                comm_set.append(node)\n",
    "        comm_set_final.append(comm_set)\n",
    "    return comm_set_final\n",
    "\n",
    "# obj=TBCD_mine()\n",
    "# perc=[20,40,60,80,100]\n",
    "# filename='LFR_500_0.0'\n",
    "# obj.execute_TBCD_txt(filename,perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "JuV0HpPsav3i"
   },
   "outputs": [],
   "source": [
    "def setBudget(graph,new_nodes):\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    # average degree\n",
    "    degrees = [graph.degree(node) for node in graph.nodes()]\n",
    "    average_degree = sum(degrees) / len(degrees)\n",
    "\n",
    "    for node in new_nodes:\n",
    "        graph.nodes[node]['cost'] = random.randint(1,int(average_degree*2))\n",
    "    budgets=[50,100,150,200,250,300]\n",
    "#     budgets=[50,100]\n",
    "    print('\\n average_degree:',average_degree)\n",
    "    print('\\n num_nodes:',num_nodes)\n",
    "    print('\\n Budget:',budgets)\n",
    "    return budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "haTniG2Lt_nM",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def executeDiv(path):\n",
    "    result_df=pd.DataFrame()\n",
    "    for i in range(1):\n",
    "        obj=TBCD_mine()\n",
    "\n",
    "        perc=[100]\n",
    "        #filename='C:/Users/suni/Downloads/Ashish/LFR_1000_0.2'\n",
    "        filename=path\n",
    "        dff=obj.execute_TBCD_txt(filename,perc)\n",
    "        dff2=result_df\n",
    "        result_df=dff\n",
    "        result_df = pd.concat([dff2, dff], ignore_index=True)\n",
    "        #result_df.to_excel('Outputt.xlsx')\n",
    "    return result_df\n",
    "def processResults():\n",
    "    df = pd.read_excel('Outputt.xlsx')\n",
    "    df=df.drop(columns='Unnamed: 0')\n",
    "    print(df.shape)\n",
    "#     print(df.head(8))\n",
    "    #TBCD = df.loc[df['Name of Algorithm']=='TBCD']\n",
    "    Greedy = df.loc[df['Name of Algorithm']=='Greedy']\n",
    "    CELF = df.loc[df['Name of Algorithm']=='CELF']\n",
    "    #PMIA = df.loc[df['Name of Algorithm']=='PMIA']\n",
    "    #SIMPATH = df.loc[df['Name of Algorithm']=='SIMPATH']\n",
    "    DEGREE = df.loc[df['Name of Algorithm']=='Degree']\n",
    "    PAGERANK = df.loc[df['Name of Algorithm']=='PageRank']\n",
    "    DBIM=df.loc[df['Name of Algorithm']=='DBIM']\n",
    "    CGIM_IC=df.loc[df['Name of Algorithm']=='CGIM_IC']\n",
    "    Dga = df.loc[df['Name of Algorithm']=='DGA']\n",
    "    Bridge = df.loc[df['Name of Algorithm']=='Bridge']\n",
    "\n",
    "    cols= df.columns\n",
    "\n",
    "    #ddris = pd.DataFrame(TBCD[cols].mean(),columns = ['TBCD'])\n",
    "    greedy = pd.DataFrame(Greedy.iloc[:,1:].mean(),columns = ['Greedy'])\n",
    "    celf = pd.DataFrame(CELF.iloc[:,1:].mean(),columns = ['CELF'])\n",
    "    #pmia = pd.DataFrame(PMIA[cols].mean(),columns = ['PMIA'])\n",
    "    #simpath = pd.DataFrame(SIMPATH[cols].mean(),columns = ['SIMPATH'])\n",
    "    degree = pd.DataFrame(DEGREE.iloc[:,1:].mean(),columns = ['DEGREE'])\n",
    "    pagerank = pd.DataFrame(PAGERANK.iloc[:,1:].mean(),columns = ['PageRank'])\n",
    "    dbim = pd.DataFrame(DBIM.iloc[:,1:].mean(),columns=['DBIM'])\n",
    "    cgim_IC=pd.DataFrame(CGIM_IC.iloc[:,1:].mean(),columns=['CGIM_IC'])\n",
    "    dga = pd.DataFrame(Dga.iloc[:,1:].mean(),columns=['DGA'])\n",
    "    bridge=pd.DataFrame(Bridge.iloc[:,1:].mean(),columns=['Bridge'])\n",
    "\n",
    "    resultant = pd.concat([greedy,celf,degree,pagerank,dbim,cgim_IC,bridge,dga], axis='columns')\n",
    "\n",
    "#     resultant = pd.concat([ddris,pmia,simpath,degree,pagerank], axis='columns')\n",
    "#     print(\"Resultant:\",resultant)\n",
    "\n",
    "    result=resultant.iloc[0:]\n",
    "#     print(\"Result:\",result)\n",
    "\n",
    "    result.to_excel('BW_Soc_Epinions1.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rdloopD9av3j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization\n",
      "508837\n",
      "[508837]\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "**************************************************************************\n",
      "Inside dissolution phase\n",
      "Inside dissolution phase\n",
      "_________________________________________________________________________________________\n",
      "Iteration   :   \n",
      "1\n",
      "_________________________________________________________________________________________\n",
      "Get result at node: 508837\n",
      "\n",
      " average_degree: 13.411800366372777\n",
      "\n",
      " num_nodes: 75879\n",
      "\n",
      " Budget: [50, 100, 150, 200, 250, 300]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7412\\121055728.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexecuteDiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'soc-Epinions1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprocessResults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7412\\765970351.py\u001b[0m in \u001b[0;36mexecuteDiv\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m#filename='C:/Users/suni/Downloads/Ashish/LFR_1000_0.2'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mdff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_TBCD_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mperc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mdff2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mresult_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7412\\4205330168.py\u001b[0m in \u001b[0;36mexecute_TBCD_txt\u001b[1;34m(self, file_name, perc)\u001b[0m\n\u001b[0;32m    741\u001b[0m                 \u001b[0mbudgets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msetBudget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbudgets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m                     \u001b[0mresultt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfindresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcomm_set_final\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseedsetlist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbudgets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    744\u001b[0m                 \u001b[1;31m#Delta.clear()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[0mdisplayresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresultt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbudgets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7412\\840375694.py\u001b[0m in \u001b[0;36mfindresult\u001b[1;34m(G, comm_set_final, Delta, seedsetlist, budget)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#DBIM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mDBIMseedSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDBIM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcomm_set_final\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbudget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseedsetlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DBIM'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m|\u001b[0m\u001b[0mnew_nodes\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mDBIMtime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n--------------- DBIM -------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7412\\700020381.py\u001b[0m in \u001b[0;36mDBIM\u001b[1;34m(graph, community_nodes, budget, candidate_nodes)\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_cost\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cost'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbudget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 \u001b[0mtemp_influence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemp_inodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_Phi_dbim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommunity_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                 \u001b[0mS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mmarginal_gain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_influence\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0minfluence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7412\\421710781.py\u001b[0m in \u001b[0;36mcompute_Phi_dbim\u001b[1;34m(G, S, communities)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mIC_S\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mactivated_set_S_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIC_S\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdiversity_activated_set_S\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcommunityDiversityFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIC_S\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommunities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#     diversity_V = len(communities)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#     diversity_V = activated_set_S_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7412\\1631781830.py\u001b[0m in \u001b[0;36mcommunityDiversityFunction\u001b[1;34m(G, S, communities)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcommunityDiversityFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommunities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     \u001b[0mactivated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_Threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m     \u001b[0mnoofcommunity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcom\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcommunities\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7412\\2501485716.py\u001b[0m in \u001b[0;36mlinear_Threshold\u001b[1;34m(graph, seeds)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minflunces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#                 print(\"Element:\",element,\"prerecored\",pre_node_record[element])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0mpre_node_record\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_node_record\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;31m#                 print(pre_node_record[element])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpre_node_record\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'thres'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\classes\\graph.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m         \"\"\"Returns a dict of neighbors of node n.  Use: 'G[n]'.\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result=executeDiv('soc-Epinions1')\n",
    "processResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jELEqoXmDRBR"
   },
   "outputs": [],
   "source": [
    "processResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIq9GJAgav3k"
   },
   "outputs": [],
   "source": [
    "# generate_graphs(result,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kO5jl2OQDRBR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WC9cVh-mDRBS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FgvDMI9DRBS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDGbsgDODRBS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s8yiEHwDRBS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMLHB-xpDRBS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRsJx91zDRBS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
