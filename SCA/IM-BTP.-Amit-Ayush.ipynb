{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26621,
     "status": "ok",
     "timestamp": 1712490083573,
     "user": {
      "displayName": "Ayush Anand",
      "userId": "06411905463708341492"
     },
     "user_tz": -330
    },
    "id": "RvNzOR1hf3s2",
    "outputId": "675c9cf6-beb0-4671-8996-72ec75c2dc75"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8759,
     "status": "ok",
     "timestamp": 1712472385120,
     "user": {
      "displayName": "Ayush Anand",
      "userId": "06411905463708341492"
     },
     "user_tz": -330
    },
    "id": "bRRcX5IHkgYQ",
    "outputId": "cc448c5c-7ebe-4ab1-ac72-03ef900c8b62"
   },
   "outputs": [],
   "source": [
    "# Installing the NetworkX Library to work with Network Graphs & their Visualizations\n",
    "# !pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NC_8kUm3fmMo"
   },
   "source": [
    "### Importing & Shaping Libraries, Modules & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJLxBdvXk1Mu"
   },
   "outputs": [],
   "source": [
    "# The NetworkX Library is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks\n",
    "import networkx as nx\n",
    "# The Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language\n",
    "import pandas as pd\n",
    "# The Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt\n",
    "# The Random Module is a built-in Python Module to work with Random Numbers\n",
    "import random\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 1063,
     "status": "ok",
     "timestamp": 1712490091638,
     "user": {
      "displayName": "Ayush Anand",
      "userId": "06411905463708341492"
     },
     "user_tz": -330
    },
    "id": "zpTKjObXMFS6",
    "outputId": "292b3fa8-f4bf-4cad-924a-2b5991b81ada"
   },
   "outputs": [],
   "source": [
    "# Converting the Dataframe structure into an Edgelist to feed into the NetworkX Library function\n",
    "edges = pd.read_csv(\"com-amazon.ungraph.txt\", sep=\"\\t\")\n",
    "# edges = pd.read_csv(\"LFR_500_0.8_edge_list.txt\", sep=\" \")\n",
    "\n",
    "# edges = pd.read_csv(\"CA-GrQc.txt\", sep=\"\\t\")\n",
    "\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0Ero_y9V7f9"
   },
   "outputs": [],
   "source": [
    "# Creating the NetworkX Graph using Pandas Edgelist\n",
    "GOT = nx.from_pandas_edgelist(edges, source = 'From', target = 'To')\n",
    "# Dictionary storing the Degree of each node in the Graph\n",
    "weighted_degrees = dict(nx.degree(GOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1712490099463,
     "user": {
      "displayName": "Ayush Anand",
      "userId": "06411905463708341492"
     },
     "user_tz": -330
    },
    "id": "VbPysQRJxMqM",
    "outputId": "afc3fe75-1aa6-40bc-d8e0-73551ca420de"
   },
   "outputs": [],
   "source": [
    "# Sort node names based on your desired range\n",
    "current_names = list(sorted(GOT.nodes()))\n",
    "\n",
    "# Sort node names based on your desired range\n",
    "num_nodes = len(current_names)\n",
    "sorted_names = list(range(0, num_nodes))\n",
    "\n",
    "# Create a mapping from current names to sorted names\n",
    "mapping = {current_names[i]: sorted_names[i] for i in range(num_nodes)}\n",
    "\n",
    "# Rename nodes\n",
    "nx.relabel_nodes(GOT, mapping, copy=False)\n",
    "\n",
    "# Now your graph nodes are renamed from 1 to the number of nodes\n",
    "# You can print the nodes to verify\n",
    "print(sorted(GOT.nodes()))\n",
    "weighted_degrees = dict(nx.degree(GOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvwYdApmfmMu"
   },
   "source": [
    "#### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpbrqDhz7UI5"
   },
   "outputs": [],
   "source": [
    "def sort_nodes(nodes, centrality):\n",
    "    \"\"\"\n",
    "    Sorts the Nodes in the given list of nodes based on the corresponding Centrality Measure of each node\n",
    "    Parameters:\n",
    "        nodes = List of nodes which has to be sorted\n",
    "        centrality = List of Centrality Measures corresponding to each node in the list\n",
    "    Returns:\n",
    "        List of nodes sorted on the basis of their Centrality Measure in descending order\n",
    "    \"\"\"\n",
    "    result = sorted(list(zip(centrality, nodes)), reverse = True)\n",
    "    result = zip(*result)\n",
    "    result = [list(tuple) for tuple in result]\n",
    "    return result[1]\n",
    "\n",
    "def sort_degree_centrality(nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Degree Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.degree_centrality(GOT)\n",
    "    cen_measure = []\n",
    "    for i in nodes:\n",
    "        cen_measure.append(centrality[i])\n",
    "    return cen_measure\n",
    "\n",
    "def sort_eigenvector_centrality(nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Eigenvector Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.degree_centrality(GOT)\n",
    "    cen_measure = []\n",
    "    for i in nodes:\n",
    "        cen_measure.append(centrality[i])\n",
    "    return sort_nodes(nodes, cen_measure)\n",
    "\n",
    "def sort_betweenness_centrality(nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Betweenness Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.betweenness_centrality(GOT)\n",
    "    return centrality\n",
    "\n",
    "def sort_closeness_centrality(nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Closeness Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.closeness_centrality(GOT)\n",
    "    return centrality\n",
    "\n",
    "def sort_katz_centrality(nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Katz Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.degree_centrality(GOT)\n",
    "    cen_measure = []\n",
    "    for i in nodes:\n",
    "        cen_measure.append(centrality[i])\n",
    "    return sort_nodes(nodes, cen_measure)\n",
    "\n",
    "def sort_percolation_centrality(nodes):\n",
    "    \"\"\"\n",
    "    Calculates the Percolation Centrality of each Node in the given list of nodes and returns a descendingly sorted list of nodes\n",
    "    Parameters:\n",
    "        nodes = List of nodes to be sorted on the basis of the Centrality Measure\n",
    "    Returns:\n",
    "        List of Nodes sorted in descending order on the basis of the Centrality Measure\n",
    "    \"\"\"\n",
    "    centrality = nx.degree_centrality(GOT)\n",
    "    cen_measure = []\n",
    "    for i in nodes:\n",
    "        cen_measure.append(centrality[i])\n",
    "    return sort_nodes(nodes, cen_measure)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvcZpnidnTOg"
   },
   "outputs": [],
   "source": [
    "def one_hop_area(meme):\n",
    "\n",
    "    temp = [] # Temporary list to store the One-Hop Area nodes of the meme\n",
    "    for i in meme:\n",
    "        temp.extend(list(GOT.neighbors(i)))\n",
    "    return list(set(temp) - set(meme)) # Returning only unique nodes, to avoid Node Repetition\n",
    "\n",
    "def two_hop_area(meme):\n",
    "\n",
    "    one_hop = one_hop_area(meme)\n",
    "    temp = [] # Temporary list to store the Two-Hop Area nodes of the meme\n",
    "    for i in one_hop:\n",
    "        temp.extend(list(GOT.neighbors(i)))\n",
    "    return list(set(temp) - set(meme) - set(one_hop)) # Returning only unique nodes, to avoid Node Repetition\n",
    "\n",
    "def calc_pcm_prob(nodes):\n",
    "    prob = [] # Temporary List to store the corresponding Cascade Probability of the Nodes\n",
    "    for i in nodes:\n",
    "        prob.append(weighted_degrees[i] / GOT.number_of_nodes())\n",
    "    return prob\n",
    "\n",
    "def calc_edges(group1, group2):\n",
    "    count = [] # Temporary storage to store the Number of edges corresponding each node\n",
    "    for i in group2:\n",
    "        edges = list(nx.edges(GOT, nbunch = [i]))\n",
    "        temp = 0\n",
    "        for i in edges:\n",
    "            if (i[1] in group1) or (i[1] in group2):\n",
    "                temp += 1\n",
    "        count.append(temp)\n",
    "    return count\n",
    "\n",
    "def sum_pd(list1, list2):\n",
    "    p = 0 # Temporary variable to store the Sum-Product of the two lists\n",
    "    for i in range(len(list1)):\n",
    "        p += (list1[i] * list2[i])\n",
    "    return p\n",
    "\n",
    "def calc_edge_prob(meme, one_hop):\n",
    "    N = GOT.number_of_nodes() # Total Nodes in the Graph\n",
    "    prob_sum_bet = 0 # Temporary variable to store the sum of the Edge Probabilities\n",
    "    prob_sum_cls = 0\n",
    "    for i in one_hop:\n",
    "        prob_prod = 1\n",
    "        for j in meme:\n",
    "            pij = 0.01\n",
    "            pij += ((GOT.degree(i) + GOT.degree(j)) / N)\n",
    "            pij += (len(list(nx.common_neighbors(GOT, i, j))) / N)\n",
    "            prob_prod *= (1 - pij)\n",
    "        prob_sum_bet += (1 - prob_prod)*bet_cen[i]\n",
    "        prob_sum_cls += (1 - prob_prod)*cls_cen[i]\n",
    "    return max(prob_sum_bet,prob_sum_cls)\n",
    "\n",
    "def LIE(meme):\n",
    "    k=len(meme)\n",
    "    Ns1_S = one_hop_area(meme) # One-Hop area of the Meme\n",
    "    Ns2_S = two_hop_area(meme) # Two-Hop Area of the Meme\n",
    "    pu = calc_pcm_prob(Ns2_S)\n",
    "    du = calc_edges(Ns1_S, Ns2_S)\n",
    "    return  k + ((1 + ((1 / len(Ns1_S)) * sum_pd(pu, du))) * calc_edge_prob(meme, Ns1_S)) if len(Ns1_S) !=0 else 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvJlPGClESat"
   },
   "outputs": [],
   "source": [
    "\n",
    "def pareto_dominance(seed_set_x, seed_set_y):\n",
    "    \"\"\"Check if seed set x dominates seed set y.\"\"\"\n",
    "    all_less_equal = seed_set_x['F'] >=seed_set_y['F'] and seed_set_x['sigma'] >= seed_set_y['sigma']\n",
    "    any_strictly_greater = seed_set_x['F'] > seed_set_y['F'] or seed_set_x['sigma'] > seed_set_y['sigma']\n",
    "    return all_less_equal and any_strictly_greater\n",
    "\n",
    "def pareto_optimal_seed_sets(seed_sets):\n",
    "    \"\"\"Identify Pareto-optimal seed sets from a list of seed sets.\"\"\"\n",
    "    pareto_optimal_sets = []\n",
    "    for seed_set_x in seed_sets:\n",
    "        if not any(pareto_dominance(seed_set_y, seed_set_x) for seed_set_y in seed_sets if seed_set_x != seed_set_y):\n",
    "            pareto_optimal_sets.append(seed_set_x)\n",
    "    return pareto_optimal_sets\n",
    "\n",
    "def pareto_rank(seed_sets):\n",
    "    \"\"\"Assign Pareto rank to seed sets.\"\"\"\n",
    "    ranks = [0] * len(seed_sets)\n",
    "    current_rank = 0\n",
    "    while True:\n",
    "        pareto_sets = []\n",
    "        for i, seed_set in enumerate(seed_sets):\n",
    "            if ranks[i] != 0:\n",
    "                continue  # Already assigned a rank\n",
    "            if all(not pareto_dominance(seed_set, other_set) for j, other_set in enumerate(seed_sets) if i != j and ranks[j] == 0):\n",
    "                pareto_sets.append(seed_set)\n",
    "                ranks[i] = current_rank + 1\n",
    "        if not pareto_sets:\n",
    "            break\n",
    "        current_rank += 1\n",
    "    for i in range(len(seed_sets)):\n",
    "      seed_sets[i]['rank'] = ranks[i]\n",
    "\n",
    "\n",
    "def crowding_distance(seed_sets):\n",
    "    \"\"\"Calculate crowding distance for seed sets based on their objectives.\"\"\"\n",
    "    for seed_set in seed_sets:\n",
    "        seed_set['distance'] = 0\n",
    "    seed_sets.sort(key=lambda x: x['F'])\n",
    "    seed_sets[0]['distance'] = float('inf')\n",
    "    seed_sets[len(seed_set)-1]['distance'] = float('inf')\n",
    "    rnge = seed_sets[len(seed_sets)-1]['F'] - seed_sets[0]['F']\n",
    "    for i in range(1,len(seed_sets)-1):\n",
    "      if rnge != 0:\n",
    "        seed_sets[i]['distance'] += (seed_sets[i+1]['F'] - seed_sets[i-1]['F'])/ rnge\n",
    "      else:\n",
    "        seed_sets[i]['distance'] = float('inf')\n",
    "\n",
    "    seed_sets.sort(key = lambda x: x['sigma'])\n",
    "    seed_sets[0]['distance'] = float('inf')\n",
    "    seed_sets[len(seed_sets)-1]['distance'] = float('inf')\n",
    "    rnge = seed_sets[len(seed_sets)-1]['sigma'] - seed_sets[0]['sigma']\n",
    "    for i in range(1,len(seed_sets)-1):\n",
    "      if rnge != 0:\n",
    "        seed_sets[i]['distance'] += (seed_sets[i+1]['sigma'] - seed_sets[i-1]['sigma'])/ rnge\n",
    "      else:\n",
    "        seed_sets[i]['distance'] = float('inf')\n",
    "\n",
    "\n",
    "def select_seed_sets(candidate_sets, Q):\n",
    "    \"\"\"Select Q seed sets from candidate sets.\"\"\"\n",
    "    selected = []\n",
    "    print(candidate_sets)\n",
    "    candidate_sets.sort(key=lambda x: (-x['rank'], x['distance']))\n",
    "    selected = candidate_sets[:Q]\n",
    "    return selected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_vr7yfUSMUy"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def calculate_gini_coefficient(network, seed_set, m):\n",
    "    \"\"\"Calculate Gini coefficient for a given seed set.\"\"\"\n",
    "    # Perform diffusion process to get influenced nodes\n",
    "    influenced_nodes = simulate_diffusion(network, seed_set)\n",
    "\n",
    "    # Calculate fractions of influenced nodes in each group\n",
    "    fractions = [len(set(group) & set(influenced_nodes)) / len(group) for group in groups]\n",
    "\n",
    "    # Sort fractions\n",
    "    sorted_fractions = sorted(fractions)\n",
    "\n",
    "    # Calculate Gini coefficient\n",
    "    n = len(sorted_fractions)\n",
    "    gini = (np.sum([(2 * i - n + 1) * sorted_fractions[i] for i in range(n)]) / (n * np.sum(sorted_fractions)))\n",
    "\n",
    "    return gini\n",
    "\n",
    "# def calculate_influence_spread(network, seed_set):\n",
    "#     influence = []\n",
    "#     for i in range(network.number_of_nodes()):\n",
    "#       if i in seed_set:\n",
    "#         influence.append(1)\n",
    "#       else:\n",
    "#         influence.append(0)\n",
    "#     influence = simulate_diffusion(network, influence)\n",
    "#     count = 0\n",
    "#     for i in range(len(influence)):\n",
    "#       count += influence[i]\n",
    "#     return count\n",
    "\n",
    "def calculate_influence_spread(network, seed_set):\n",
    "#     influence = []\n",
    "#     for i in range(network.number_of_nodes()):\n",
    "#       if i in seed_set:\n",
    "#         influence.append(1)\n",
    "#       else:\n",
    "#         influence.append(0)\n",
    "    influence = simulate_diffusion(network, seed_set)\n",
    "#     count = 0\n",
    "#     for i in range(len(influence)):\n",
    "#       count += influence[i]\n",
    "    return len(influence)\n",
    "\n",
    "def calculate_F(network, seed_set):\n",
    "    \"\"\"Calculate fairness metric F(S) for a given seed set.\"\"\"\n",
    "    gini_coefficient = calculate_gini_coefficient(network, seed_set,0.5)\n",
    "    return 1 - gini_coefficient\n",
    "\n",
    "def calculate_sigma(network, seed_set):\n",
    "    \"\"\"Calculate influence spread sigma(S) for a given seed set.\"\"\"\n",
    "    return LIE(seed_set)\n",
    "\n",
    "def seed_set_evaluation(network, seed_sets):\n",
    "    \"\"\"Evaluate F(S) and sigma(S) for a list of seed sets.\"\"\"\n",
    "    info = []\n",
    "    for seed_set in seed_sets:\n",
    "        F = calculate_F(network, seed_set)\n",
    "        sigma = calculate_sigma(network, seed_set)\n",
    "        inf_spr = calculate_influence_spread(network,seed_set)\n",
    "        info.append({'seed':seed_set,'F':F,'sigma':sigma, 'spread':inf_spr})\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiemOi96elxB"
   },
   "outputs": [],
   "source": [
    "bet_cen = nx.betweenness_centrality(GOT)\n",
    "cls_cen = nx.closeness_centrality(GOT)\n",
    "# bet_cen=cls_cen.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SOC_Amazon_Closeness_centrality.txt\", \"w\") as f:\n",
    "    for node, centrality in cls_cen.items():\n",
    "        f.write(f\"Node {node}: {centrality}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277134,
     "status": "ok",
     "timestamp": 1712490684203,
     "user": {
      "displayName": "Ayush Anand",
      "userId": "06411905463708341492"
     },
     "user_tz": -330
    },
    "id": "wy6Ls_TvfsK4",
    "outputId": "3d11113f-7215-40ab-be33-8bbfeb27265c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "def levy_walk(position, scale=0.01, beta=1.5):\n",
    "    u = np.random.normal()\n",
    "    v = np.random.normal()\n",
    "    s = u / (np.linalg.norm(v) ** (1/beta))\n",
    "    stepsize = scale * s\n",
    "\n",
    "    random_numbers = np.random.standard_cauchy()\n",
    "\n",
    "    step = stepsize * position\n",
    "    levy_walk_step = position + step * random_numbers\n",
    "\n",
    "    return levy_walk_step\n",
    "\n",
    "def initialize_particle(npop,k):\n",
    "    position=[]\n",
    "    for i in range(0,npop):\n",
    "      position.append(random.sample(range(GOT.number_of_nodes()),k))\n",
    "    return position\n",
    "\n",
    "def probabilityupdate(action_probabilities, chosen_action, signal, alpha, beta):\n",
    "    if signal == 1:  # If action was successful\n",
    "        action_probabilities[chosen_action] += alpha * (1 - action_probabilities[chosen_action])\n",
    "        action_probabilities[1 - chosen_action] = (1 - alpha) * action_probabilities[1 - chosen_action]\n",
    "    else:  # If action failed\n",
    "        action_probabilities[chosen_action] = (1 - beta) * action_probabilities[chosen_action]\n",
    "        action_probabilities[1 - chosen_action] = beta + (1 - beta) * action_probabilities[1 - chosen_action]\n",
    "\n",
    "    return action_probabilities\n",
    "\n",
    "def update_position(particle, destination, num_nodes, iteration, max_iter, a, action_probabilities, signal):\n",
    "    position = particle\n",
    "    r1 = a * (1 - iteration / max_iter)\n",
    "    r2 = 2 * np.pi * random.random()\n",
    "    r3 = 2 * random.random()\n",
    "    present_nodes = set()\n",
    "    not_present_nodes = set(GOT.nodes() - present_nodes)\n",
    "\n",
    "    chosen_action =  np.random.choice([0, 1], p=action_probabilities)\n",
    "\n",
    "\n",
    "    if chosen_action == 1:\n",
    "      for i in range(len(position)):\n",
    "        if random.random() <0.5:\n",
    "          position[i] = levy_walk(position[i]) + r1 * np.sin(r2) * np.abs(r3 * destination[i] - position[i])\n",
    "          position[i] = min(num_nodes - 1, max(0, int(round(position[i]))))\n",
    "          if position[i] in present_nodes:\n",
    "            position[i] = random.choice(list(not_present_nodes))\n",
    "            present_nodes.add(position[i])\n",
    "            not_present_nodes.remove(position[i])\n",
    "          else:\n",
    "            present_nodes.add(position[i])\n",
    "            not_present_nodes.remove(position[i])\n",
    "        else:\n",
    "          position[i] = levy_walk(position[i]) + r1 * np.cos(r2) * np.abs(r3 * destination[i] - position[i])\n",
    "          position[i] = min(num_nodes - 1, max(0, int(round(position[i]))))\n",
    "          if position[i] in present_nodes:\n",
    "            position[i] = random.choice(list(not_present_nodes))\n",
    "            present_nodes.add(position[i])\n",
    "            not_present_nodes.remove(position[i])\n",
    "          else:\n",
    "            present_nodes.add(position[i])\n",
    "            not_present_nodes.remove(position[i])\n",
    "    else:\n",
    "      for i in range(len(position)):\n",
    "        if random.random() <0.5:\n",
    "          position[i] = position[i] + r1 * np.sin(r2) * np.abs(r3 * destination[i] - position[i])\n",
    "          position[i] = min(num_nodes - 1, max(0, int(round(position[i]))))\n",
    "          if position[i] in present_nodes:\n",
    "            position[i] = random.choice(list(not_present_nodes))\n",
    "            present_nodes.add(position[i])\n",
    "            not_present_nodes.remove(position[i])\n",
    "          else:\n",
    "            present_nodes.add(position[i])\n",
    "            not_present_nodes.remove(position[i])\n",
    "        else:\n",
    "          position[i] = position[i] + r1 * np.cos(r2) * np.abs(r3 * destination[i] - position[i])\n",
    "          position[i] = min(num_nodes - 1, max(0, int(round(position[i]))))\n",
    "          if position[i] in present_nodes:\n",
    "            position[i] = random.choice(list(not_present_nodes))\n",
    "            present_nodes.add(position[i])\n",
    "            not_present_nodes.remove(position[i])\n",
    "          else:\n",
    "            present_nodes.add(position[i])\n",
    "            not_present_nodes.remove(position[i])\n",
    "    # Update probabilities based on the success or failure of chosen action\n",
    "    alpha = 0.1  # Learning rate for success\n",
    "    beta = 0.1   # Learning rate for failure\n",
    "    action_probabilities = probabilityupdate(action_probabilities, chosen_action, signal, alpha, beta)\n",
    "\n",
    "    return position,action_probabilities\n",
    "\n",
    "def optimize(k,npop, max_iter, num_nodes,a):\n",
    "    swarm = initialize_particle(npop,k)\n",
    "    global_best_position = random.sample(range(num_nodes), k)\n",
    "    global_best_fitness = calculate_influence_spread(GOT,global_best_position)\n",
    "    prev_swarm = copy.deepcopy(swarm)\n",
    "    action_probabilities = [0.5,0.5]\n",
    "    signal = 1\n",
    "    info = []\n",
    "    for iter in range(max_iter):\n",
    "        for i, particle in enumerate(swarm):\n",
    "            position,action_probabilities = update_position(particle, global_best_position, num_nodes, iter+1, max_iter,a,action_probabilities,signal)\n",
    "            swarm[i] = position\n",
    "            fitness = calculate_influence_spread(GOT,position)\n",
    "            if fitness > global_best_fitness:\n",
    "              signal = 1\n",
    "              global_best_fitness = fitness\n",
    "              global_best_position = position\n",
    "            else:\n",
    "              signal = 0\n",
    "        seed_sets = swarm + prev_swarm\n",
    "        info = seed_set_evaluation(GOT, seed_sets)\n",
    "        pareto_rank(info)\n",
    "        crowding_distance(info)\n",
    "    # Selection\n",
    "    population = select_seed_sets(info, 5)\n",
    "    return population\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def divide_nodes_into_groups(n, m):\n",
    "    if n.number_of_nodes() < m:\n",
    "        raise ValueError(\"Number of nodes must be greater than or equal to the number of groups.\")\n",
    "\n",
    "    # Initialize an empty list of groups\n",
    "    groups = [[] for _ in range(m)]\n",
    "\n",
    "    # Assign each node to a random group\n",
    "    nodes = list(n.nodes())\n",
    "    random.shuffle(nodes)\n",
    "    for i, node in enumerate(nodes):\n",
    "        group_index = i % m  # Cycle through groups\n",
    "        groups[group_index].append(node)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simulate_diffusion(graph, seeds):\n",
    "    influnces = seeds[:]\n",
    "    queue = influnces[:]\n",
    "    pre_node_record = defaultdict(float) \n",
    "#     print(\"Queue:\",queue)\n",
    "#     print(\"Influences:\",influnces)\n",
    "    while len(queue) != 0:\n",
    "        node = queue.pop(0)\n",
    "#         print(\"----------------------------------------------------------------------\")\n",
    "#         print(\"Take node:\",node)\n",
    "#         print(\"Neighbour:\",graph[node])\n",
    "        for element in graph[node]:\n",
    "            if element not in influnces:\n",
    "#                 print(\"Element:\",element,\"prerecored\",pre_node_record[element])\n",
    "                pre_node_record[element] = pre_node_record[element] + graph[node][element]['p'] \n",
    "#                 print(pre_node_record[element])\n",
    "                if pre_node_record[element] >= graph.nodes[element]['t']:\n",
    "#                     print(\">>>>>>>>>>>>>>>>>>node influeced:\",element)\n",
    "                    influnces.append(element)\n",
    "                    queue.append(element)\n",
    "#     influnce_num = len(influnces)\n",
    "#     print(\"Seed set:\",seeds,\"Activated nodes:\",influnces)\n",
    "    return influnces\n",
    "\n",
    "\n",
    "\n",
    "def logarithmic_penalty(actual_ratio, threshold_ratio, penalty_factor):\n",
    "    if actual_ratio >= threshold_ratio:\n",
    "        return 0  # No penalty if ratio meets or exceeds threshold\n",
    "    else:\n",
    "        logvalue=math.log(actual_ratio)\n",
    "#         print(\"logvalue:\",logvalue)\n",
    "        return -penalty_factor * logvalue\n",
    "\n",
    "\n",
    "def Calscore(threshold_ratio,actual_ratio):\n",
    "    penalty_factor = 1\n",
    "    penalty = logarithmic_penalty(actual_ratio, threshold_ratio, penalty_factor)\n",
    "    return penalty\n",
    "\n",
    "\n",
    "\n",
    "def findInf(IC_S,communities):\n",
    "    info={}\n",
    "    i=0\n",
    "    for comm in communities:\n",
    "        llist=[1,1]\n",
    "        infnodes=set(IC_S) & set(comm)\n",
    "        totalnodes=len(comm)\n",
    "        llist[0]=len(infnodes)\n",
    "        llist[1]=totalnodes\n",
    "        info[i]=llist\n",
    "        i=i+1\n",
    "    return info\n",
    "\n",
    "\n",
    "def balancecheck(G, activated, communities):\n",
    "    IC_S=activated\n",
    "    info=findInf(IC_S,communities)\n",
    "    threshold=0.5\n",
    "    summ=0\n",
    "    final_penalty=0\n",
    "    for comm in info:\n",
    "        infn=info[comm][0]\n",
    "        totalnodes=info[comm][1]\n",
    "        actual_ratio=infn/totalnodes\n",
    "        if(actual_ratio==0):\n",
    "            actual_ratio=0.1/totalnodes\n",
    "        pnlty=Calscore(threshold,actual_ratio)\n",
    "#         print(pnlty)\n",
    "        summ=summ+pnlty\n",
    "#         print(summ)\n",
    "    final_penalty=summ\n",
    "    return final_penalty\n",
    "\n",
    "\n",
    "def GetAllTheInfo(GOT,selected_seed,influenced_nodes):\n",
    "    communities=groups.copy()\n",
    "    community_spreads = [sum(1 for node in comm if node in influenced_nodes) for comm in communities]\n",
    "    worst_case_influence = min(community_spreads)\n",
    "    \n",
    "    LogBased=balancecheck(GOT, influenced_nodes, communities)\n",
    "    return LogBased,worst_case_influence\n",
    "\n",
    "\n",
    "\n",
    "print(\"Hello\")\n",
    "npop = 30\n",
    "max_iter = 5\n",
    "num_nodes = GOT.number_of_nodes()\n",
    "a = 10\n",
    "# p = np.zeros((GOT.number_of_nodes(),GOT.number_of_nodes()))\n",
    "groups = divide_nodes_into_groups(GOT, 10)\n",
    "# print(groups)\n",
    "# print(\"number of groups:\",len(groups))\n",
    "# for e in GOT.edges():\n",
    "#   n = e[1]\n",
    "#   if GOT.degree[n] > 0:\n",
    "#     p[e[0],e[1]] = random.uniform(0,1/GOT.degree[n])\n",
    "#   else:\n",
    "#     p[e[0],e[1]] = random.random()\n",
    "\n",
    "# psum = p.sum(axis=0)\n",
    "\n",
    "for e in GOT.edges():\n",
    "  GOT.edges[e]['p'] = random.uniform(0,1/GOT.degree[e[1]])\n",
    "\n",
    "for n in GOT.nodes():\n",
    "  GOT.nodes[n]['t'] = random.random()\n",
    "\n",
    "MyFinalData={} \n",
    "\n",
    "\n",
    "\n",
    "# print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2005,
     "status": "ok",
     "timestamp": 1712490810340,
     "user": {
      "displayName": "Ayush Anand",
      "userId": "06411905463708341492"
     },
     "user_tz": -330
    },
    "id": "r5v4CkF9Ne01",
    "outputId": "4bea6aad-42b6-496a-99c4-06475e4e0fcd"
   },
   "outputs": [],
   "source": [
    "k=10\n",
    "start_time = time.time()\n",
    "info=optimize(k, npop, max_iter, num_nodes,a)\n",
    "end_time = time.time()  # End time\n",
    "print(\"-------------------------------------------\")\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "selected_seed = info[-1]['seed']\n",
    "print(\"k:\",len(selected_seed))\n",
    "# print(\"Lie:\",LIE(selected_seed))\n",
    "Inf=simulate_diffusion(GOT,selected_seed)\n",
    "FFairness=calculate_F(GOT,selected_seed)\n",
    "print(\"Activated nodes:\",len(Inf))\n",
    "# print(\"Fairness:\",)\n",
    "\n",
    "LogBased,Maxmin=GetAllTheInfo(GOT,selected_seed,Inf)\n",
    "MyFinalData[k]={\n",
    "    'k':k,\n",
    "    'inf':len(Inf),\n",
    "    'Maxmin':Maxmin,\n",
    "    'LogBased':LogBased,\n",
    "    'F_Fairness':FFairness,\n",
    "    'time':end_time,\n",
    "    'seedSet':selected_seed,\n",
    "}\n",
    "print(MyFinalData)\n",
    "\n",
    "df = pd.DataFrame.from_dict(MyFinalData, orient='index')\n",
    "excel_filename = \"Amazon_\"+str(k)+\"_\"+\".xlsx\"\n",
    "df.to_excel(excel_filename, index=False)\n",
    "print(f\"Excel file '{excel_filename}' has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124413,
     "status": "ok",
     "timestamp": 1712493329894,
     "user": {
      "displayName": "Ayush Anand",
      "userId": "06411905463708341492"
     },
     "user_tz": -330
    },
    "id": "YC5PVirUVCvL",
    "outputId": "4aeb9495-5c3b-4790-9117-351b1e9fc9ee"
   },
   "outputs": [],
   "source": [
    "# k=10\n",
    "# info=optimize(k, npop, max_iter, num_nodes,a )\n",
    "# print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20\n",
    "start_time = time.time()\n",
    "info=optimize(k, npop, max_iter, num_nodes,a)\n",
    "end_time = time.time()  # End time\n",
    "\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "selected_seed = info[-1]['seed']\n",
    "print(\"k:\",len(selected_seed))\n",
    "# print(\"Lie:\",LIE(selected_seed))\n",
    "Inf=simulate_diffusion(GOT,selected_seed)\n",
    "FFairness=calculate_F(GOT,selected_seed)\n",
    "print(\"Activated nodes:\",len(Inf))\n",
    "# print(\"Fairness:\",)\n",
    "\n",
    "LogBased,Maxmin=GetAllTheInfo(GOT,selected_seed,Inf)\n",
    "MyFinalData[k]={\n",
    "    'k':k,\n",
    "    'inf':len(Inf),\n",
    "    'Maxmin':Maxmin,\n",
    "    'LogBased':LogBased,\n",
    "    'F_Fairness':FFairness,\n",
    "    'time':end_time,\n",
    "    'seedSet':selected_seed,\n",
    "}\n",
    "print(MyFinalData)\n",
    "\n",
    "df = pd.DataFrame.from_dict(MyFinalData, orient='index')\n",
    "excel_filename = \"Amazon_\"+str(k)+\"_\"+\".xlsx\"\n",
    "df.to_excel(excel_filename, index=False)\n",
    "print(f\"Excel file '{excel_filename}' has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=30\n",
    "start_time = time.time()\n",
    "info=optimize(k, npop, max_iter, num_nodes,a)\n",
    "end_time = time.time()  # End time\n",
    "\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "selected_seed = info[-1]['seed']\n",
    "print(\"k:\",len(selected_seed))\n",
    "# print(\"Lie:\",LIE(selected_seed))\n",
    "Inf=simulate_diffusion(GOT,selected_seed)\n",
    "FFairness=calculate_F(GOT,selected_seed)\n",
    "print(\"Activated nodes:\",len(Inf))\n",
    "# print(\"Fairness:\",)\n",
    "\n",
    "LogBased,Maxmin=GetAllTheInfo(GOT,selected_seed,Inf)\n",
    "MyFinalData[k]={\n",
    "    'k':k,\n",
    "    'inf':len(Inf),\n",
    "    'Maxmin':Maxmin,\n",
    "    'LogBased':LogBased,\n",
    "    'F_Fairness':FFairness,\n",
    "    'time':end_time,\n",
    "    'seedSet':selected_seed,\n",
    "}\n",
    "print(MyFinalData)\n",
    "\n",
    "df = pd.DataFrame.from_dict(MyFinalData, orient='index')\n",
    "excel_filename = \"Amazon_\"+str(k)+\"_\"+\".xlsx\"\n",
    "df.to_excel(excel_filename, index=False)\n",
    "print(f\"Excel file '{excel_filename}' has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=40\n",
    "start_time = time.time()\n",
    "info=optimize(k, npop, max_iter, num_nodes,a)\n",
    "end_time = time.time()  # End time\n",
    "\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "selected_seed = info[-1]['seed']\n",
    "print(\"k:\",len(selected_seed))\n",
    "# print(\"Lie:\",LIE(selected_seed))\n",
    "Inf=simulate_diffusion(GOT,selected_seed)\n",
    "FFairness=calculate_F(GOT,selected_seed)\n",
    "print(\"Activated nodes:\",len(Inf))\n",
    "# print(\"Fairness:\",)\n",
    "\n",
    "LogBased,Maxmin=GetAllTheInfo(GOT,selected_seed,Inf)\n",
    "MyFinalData[k]={\n",
    "    'k':k,\n",
    "    'inf':len(Inf),\n",
    "    'Maxmin':Maxmin,\n",
    "    'LogBased':LogBased,\n",
    "    'F_Fairness':FFairness,\n",
    "    'time':end_time,\n",
    "    'seedSet':selected_seed,\n",
    "}\n",
    "print(MyFinalData)\n",
    "\n",
    "df = pd.DataFrame.from_dict(MyFinalData, orient='index')\n",
    "excel_filename = \"Amazon_\"+str(k)+\"_\"+\".xlsx\"\n",
    "df.to_excel(excel_filename, index=False)\n",
    "print(f\"Excel file '{excel_filename}' has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=50\n",
    "start_time = time.time()\n",
    "info=optimize(k, npop, max_iter, num_nodes,a)\n",
    "end_time = time.time()  # End time\n",
    "\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "selected_seed = info[-1]['seed']\n",
    "print(\"k:\",len(selected_seed))\n",
    "# print(\"Lie:\",LIE(selected_seed))\n",
    "Inf=simulate_diffusion(GOT,selected_seed)\n",
    "FFairness=calculate_F(GOT,selected_seed)\n",
    "print(\"Activated nodes:\",len(Inf))\n",
    "# print(\"Fairness:\",)\n",
    "\n",
    "LogBased,Maxmin=GetAllTheInfo(GOT,selected_seed,Inf)\n",
    "MyFinalData[k]={\n",
    "    'k':k,\n",
    "    'inf':len(Inf),\n",
    "    'Maxmin':Maxmin,\n",
    "    'LogBased':LogBased,\n",
    "    'F_Fairness':FFairness,\n",
    "    'time':end_time,\n",
    "    'seedSet':selected_seed,\n",
    "}\n",
    "print(MyFinalData)\n",
    "\n",
    "df = pd.DataFrame.from_dict(MyFinalData, orient='index')\n",
    "excel_filename = \"Amazon_\"+str(k)+\"_\"+\".xlsx\"\n",
    "df.to_excel(excel_filename, index=False)\n",
    "print(f\"Excel file '{excel_filename}' has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
